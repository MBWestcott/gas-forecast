{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9c3764d3",
      "metadata": {
        "id": "9c3764d3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557a2bba",
      "metadata": {},
      "source": [
        "Initial steps if running on Google Colab, to download support files from GitHub and set the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "vsukzbcUFNAQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsukzbcUFNAQ",
        "outputId": "d014ee2a-a929-41f8-94f9-5b56a384a2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/gas-forecast/notebooks'\n",
            "d:\\dev\\gas-forecast\\notebooks\n"
          ]
        }
      ],
      "source": [
        "#for running on Colab\n",
        "#!git clone https://github.com/MBWestcott/gas-forecast.git\n",
        "\n",
        "# 2. Change into the repo directory\n",
        "%cd /content/gas-forecast/notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4cfcb1",
      "metadata": {},
      "source": [
        "### First download the raw data from the National Gas data portal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "69f4bd0f",
      "metadata": {
        "id": "69f4bd0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "raw_data_folder = Path(\"../data/raw/\")\n",
        "\n",
        "def download_csv(url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a CSV file from the given URL and saves it to the specified file.\n",
        "\n",
        "    :param url: URL to download the CSV data from.\n",
        "    :param output_file: Path to the local file where the CSV will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure we notice bad responses\n",
        "\n",
        "        # Write the content (CSV data) to a file in binary mode\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"CSV file has been successfully downloaded and saved as '{output_file}'.\")\n",
        "\n",
        "    except requests.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"An error occurred: {err}\")\n",
        "\n",
        "\n",
        "def download_raw_data():\n",
        "    pubIdsFile = Path(\"../PUB ids.txt\")\n",
        "    with open(pubIdsFile) as f:\n",
        "        pubIds = f.read()\n",
        "        pubIds = pubIds.replace(\"\\n\", \",\").strip()\n",
        "\n",
        "    earliest = datetime.date(2020,4,1) # Download data going back 5 years\n",
        "    \n",
        "    download_from = datetime.date.today().replace(day=1) # start first download on first day of current month\n",
        "    download_to = datetime.date.today() # end first download on today's date\n",
        "    while(download_from > earliest):\n",
        "\n",
        "        # Format the date in yyyy-mm-dd format\n",
        "        formatted_from = download_from.strftime(\"%Y-%m-%d\")\n",
        "        formatted_to = download_to.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        csv_url = f\"https://data.nationalgas.com/api/find-gas-data-download?applicableFor=Y&dateFrom={formatted_from}&dateTo={formatted_to}&dateType=GASDAY&latestFlag=Y&ids={pubIds}&type=CSV\"\n",
        "        month_format = download_from.strftime(\"%Y-%m\")\n",
        "        output_filename = raw_data_folder /  f\"{month_format}.csv\"\n",
        "\n",
        "        download_csv(csv_url, output_filename)\n",
        "        time.sleep(2) # brief courtesy sleep\n",
        "        download_to = download_from - datetime.timedelta(days=1) # next download should go up to the day before the previous download start date\n",
        "        download_from = download_to.replace(day=1) # next download should start on the first day of the month\n",
        "\n",
        "# Do the download if the raw data is not there already\n",
        "csvCount = sum(1 for f in raw_data_folder.iterdir() if f.is_file() and f.suffix == '.csv')\n",
        "if(csvCount < 60):\n",
        "    download_raw_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a322d6",
      "metadata": {},
      "source": [
        "### Load the raw data\n",
        "\n",
        "Load the raw CSVs into a single and dataframe, pivot it so that each column represents a feature.\n",
        "Rename the Applicable At date field to Gas Day, and rename the columns that are going to be reused for ground truth and time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dd682eab",
      "metadata": {
        "id": "dd682eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n"
          ]
        },
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '..\\\\data\\\\processed\\\\pivoted.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     66\u001b[0m df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m---> 67\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/pivoted.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m df\u001b[38;5;241m.\u001b[39minfo()\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '..\\\\data\\\\processed\\\\pivoted.csv'"
          ]
        }
      ],
      "source": [
        "label_cols = [\"SAP\", \"SMPBuy\", \"SMPSell\"]\n",
        "\n",
        "def pivot(df, cols):\n",
        "\n",
        "    #only keep the values we are interested in\n",
        "    mask = df[\"Data Item\"].isin(cols)\n",
        "\n",
        "    df_filtered = df[mask]\n",
        "\n",
        "    # if there are duplicates for the field and gas day, take the latest\n",
        "    df_latest = (\n",
        "        df_filtered\n",
        "        .sort_values(\"Applicable At\")\n",
        "        .groupby([\"Gas Day\", \"Data Item\"])\n",
        "        .last()  # this takes the row with the highest (i.e. latest) \"Applicable At\" per group\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # pivot to get 1 row per gas day\n",
        "    df_latest = df_latest.pivot(index=\"Gas Day\", columns=\"Data Item\", values=\"Value\").reset_index()\n",
        "\n",
        "    # Drop 1 column that accounts for most of the NaNs\n",
        "    #df_latest.drop(columns=[\"Composite Weather Variable - Actual\"], inplace=True)\n",
        "\n",
        "    return df_latest\n",
        "\n",
        "def load_data():\n",
        "    #Read raw CSVs\n",
        "    pathlist = list(Path(raw_data_folder).rglob('*.csv'))\n",
        "    file_count = len(pathlist)\n",
        "    dfs = []\n",
        "    files_done = 0\n",
        "    for path_obj in pathlist:\n",
        "        path = str(path_obj)\n",
        "\n",
        "        df = pd.read_csv(path,\n",
        "            parse_dates=[\"Applicable At\", \"Applicable For\", \"Generated Time\"],\n",
        "            dayfirst=True)\n",
        "\n",
        "        df.rename(columns={'Applicable For': 'Gas Day'}, inplace=True)\n",
        "        df['Gas Day'] = pd.to_datetime(df['Gas Day'], dayfirst=True)\n",
        "        # daily summary columns:\n",
        "\n",
        "        daily_cols = df[\"Data Item\"].unique()\n",
        "        # print(daily_cols)\n",
        "        # Get price and demand columns, to use as tommorow's ground truth, and with 1-3 days lag\n",
        "\n",
        "        #label_cols = [\"SAP, Actual Day\", \"SMP Buy, Actual Day\", \"SMP Sell, Actual Day\", \"Demand Actual, NTS, D+1\"]\n",
        "\n",
        "        #df_labels = pivot(df, label_cols)\n",
        "\n",
        "\n",
        "        df_daily = pivot(df, daily_cols)\n",
        "        dfs.append(df_daily)\n",
        "\n",
        "        files_done += 1\n",
        "        if files_done % 10 == 0:\n",
        "            print(f\"Processed {files_done} of {file_count} raw files\")\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    #Rename the columns that are going to be reused for ground truth and time series\n",
        "    df.rename(columns={\"SAP, Actual Day\": 'SAP', \"SMP Buy, Actual Day\": 'SMPBuy', \"SMP Sell, Actual Day\": 'SMPSell'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "df.to_csv(Path(\"../data/processed/pivoted.csv\"), index=False)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c8e618",
      "metadata": {},
      "source": [
        "### Preprocess data\n",
        "\n",
        "Add the previous 5 days' prices as lag features, and 7- and 30-day rolling averages and standard deviations. Also add day of week features, and a cyclical coding of the day of year for seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "267c8bfe",
      "metadata": {},
      "outputs": [
        {
          "ename": "PermissionError",
          "evalue": "[Errno 13] Permission denied: '..\\\\data\\\\processed\\\\preprocessed.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 44\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m preprocess(df)\n\u001b[1;32m---> 44\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../data/processed/preprocessed.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3967\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3956\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3958\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3959\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3960\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3964\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3965\u001b[0m )\n\u001b[1;32m-> 3967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3968\u001b[0m     path_or_buf,\n\u001b[0;32m   3969\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3970\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3971\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3972\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3973\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3974\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3975\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3976\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3977\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3978\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3979\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3980\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3981\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3982\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3983\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3984\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1014\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m    993\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    995\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m    996\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m    997\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1013\u001b[0m )\n\u001b[1;32m-> 1014\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1017\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:251\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    254\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    255\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    256\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    257\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    261\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    262\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    267\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    268\u001b[0m     )\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
            "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '..\\\\data\\\\processed\\\\preprocessed.csv'"
          ]
        }
      ],
      "source": [
        "def preprocess(df, add_lags=True, add_labels=True):\n",
        "\n",
        "    \"\"\"Deal with missing values, add lagged features, rolling averages and stds, Day of Week, and cyclic encoding for seasonality\"\"\"\n",
        "\n",
        "    if add_lags:\n",
        "        lag_days = 5\n",
        "        for i in range(1, lag_days+1):\n",
        "            for col in label_cols:\n",
        "                df[f\"{col} D-{i}\"] = df[col].shift(i)\n",
        "\n",
        "        # add rolling averages and stds\n",
        "        for col in label_cols:\n",
        "            for window in [7, 30]:\n",
        "                df[f'{col} D{window} roll mean'] = (\n",
        "                    df[col]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .mean()\n",
        "                    )\n",
        "                df[f'{col} D{window} roll std'] = (\n",
        "                    df[col]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .std()\n",
        "                )\n",
        "\n",
        "    # add day of week\n",
        "    df['Day of Week'] = df['Gas Day'].dt.weekday\n",
        "    df['Is Weekday'] = (df['Gas Day'].dt.weekday < 5).astype(int)\n",
        "    df['Next Day Is Weekday'] = ((df['Gas Day'] + pd.Timedelta(days=1)).dt.weekday < 5).astype(int)\n",
        "    # cyclic encoding for seasonality\n",
        "    df['Day of Year'] = df['Gas Day'].dt.dayofyear\n",
        "    df['sin_DoY'] = np.sin(2 * np.pi * df['Day of Year'] / 365)\n",
        "    df['cos_DoY'] = np.cos(2 * np.pi * df['Day of Year'] / 365)\n",
        "\n",
        "    if add_labels:\n",
        "        # Add labels for next day's actuals\n",
        "        for col in label_cols:\n",
        "            df[f\"Next Day {col}\"] = df[col].shift(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3358d8",
      "metadata": {},
      "source": [
        "### Clean missing values and outliers\n",
        "\n",
        "Most of the missing values are missing \"Composite Weather Variable - Actual\" from 2020-21. These affect around 15% of the dataset. Best way to fill in those is with the Normal forecast, which should usually be the closest. Apart from that there are very few missing readings so it is feasible to discard any remaining rows with missing data (done at the end, to avoid introducing errors into the lag features)\n",
        "\n",
        "Also remove outliers where any of the prices was 0, and one of the next day prices was more 50% away from the current day's price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c9c69e5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1784, 78)\n",
            "(1784, 78)\n",
            "(1783, 78)\n",
            "(1783, 78)\n",
            "(1762, 78)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>SMPSell D30 roll std</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "      <th>Next Day SAP</th>\n",
              "      <th>Next Day SMPBuy</th>\n",
              "      <th>Next Day SMPSell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-05-06</td>\n",
              "      <td>50.23730</td>\n",
              "      <td>142.20118</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>12.0645</td>\n",
              "      <td>245.399580</td>\n",
              "      <td>216.150490</td>\n",
              "      <td>145.432835</td>\n",
              "      <td>116.183744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005142</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>0.816538</td>\n",
              "      <td>-0.577292</td>\n",
              "      <td>0.4834</td>\n",
              "      <td>0.5187</td>\n",
              "      <td>0.4481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-05-07</td>\n",
              "      <td>53.59770</td>\n",
              "      <td>141.87295</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>13.4655</td>\n",
              "      <td>243.927941</td>\n",
              "      <td>214.341578</td>\n",
              "      <td>145.153439</td>\n",
              "      <td>115.567076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011180</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.806480</td>\n",
              "      <td>-0.591261</td>\n",
              "      <td>0.4756</td>\n",
              "      <td>0.5109</td>\n",
              "      <td>0.4403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-05-08</td>\n",
              "      <td>51.08822</td>\n",
              "      <td>135.19872</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>15.5400</td>\n",
              "      <td>171.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010249</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0.796183</td>\n",
              "      <td>-0.605056</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.5075</td>\n",
              "      <td>0.4369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-05-09</td>\n",
              "      <td>53.28634</td>\n",
              "      <td>127.04213</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>15.0800</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009697</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0.785650</td>\n",
              "      <td>-0.618671</td>\n",
              "      <td>0.4615</td>\n",
              "      <td>0.4968</td>\n",
              "      <td>0.4262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-05-10</td>\n",
              "      <td>53.14522</td>\n",
              "      <td>127.29315</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>12.6200</td>\n",
              "      <td>264.115666</td>\n",
              "      <td>226.223933</td>\n",
              "      <td>180.336933</td>\n",
              "      <td>142.445199</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009489</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>0.774884</td>\n",
              "      <td>-0.632103</td>\n",
              "      <td>0.4569</td>\n",
              "      <td>0.4922</td>\n",
              "      <td>0.4216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "5         2020-05-06                                 50.23730   \n",
              "6         2020-05-07                                 53.59770   \n",
              "7         2020-05-08                                 51.08822   \n",
              "8         2020-05-09                                 53.28634   \n",
              "9         2020-05-10                                 53.14522   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "5                                    142.20118   \n",
              "6                                    141.87295   \n",
              "7                                    135.19872   \n",
              "8                                    127.04213   \n",
              "9                                    127.29315   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "5                               192.43848                   192.43848   \n",
              "6                               195.47065                   195.47065   \n",
              "7                               186.28694                   186.28694   \n",
              "8                               180.32847                   180.32847   \n",
              "9                               180.43837                   180.43837   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "5                                      12.0645     245.399580   \n",
              "6                                      13.4655     243.927941   \n",
              "7                                      15.5400     171.000000   \n",
              "8                                      15.0800     172.000000   \n",
              "9                                      12.6200     264.115666   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "5                                                 216.150490       \n",
              "6                                                 214.341578       \n",
              "7                                                 131.000000       \n",
              "8                                                 134.000000       \n",
              "9                                                 226.223933       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "5             145.432835   \n",
              "6             145.153439   \n",
              "7             154.000000   \n",
              "8             141.000000   \n",
              "9             180.336933   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "5                                                 116.183744      ...   \n",
              "6                                                 115.567076      ...   \n",
              "7                                                 114.000000      ...   \n",
              "8                                                 104.000000      ...   \n",
              "9                                                 142.445199      ...   \n",
              "\n",
              "Data Item  SMPSell D30 roll std  Day of Week  Is Weekday  Next Day Is Weekday  \\\n",
              "5                      0.005142            2           1                    1   \n",
              "6                      0.011180            3           1                    1   \n",
              "7                      0.010249            4           1                    0   \n",
              "8                      0.009697            5           0                    0   \n",
              "9                      0.009489            6           0                    1   \n",
              "\n",
              "Data Item  Day of Year   sin_DoY   cos_DoY  Next Day SAP  Next Day SMPBuy  \\\n",
              "5                  127  0.816538 -0.577292        0.4834           0.5187   \n",
              "6                  128  0.806480 -0.591261        0.4756           0.5109   \n",
              "7                  129  0.796183 -0.605056        0.4722           0.5075   \n",
              "8                  130  0.785650 -0.618671        0.4615           0.4968   \n",
              "9                  131  0.774884 -0.632103        0.4569           0.4922   \n",
              "\n",
              "Data Item  Next Day SMPSell  \n",
              "5                    0.4481  \n",
              "6                    0.4403  \n",
              "7                    0.4369  \n",
              "8                    0.4262  \n",
              "9                    0.4216  \n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean(df, remove_outliers=True):\n",
        "    # fill missing CWV actuals with the normal forecast\n",
        "    df['Composite Weather Variable - Actual'] = df['Composite Weather Variable - Actual'].fillna(df['Composite Weather Variable - Normal'])\n",
        "\n",
        "    # There should be very remaining few rows that have any NaNs so we can drop any that do\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Can drop the composite weather forecasts\n",
        "    df.drop(columns=[\"Composite Weather Variable - Normal\", \"Composite Weather Variable - Cold\", \"Composite Weather Variable - Warm\"], inplace=True)\n",
        "\n",
        "    if(remove_outliers):\n",
        "        for col in label_cols:    \n",
        "            # remove outliers where any of the prices was 0\n",
        "            print(df.shape)\n",
        "            df = df[df[col] != 0]\n",
        "            print(df.shape)\n",
        "            df = df[df[f\"Next Day {col}\"] != 0]\n",
        "            print(df.shape)\n",
        "            #... and where the next day price is more than least 50% away from the current day's price\n",
        "            df = df[abs(df[col] - df[f\"Next Day {col}\"])/df[col] < 0.5]\n",
        "            print(df.shape)\n",
        "    return df    \n",
        "\n",
        "df = clean(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed_and_cleaned.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821e51d3",
      "metadata": {},
      "source": [
        "### Split the data into training and test sets\n",
        "Various ways were investigated:\n",
        "- Use the data before a cutoff date to train, and after that date to test. Designed to test whether the model will generalise to the most recent period, despite having been trained on earlier periods\n",
        "- Split the data 50/50 by date. Use all of the earlier half, and a random half of the later half, to train. Use the remaining half of the later half to test. Designed as a compromise between the above approach and a random split\n",
        "- Split the data randomly regardless of date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "f6118dc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def split_train_test_on_date(df, split_date, discard_before_date):\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into training set (gas days before split date) and test set (ga days fron the split date on)\n",
        "\n",
        "    :param df: The DataFrame to split.\n",
        "    :param split_date: The date to split the DataFrame on.\n",
        "    :param discard_before_date: Discard anything before this date. Added to exclude time of Covid lockdowns.\n",
        "    :return: Tuple of (training set, testing set).\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the DataFrame into training and testing sets\n",
        "    train_df = df[df['Gas Day'].between(discard_before_date, split_date, inclusive = \"neither\")]\n",
        "    test_df = df[df['Gas Day'] >= split_date]\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def split_with_test_half_of_last_half(df, discard_before_date):\n",
        "    \"\"\"\n",
        "    Use all the earlier half, and half the later half, to train\n",
        "    Use the other half of the later half to test\n",
        "    \"\"\"\n",
        "    df2 = df[df['Gas Day'] >= discard_before_date]\n",
        "    mid_date = df2['Gas Day'].mean()\n",
        "    first_half = df2[df2['Gas Day'] < mid_date]\n",
        "    second_half = df2[df2['Gas Day'] >= mid_date]\n",
        "    \n",
        "    train_df, test_df = train_test_split(second_half, test_size=0.5, shuffle=True)\n",
        "    train_df = pd.concat([first_half, train_df])\n",
        "    return train_df, test_df\n",
        "\n",
        "def n_train_n_test(df, n_train, n_test, discard_before_date):\n",
        "    \"\"\"Split based on number or fraction of rows\"\"\"\n",
        "    #df.to_csv(\"..\\\\data\\\\processed\\\\all.csv\", index=False)\n",
        "    df = df[df['Gas Day'] >= discard_before_date]\n",
        "    # Split the DataFrame into training and testing sets\n",
        "    train_df, test_df = train_test_split(df, test_size=n_test, train_size=n_train, shuffle=True)\n",
        "    #train_df.to_csv(\"..\\\\data\\\\processed\\\\train.csv\", index=False)\n",
        "    #test_df.to_csv(\"..\\\\data\\\\processed\\\\test.csv\", index=False)\n",
        "    return train_df, test_df\n",
        "\n",
        "def get_X(df):\n",
        "    ys = [\"Next Day \" + col for col in label_cols]\n",
        "    df2 = df.drop(columns=ys)\n",
        "    df2.drop(columns=[\"Gas Day\"], inplace=True)\n",
        "    # experimentally - just include the price time series columns\n",
        "    #for col in df2.columns.tolist():      # iterate over a copy of the column list\n",
        "    #    if not is_price_column(col):             # if the substring isnâ€™t found\n",
        "    #        df2.drop(columns=col, inplace=True)\n",
        "    #df2 = df2[label_cols]\n",
        "    #df2 = df2[[\"SAP\"]]\n",
        "    return df2\n",
        "\n",
        "#def is_price_column(column_name):\n",
        "#    if \"SMP\" in column_name or \"SAP\" in column_name:\n",
        "#        return True\n",
        "#    return False\n",
        "\n",
        "\n",
        "\n",
        "# split on date, or random proportions\n",
        "#train, test = split_train_test(df, '2024-10-01', '2021-03-01')\n",
        "#train, test = n_train_n_test(df, n_train=250, n_test=50, discard_before_date='2021-03-01')\n",
        "#train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "#train = train[train['Gas Day'] > '2021-03-01']\n",
        "#test = test[test['Gas Day'] > '2021-03-01']\n",
        "train, test = split_with_test_half_of_last_half(df, '2023-09-01')\n",
        "X_train = get_X(train)\n",
        "X_test = get_X(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aabed2",
      "metadata": {},
      "source": [
        "### Use Root Mean Squared Error as the measure of accuracy\n",
        "\n",
        "This is appropriate to price forecasting because it penalises larger inaccuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "9cbf2053",
      "metadata": {
        "id": "9cbf2053"
      },
      "outputs": [],
      "source": [
        "# Root mean squared error - penalises larger errors more than smaller ones\n",
        "def get_rmse(actuals, predictions):\n",
        "    rmse =  np.sqrt(np.mean((predictions - actuals)**2))\n",
        "    return round(rmse, 4)\n",
        "\n",
        "#Mean absolute percentage error\n",
        "def get_mape(actuals, predictions):\n",
        "    mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100\n",
        "    return round(mape, 4)\n",
        "\n",
        "\n",
        "def print_model_stats(model, X):\n",
        "\n",
        "    # 1. Coefficients and intercept\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        #print(\"Coefficients:\", model.coef_)      # array of shape (n_features,)\n",
        "        cdf = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])\n",
        "        cdf = cdf.sort_values(by='Coefficients', ascending=False)\n",
        "        print(cdf)\n",
        "    if hasattr(model, \"intercept_\"):\n",
        "        print(\"Intercept:\", model.intercept_)    # scalar (or array if multi-output)\n",
        "\n",
        "    # 2. Model parameters\n",
        "    print(\"Parameters:\", model.get_params())\n",
        "\n",
        "    # 3. Dataâ€related attributes\n",
        "    #print(\"Number of features seen during fit:\", model.n_features_in_)\n",
        "    #if hasattr(model, \"feature_names_in_\"):\n",
        "    #    print(\"Feature names:\", model.feature_names_in_)\n",
        "\n",
        "    # 4. Linear algebra internals (rarely needed)\n",
        "    if hasattr(model, \"rank_\"):\n",
        "        print(\"Rank of design matrix:\", model.rank_)\n",
        "    if hasattr(model, \"singular_\"):\n",
        "        print(\"Singular values of X:\", model.singular_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe56398",
      "metadata": {},
      "source": [
        "### Set up a framework to train models, and compare their performance on the test dataset against a naive predictor\n",
        "\n",
        "The naive predictor takes the current day's System Average Price and System Marginal Prices as the predictions for the next day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "8a0ec407",
      "metadata": {
        "id": "8a0ec407"
      },
      "outputs": [],
      "source": [
        "# Use the previous day's actual as a naive predictor\n",
        "\n",
        "def get_y(df, col):\n",
        "    return df[\"Next Day \" + col]\n",
        "\n",
        "def test_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    rmse = get_rmse(y, y_pred)\n",
        "    return rmse\n",
        "\n",
        "def train_and_test_model(model, df_train, df_test, col):\n",
        "    X_train = get_X(df_train)\n",
        "    X_test = get_X(df_test)\n",
        "    y_train = get_y(df_train, col)\n",
        "    y_test = get_y(df_test, col)\n",
        "    #scaler = StandardScaler()\n",
        "    #X_train_scaled = scaler.fit_transform(X_train)\n",
        "    #X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_train_scaled = X_train\n",
        "    X_test_scaled = X_test\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    rmse_train = test_model(model, X_train_scaled, y_train)\n",
        "    rmse_test = test_model(model, X_test_scaled, y_test)\n",
        "\n",
        "    return model, rmse_train, rmse_test\n",
        "\n",
        "def train_test_and_report_for_prices(model_factory, df_train, df_test, include_model_stats=True):\n",
        "    for col in label_cols:\n",
        "        # Instantiate model.\n",
        "        model = model_factory()\n",
        "\n",
        "        # Train and test it\n",
        "        model, rmse_train, rmse_test = train_and_test_model(model, df_train, df_test, col)\n",
        "\n",
        "        # Print model details\n",
        "        if include_model_stats:\n",
        "            X_train = get_X(df_train)\n",
        "            print_model_stats(model, X_train)\n",
        "\n",
        "        # Get naive prediction stats for comparison\n",
        "        rmse_naive_train, rmse_naive_test = naive_predictions(df_train, df_test, col)\n",
        "\n",
        "        print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
        "        print_results(col + \" test\", rmse_naive_test, rmse_test)\n",
        "\n",
        "def naive_predictions(df_train, df_test, col):\n",
        "    naive_predictions_train = df_train[col]\n",
        "    actuals_train = df_train[f\"Next Day {col}\"]\n",
        "    #mape_naive_train = get_mape(actuals_train, naive_predictions_train)\n",
        "    #print(f\"MAPE train (naive predictor) for {col}: {mape_naive_train}\")\n",
        "    rmse_naive_train = get_rmse(actuals_train, naive_predictions_train)\n",
        "    #print(f\"RMSE train (naive predictor) for {col}: {rmse_naive_train}\")\n",
        "\n",
        "    naive_predictions_test = df_test[col]\n",
        "    actuals_test = df_test[f\"Next Day {col}\"]\n",
        "    #mape_naive_test = get_mape(actuals_test, naive_predictions_test)\n",
        "    #print(f\"MAPE test (naive predictor) for {col}: {mape_naive_test}\")\n",
        "    rmse_naive_test = get_rmse(actuals_test, naive_predictions_test)\n",
        "    #print(f\"RMSE test (naive predictor) for {col}: {rmse_naive_test}\")\n",
        "    return rmse_naive_train, rmse_naive_test\n",
        "\n",
        "def print_results(case, rmse_naive, rmse_model):\n",
        "    headline = \"Worse\" if rmse_naive <= rmse_model else \"Better\"\n",
        "    print(f\"{case} - {headline} - model {rmse_model} v naive {rmse_naive}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cb01d0",
      "metadata": {},
      "source": [
        "### Try linear regression models\n",
        "\n",
        "...to predict each of SAP (System Average Price), SMPBuy (System Marginal Price - Buy) and SMPSell (System Marginal Price - Sell). This generally performs worse than the naive predictor in testing, especially using a date-based split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "39429681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39429681",
        "outputId": "9a1f327b-ccba-4a85-f298-0126ade5c540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression model:\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.3856 v naive 0.4819\n",
            "SAP test - Worse - model 0.3817 v naive 0.3699\n",
            "SMPBuy train - Better - model 0.4434 v naive 0.534\n",
            "SMPBuy test - Worse - model 0.521 v naive 0.4728\n",
            "SMPSell train - Better - model 0.5614 v naive 0.6358\n",
            "SMPSell test - Worse - model 0.5344 v naive 0.4612\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.4075 v naive 0.4965\n",
            "SAP test - Worse - model 0.6024 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.4878 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.8553 v naive 0.0966\n",
            "SMPSell train - Better - model 0.5906 v naive 0.6496\n",
            "SMPSell test - Worse - model 1.0567 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.434 v naive 0.529\n",
            "SAP test - Worse - model 0.1588 v naive 0.1274\n",
            "SMPBuy train - Better - model 0.5215 v naive 0.6017\n",
            "SMPBuy test - Worse - model 0.1892 v naive 0.1475\n",
            "SMPSell train - Better - model 0.6307 v naive 0.6935\n",
            "SMPSell test - Worse - model 0.1998 v naive 0.1449\n"
          ]
        }
      ],
      "source": [
        "print (\"Linear regression model:\")\n",
        "model_factory = lambda: LinearRegression()\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2dd9746",
      "metadata": {},
      "source": [
        "### Try a random forest model\n",
        "Linear regression generally performed worse than the naive predictor in testing, especially using a date-based split, so let's try a random forest model. The hyperparameters for the best version were obtained by random search in the second code block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc25ac1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc25ac1d",
        "outputId": "896fff9c-bc0c-4214-9d84-c79f832d572a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest model:\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.1844 v naive 0.4545\n",
            "SAP test - Better - model 0.4361 v naive 0.4623\n",
            "SMPBuy train - Better - model 0.2174 v naive 0.5183\n",
            "SMPBuy test - Better - model 0.4992 v naive 0.5225\n",
            "SMPSell train - Better - model 0.2703 v naive 0.5929\n",
            "SMPSell test - Worse - model 0.6279 v naive 0.6088\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.1958 v naive 0.4965\n",
            "SAP test - Worse - model 0.1021 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.2334 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.1236 v naive 0.0966\n",
            "SMPSell train - Better - model 0.2922 v naive 0.6496\n",
            "SMPSell test - Worse - model 0.1203 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.2096 v naive 0.5302\n",
            "SAP test - Worse - model 0.1186 v naive 0.1116\n",
            "SMPBuy train - Better - model 0.2452 v naive 0.6026\n",
            "SMPBuy test - Worse - model 0.1443 v naive 0.1366\n",
            "SMPSell train - Better - model 0.3105 v naive 0.6943\n",
            "SMPSell test - Worse - model 0.1375 v naive 0.1335\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "print (\"Random forest model:\")\n",
        "#RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0) # best from random searh\n",
        "#RandomForestRegressor(n_estimators = 200, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.7, max_depth = 20, ccp_alpha = 0.0) # best for SAP: SAP test - Better - model 0.77 v naive 0.8\n",
        "model_factory = lambda: RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0)\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51ccc4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51ccc4e",
        "outputId": "066a975b-5ff8-4b2a-b20d-d88898dce52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.7, 'max_depth': 10, 'ccp_alpha': 0.0}\n",
            "Best CV RMSE on train set: 0.4529\n",
            "SAP test - Worse - model 0.3753 v naive 0.3689\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1.0, 'max_depth': None, 'ccp_alpha': 0.0}\n",
            "Best CV RMSE on train set: 0.5374\n",
            "SMPBuy test - Worse - model 0.4247 v naive 0.4164\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best hyperparameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.5, 'max_depth': 20, 'ccp_alpha': 0.0}\n",
            "Best CV RMSE on train set: 0.6426\n",
            "SMPSell test - Worse - model 0.4858 v naive 0.4636\n"
          ]
        }
      ],
      "source": [
        "#Random search for hyperparameter tuning\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators':     [100, 200, 500],\n",
        "    'max_depth':        [None, 10, 20],\n",
        "    'min_samples_split':[2],\n",
        "    'min_samples_leaf': [2],\n",
        "    'max_features':     [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0],\n",
        "    'ccp_alpha':        [0.0, 0.001]\n",
        "}\n",
        "#param_grid = {\n",
        "#    'n_estimators':     [100, 200],\n",
        "#    'max_depth':        [None, 10, 20],\n",
        "#    'min_samples_split':[2, 5],\n",
        "#    'min_samples_leaf': [1, 2],\n",
        "#    'max_features':     ['sqrt'],\n",
        "#    'ccp_alpha':        [0.001, 0.01]\n",
        "#}\n",
        "X_train = get_X(train)\n",
        "for col in label_cols:\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        oob_score=True   # optional: get outâ€‘ofâ€‘bag score on your train set\n",
        "    )\n",
        "\n",
        "    #grid = GridSearchCV(\n",
        "    #    estimator=rf,\n",
        "    #    param_grid=param_grid,\n",
        "    #    cv=5,                             # 5â€‘fold CV on X_train/y_train\n",
        "    #    scoring='neg_root_mean_squared_error',\n",
        "    #    n_jobs=-1,\n",
        "    #    verbose=3\n",
        "    #)\n",
        "    rand_search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_grid,             \n",
        "        n_iter=50,                                  \n",
        "        cv=5,\n",
        "        scoring='neg_root_mean_squared_error',      \n",
        "        n_jobs=-1,\n",
        "        verbose=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    y_train = get_y(train, col)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best hyperparameters:\", rand_search.best_params_)\n",
        "    print(\"Best CV RMSE on train set: {:.4f}\".format(-rand_search.best_score_))\n",
        "\n",
        "\n",
        "    X_test = get_X(test)\n",
        "    y_test = get_y(test, col)\n",
        "    # -----------------------------------------------------------------------------\n",
        "    # 5. Evaluate the best model on the TEST set\n",
        "    # -----------------------------------------------------------------------------\n",
        "    best_model = rand_search.best_estimator_\n",
        "\n",
        "    rmse_test = test_model(best_model, X_test, y_test)\n",
        "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)\n",
        "\n",
        "    print_results(col + \" test\", rmse_naive_test, rmse_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1SNRfaoGOZKc",
      "metadata": {
        "id": "1SNRfaoGOZKc"
      },
      "source": [
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1.0, 'max_depth': 20, 'ccp_alpha': 0.0}\n",
        "Best CV RMSE on train set: 0.7879\n",
        "SAP test - Worse - model 0.78 v naive 0.78\n",
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.7, 'max_depth': None, 'ccp_alpha': 0.001}\n",
        "Best CV RMSE on train set: 0.8540\n",
        "SMPBuy test - Worse - model 0.86 v naive 0.83\n",
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.001}\n",
        "Best CV RMSE on train set: 0.8906\n",
        "SMPSell test - Worse - model 0.9 v naive 0.83"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24774087",
      "metadata": {},
      "source": [
        "### Next, try gradient booster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "0cecf345",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting model (XGBoost XGBRegressor):\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.0242 v naive 0.4618\n",
            "SAP test - Worse - model 0.453 v naive 0.44\n",
            "SMPBuy train - Better - model 0.0277 v naive 0.5259\n",
            "SMPBuy test - Worse - model 0.5249 v naive 0.4991\n",
            "SMPSell train - Better - model 0.0298 v naive 0.6108\n",
            "SMPSell test - Worse - model 0.589 v naive 0.5532\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.0332 v naive 0.4965\n",
            "SAP test - Worse - model 0.116 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.0373 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.1506 v naive 0.0966\n",
            "SMPSell train - Better - model 0.0401 v naive 0.6496\n",
            "SMPSell test - Worse - model 0.1448 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.028 v naive 0.5301\n",
            "SAP test - Worse - model 0.1255 v naive 0.1134\n",
            "SMPBuy train - Better - model 0.033 v naive 0.6027\n",
            "SMPBuy test - Worse - model 0.1502 v naive 0.1348\n",
            "SMPSell train - Better - model 0.0322 v naive 0.6942\n",
            "SMPSell test - Worse - model 0.1484 v naive 0.1338\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "print(\"Gradient Boosting model (XGBoost XGBRegressor):\")\n",
        "\n",
        "model_factory = lambda: XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "train_test_and_report_for_prices(model_factory, train, test, include_model_stats=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d38f8a",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "def naive_for_tcn(y_seq, y_train, y_test, SEQ_LEN):\n",
        "    # total number of sequences\n",
        "    n_seq = len(y_seq)\n",
        "    # number of train sequences\n",
        "    n_train = len(y_train)\n",
        "\n",
        "    # map each sequence to its targetâ€™s original df index:\n",
        "    #   sequences start at df index [0..], target of seq i is df index i+SEQ_LEN\n",
        "    target_idx = np.arange(SEQ_LEN, SEQ_LEN + n_seq)\n",
        "\n",
        "    # split those indices into train / test\n",
        "    train_target_idx = target_idx[:n_train]\n",
        "    test_target_idx  = target_idx[n_train:]\n",
        "\n",
        "    # naive prediction uses â€œtodayâ€™s SAPâ€ = df['SAP'] at (target_idx-1)\n",
        "    naive_train = df['SAP'].iloc[train_target_idx - 1].values\n",
        "    naive_test  = df['SAP'].iloc[test_target_idx  - 1].values\n",
        "\n",
        "    # calculate RMSE\n",
        "    rmse_naive_train = np.sqrt(mean_squared_error(y_train, naive_train))\n",
        "    rmse_naive_test  = np.sqrt(mean_squared_error(y_test,  naive_test))\n",
        "    return rmse_naive_train, rmse_naive_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "2c2f8c25",
      "metadata": {
        "id": "2c2f8c25",
        "outputId": "38a0cf30-02ea-4a39-9b0f-0da59141f21b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['Next Day SAP', 'Next Day SMPBuy', 'Next Day SMPSell'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[66], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m TEST_SIZE  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# \u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m X \u001b[38;5;241m=\u001b[39m get_X(df)\n\u001b[0;32m     44\u001b[0m y \u001b[38;5;241m=\u001b[39m get_y(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# scale features\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[29], line 43\u001b[0m, in \u001b[0;36mget_X\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_X\u001b[39m(df):\n\u001b[0;32m     42\u001b[0m     ys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext Day \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m label_cols]\n\u001b[1;32m---> 43\u001b[0m     df2 \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mys)\n\u001b[0;32m     44\u001b[0m     df2\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGas Day\u001b[39m\u001b[38;5;124m\"\u001b[39m], inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# experimentally - just include the price time series columns\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;66;03m#for col in df2.columns.tolist():      # iterate over a copy of the column list\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m#    if not is_price_column(col):             # if the substring isnâ€™t found\u001b[39;00m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m#        df2.drop(columns=col, inplace=True)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;66;03m#df2 = df2[label_cols]\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m#df2 = df2[[\"SAP\"]]\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['Next Day SAP', 'Next Day SMPBuy', 'Next Day SMPSell'] not found in axis\""
          ]
        }
      ],
      "source": [
        "# 1. Install the TCN layer\n",
        "#!pip install keras-tcn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tcn import TCN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# For TCN, let it build up its own sequences.\n",
        "# Reload the data, don't preprocess, and clean without removing outliers\n",
        "df = load_data()\n",
        "df = clean(df, remove_outliers=False)\n",
        "\n",
        "# 2. Suppose you already have a DataFrame `df` with:\n",
        "#    - 'Gas Day' datetime index or column\n",
        "#    - features (lags, rolling stats, calendar encodings, â€¦)\n",
        "#    - target column 'price'\n",
        "\n",
        "# If 'Gas Day' is a column:\n",
        "# df['Gas Day'] = pd.to_datetime(df['Gas Day'])\n",
        "# df.set_index('Gas Day', inplace=True)\n",
        "\n",
        "# 3. Prepare sequences for TCN\n",
        "def make_sequences(X, y, seq_len):\n",
        "    Xs, ys = [], []\n",
        "    for i in range(len(X) - seq_len):\n",
        "        # positional slice for X\n",
        "        Xs.append(X.iloc[i : i + seq_len].values)\n",
        "        # positional lookup for y\n",
        "        ys.append(y.iloc[i + seq_len])\n",
        "    return np.stack(Xs), np.array(ys)\n",
        "# parameters\n",
        "SEQ_LEN    = 30          # e.g. use past 30 days to predict next-day price\n",
        "TEST_SIZE  = 0.3\n",
        "\n",
        "# \n",
        "\n",
        "X = get_X(df)\n",
        "y = get_y(df, \"SAP\")\n",
        "\n",
        "\n",
        "# scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
        "\n",
        "# build sequences\n",
        "X_seq, y_seq = make_sequences(X_scaled, y, SEQ_LEN)\n",
        "\n",
        "# train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_seq, y_seq, test_size=TEST_SIZE, shuffle=False\n",
        ")\n",
        "\n",
        "\n",
        "#naive_predictions_test = df_test[col]\n",
        "rmse_naive_train, rmse_naive_test = naive_for_tcn(y_seq, y_train, y_test, SEQ_LEN)\n",
        "\n",
        "# 4. Build a simple TCN model\n",
        "n_features = X_train.shape[2]\n",
        "\n",
        "inputs = Input(shape=(SEQ_LEN, n_features))\n",
        "# TCN defaults: 64 filters, kernel_size=3, 8 stacks with exponentially increasing dilation\n",
        "tcn_layer = TCN(return_sequences=False)(inputs)\n",
        "output    = Dense(1)(tcn_layer)\n",
        "\n",
        "model = Model(inputs, output)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=[RootMeanSquaredError()])\n",
        "model.summary()\n",
        "\n",
        "# 5. Train\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=20, #50\n",
        "    batch_size=16,\n",
        "    callbacks=[\n",
        "        # optional: early stopping\n",
        "        # tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 6. Evaluate & predict\n",
        "loss, rmse = model.evaluate(X_test, y_test)\n",
        "#y_pred = model.predict(X_test)\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "#print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
        "print_results(\"SAP\" + \" test\", rmse_naive_test, rmse_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44d0ce63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>Storage, Short Range, Maximum potential flow</th>\n",
              "      <th>Storage, Short Range, Stock Levels</th>\n",
              "      <th>System Entry Flows, National, Forecast</th>\n",
              "      <th>System Entry Flows, National, Physical</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>66.15110</td>\n",
              "      <td>135.26344</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>10.5824</td>\n",
              "      <td>268.090483</td>\n",
              "      <td>240.620483</td>\n",
              "      <td>160.371026</td>\n",
              "      <td>132.901026</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247.492971</td>\n",
              "      <td>253.383732</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0.863142</td>\n",
              "      <td>-0.504961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-05-02</td>\n",
              "      <td>58.78630</td>\n",
              "      <td>131.66283</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>11.3089</td>\n",
              "      <td>244.938804</td>\n",
              "      <td>217.103350</td>\n",
              "      <td>145.468324</td>\n",
              "      <td>117.632869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>208.688512</td>\n",
              "      <td>229.860588</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>0.854322</td>\n",
              "      <td>-0.519744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-03</td>\n",
              "      <td>56.55015</td>\n",
              "      <td>141.57363</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>11.6531</td>\n",
              "      <td>242.854588</td>\n",
              "      <td>214.663679</td>\n",
              "      <td>144.375923</td>\n",
              "      <td>116.185014</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>221.676044</td>\n",
              "      <td>252.336835</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>0.845249</td>\n",
              "      <td>-0.534373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-04</td>\n",
              "      <td>52.82721</td>\n",
              "      <td>152.87412</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>11.9252</td>\n",
              "      <td>242.098717</td>\n",
              "      <td>213.561444</td>\n",
              "      <td>144.746098</td>\n",
              "      <td>116.208825</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>206.768342</td>\n",
              "      <td>231.939225</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>0.835925</td>\n",
              "      <td>-0.548843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-05</td>\n",
              "      <td>62.21188</td>\n",
              "      <td>134.50813</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>11.4803</td>\n",
              "      <td>247.112422</td>\n",
              "      <td>218.209695</td>\n",
              "      <td>145.956044</td>\n",
              "      <td>117.053317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>201.921238</td>\n",
              "      <td>189.255071</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>0.826354</td>\n",
              "      <td>-0.563151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "0         2020-05-01                                 66.15110   \n",
              "1         2020-05-02                                 58.78630   \n",
              "2         2020-05-03                                 56.55015   \n",
              "3         2020-05-04                                 52.82721   \n",
              "4         2020-05-05                                 62.21188   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "0                                    135.26344   \n",
              "1                                    131.66283   \n",
              "2                                    141.57363   \n",
              "3                                    152.87412   \n",
              "4                                    134.50813   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "0                               201.41454                   201.41454   \n",
              "1                               190.44913                   190.44913   \n",
              "2                               198.12378                   198.12378   \n",
              "3                               205.70133                   205.70133   \n",
              "4                               196.72001                   196.72001   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "0                                      10.5824     268.090483   \n",
              "1                                      11.3089     244.938804   \n",
              "2                                      11.6531     242.854588   \n",
              "3                                      11.9252     242.098717   \n",
              "4                                      11.4803     247.112422   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "0                                                 240.620483       \n",
              "1                                                 217.103350       \n",
              "2                                                 214.663679       \n",
              "3                                                 213.561444       \n",
              "4                                                 218.209695       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "0             160.371026   \n",
              "1             145.468324   \n",
              "2             144.375923   \n",
              "3             144.746098   \n",
              "4             145.956044   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "0                                                 132.901026      ...   \n",
              "1                                                 117.632869      ...   \n",
              "2                                                 116.185014      ...   \n",
              "3                                                 116.208825      ...   \n",
              "4                                                 117.053317      ...   \n",
              "\n",
              "Data Item  Storage, Short Range, Maximum potential flow  \\\n",
              "0                                                   0.0   \n",
              "1                                                   0.0   \n",
              "2                                                   0.0   \n",
              "3                                                   0.0   \n",
              "4                                                   0.0   \n",
              "\n",
              "Data Item  Storage, Short Range, Stock Levels  \\\n",
              "0                                         0.0   \n",
              "1                                         0.0   \n",
              "2                                         0.0   \n",
              "3                                         0.0   \n",
              "4                                         0.0   \n",
              "\n",
              "Data Item  System Entry Flows, National, Forecast  \\\n",
              "0                                      247.492971   \n",
              "1                                      208.688512   \n",
              "2                                      221.676044   \n",
              "3                                      206.768342   \n",
              "4                                      201.921238   \n",
              "\n",
              "Data Item  System Entry Flows, National, Physical  Day of Week  Is Weekday  \\\n",
              "0                                      253.383732            4           1   \n",
              "1                                      229.860588            5           0   \n",
              "2                                      252.336835            6           0   \n",
              "3                                      231.939225            0           1   \n",
              "4                                      189.255071            1           1   \n",
              "\n",
              "Data Item  Next Day Is Weekday  Day of Year   sin_DoY   cos_DoY  \n",
              "0                            0          122  0.863142 -0.504961  \n",
              "1                            0          123  0.854322 -0.519744  \n",
              "2                            1          124  0.845249 -0.534373  \n",
              "3                            1          125  0.835925 -0.548843  \n",
              "4                            1          126  0.826354 -0.563151  \n",
              "\n",
              "[5 rows x 48 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = load_data()\n",
        "df = preprocess(df, add_lags=False, add_labels=False)\n",
        "df = clean(df, remove_outliers=False)\n",
        "df = df.sort_values('Gas Day').reset_index(drop=True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "aec8afd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[4.17000000e+00 3.66755521e+02 3.66624661e+02 8.73802500e+01\n",
            "  6.98598287e+01 1.20539677e+04 3.61734180e+02 2.13920000e+00\n",
            "  2.17770000e+00 2.10070000e+00 3.59189300e+01 1.83655700e+01\n",
            "  1.20539677e+04 3.64579380e+02 3.58993205e+02 5.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 9.00000000e+00]\n",
            " [4.14000000e+00 3.47255467e+02 3.48694237e+02 8.56777300e+01\n",
            "  6.75815802e+01 1.16608672e+04 3.64646208e+02 2.08810000e+00\n",
            "  2.12660000e+00 2.03020000e+00 2.14951600e+01 9.99965000e+00\n",
            "  1.16608672e+04 3.50538210e+02 3.39637023e+02 6.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.00000000e+01]\n",
            " [4.09000000e+00 3.27737849e+02 3.27989177e+02 7.43592000e+01\n",
            "  6.68779507e+01 1.15394594e+04 3.64945794e+02 2.22000000e+00\n",
            "  2.25850000e+00 2.08140000e+00 1.57294800e+01 8.73999000e+00\n",
            "  1.15394594e+04 3.29124762e+02 3.30097630e+02 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.10000000e+01]\n",
            " [4.02000000e+00 3.49945729e+02 3.49664137e+02 8.77308000e+01\n",
            "  6.68842573e+01 1.15405476e+04 3.65249599e+02 2.64340000e+00\n",
            "  2.68190000e+00 2.60490000e+00 1.36906200e+01 7.98583000e+00\n",
            "  1.15405476e+04 3.47528659e+02 3.66220217e+02 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.20000000e+01]\n",
            " [3.97000000e+00 3.54577635e+02 3.54486763e+02 9.71942800e+01\n",
            "  6.70555735e+01 1.15701073e+04 3.63461342e+02 2.33640000e+00\n",
            "  2.37490000e+00 2.29790000e+00 2.18329800e+01 1.13424000e+01\n",
            "  1.15701073e+04 3.51387899e+02 3.58531477e+02 2.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.30000000e+01]\n",
            " [3.93000000e+00 3.56607505e+02 3.58224325e+02 9.78483700e+01\n",
            "  6.65784349e+01 1.14877794e+04 3.66085003e+02 2.11230000e+00\n",
            "  2.15080000e+00 2.04730000e+00 1.38920800e+01 7.45175000e+00\n",
            "  1.14877794e+04 3.58222027e+02 3.50678093e+02 3.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.40000000e+01]\n",
            " [3.90000000e+00 3.62493365e+02 3.62540851e+02 9.37410700e+01\n",
            "  6.62520566e+01 1.14314645e+04 3.63551721e+02 2.00890000e+00\n",
            "  2.04740000e+00 1.97040000e+00 2.61194400e+01 1.07673300e+01\n",
            "  1.14314645e+04 3.59329283e+02 3.38615797e+02 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 1.50000000e+01]\n",
            " [3.87000000e+00 3.35298375e+02 3.36964726e+02 8.63254500e+01\n",
            "  6.46121796e+01 1.11485118e+04 3.67896900e+02 1.91360000e+00\n",
            "  1.95210000e+00 1.87510000e+00 5.44255000e+00 5.43397000e+00\n",
            "  1.11485118e+04 3.37909205e+02 3.44612182e+02 5.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.60000000e+01]\n",
            " [3.87000000e+00 3.36087534e+02 3.36361153e+02 8.11531800e+01\n",
            "  6.70993138e+01 1.15776545e+04 3.65913857e+02 1.83370000e+00\n",
            "  1.87220000e+00 1.70610000e+00 5.15844000e+00 5.15236000e+00\n",
            "  1.15776545e+04 3.31938403e+02 3.30271005e+02 6.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 1.70000000e+01]\n",
            " [3.88000000e+00 3.35182696e+02 3.35425200e+02 8.55667100e+01\n",
            "  6.97513269e+01 1.20352462e+04 3.64846007e+02 1.83520000e+00\n",
            "  1.87370000e+00 1.79670000e+00 6.62107000e+00 6.61413000e+00\n",
            "  1.20352462e+04 3.33560651e+02 3.35179002e+02 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.80000000e+01]\n",
            " [3.88000000e+00 3.26231875e+02 3.29472621e+02 7.68195400e+01\n",
            "  7.01272078e+01 1.21001026e+04 3.61576215e+02 1.82770000e+00\n",
            "  1.86620000e+00 1.78920000e+00 5.44013000e+00 5.43670000e+00\n",
            "  1.21001026e+04 3.24158206e+02 3.28448347e+02 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 1.90000000e+01]\n",
            " [3.91000000e+00 3.04174953e+02 3.11792254e+02 6.42670700e+01\n",
            "  7.22007299e+01 1.24578786e+04 3.61410881e+02 1.95460000e+00\n",
            "  1.99310000e+00 1.91610000e+00 1.26302200e+01 7.23942000e+00\n",
            "  1.24578786e+04 3.06372994e+02 2.86989635e+02 2.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.00000000e+01]\n",
            " [3.94000000e+00 3.05150137e+02 3.06071267e+02 7.54629400e+01\n",
            "  7.23728537e+01 1.24875777e+04 3.62659602e+02 1.95890000e+00\n",
            "  1.99740000e+00 1.92040000e+00 1.11270400e+01 6.32331000e+00\n",
            "  1.24875777e+04 3.04031057e+02 3.25966638e+02 3.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.10000000e+01]\n",
            " [3.97000000e+00 3.37742175e+02 3.40141162e+02 8.28053500e+01\n",
            "  7.17230443e+01 1.23754563e+04 3.58047958e+02 2.05210000e+00\n",
            "  2.09060000e+00 2.01320000e+00 1.28627800e+01 6.99654000e+00\n",
            "  1.23754563e+04 3.35471650e+02 3.61992324e+02 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 2.20000000e+01]\n",
            " [3.99000000e+00 3.52872998e+02 3.57209999e+02 8.14781200e+01\n",
            "  7.09038313e+01 1.22341051e+04 3.62484225e+02 1.98770000e+00\n",
            "  2.02620000e+00 1.94920000e+00 2.12828000e+01 5.51982000e+00\n",
            "  1.22341051e+04 3.56018694e+02 3.56165391e+02 5.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 2.30000000e+01]\n",
            " [4.02000000e+00 3.63607859e+02 3.66632318e+02 8.14781200e+01\n",
            "  6.99285938e+01 1.20658327e+04 3.62261846e+02 2.10250000e+00\n",
            "  2.14100000e+00 2.06400000e+00 4.34152200e+01 1.46448600e+01\n",
            "  1.20658327e+04 3.62324819e+02 3.76435586e+02 6.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 2.40000000e+01]\n",
            " [4.04000000e+00 3.68875772e+02 3.70882937e+02 8.14750400e+01\n",
            "  6.71795371e+01 1.15914966e+04 3.64335504e+02 1.97250000e+00\n",
            "  2.01100000e+00 1.93400000e+00 2.59839700e+01 1.13918200e+01\n",
            "  1.15914966e+04 3.70344217e+02 3.61573227e+02 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.50000000e+01]\n",
            " [4.06000000e+00 3.70377765e+02 3.71185999e+02 8.11217800e+01\n",
            "  6.55383834e+01 1.13083237e+04 3.63659205e+02 1.90690000e+00\n",
            "  1.94540000e+00 1.86840000e+00 2.40640000e+01 9.62020000e+00\n",
            "  1.13083237e+04 3.69349786e+02 3.79842328e+02 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.60000000e+01]\n",
            " [4.08000000e+00 3.22301865e+02 3.24227819e+02 7.66355800e+01\n",
            "  6.40194067e+01 1.10462318e+04 3.68454145e+02 1.79990000e+00\n",
            "  1.83840000e+00 1.75900000e+00 5.79152000e+00 5.08811000e+00\n",
            "  1.10462318e+04 3.27563747e+02 3.31523437e+02 2.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.70000000e+01]\n",
            " [4.09000000e+00 2.95281887e+02 2.97995132e+02 5.11899600e+01\n",
            "  6.40348872e+01 1.10489028e+04 3.64428372e+02 1.90660000e+00\n",
            "  1.94510000e+00 1.80840000e+00 4.82739000e+00 4.82041000e+00\n",
            "  1.10489028e+04 2.93723882e+02 2.89895116e+02 3.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 2.80000000e+01]\n",
            " [4.09000000e+00 3.04922320e+02 3.07770285e+02 6.62658200e+01\n",
            "  6.50668691e+01 1.12269662e+04 3.63548751e+02 1.81570000e+00\n",
            "  1.85420000e+00 1.75730000e+00 4.49033000e+00 4.48812000e+00\n",
            "  1.12269662e+04 3.05687915e+02 3.10879805e+02 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 2.90000000e+01]\n",
            " [4.08000000e+00 3.23408741e+02 3.24459662e+02 7.34770400e+01\n",
            "  6.68130613e+01 1.15282630e+04 3.59758971e+02 1.78190000e+00\n",
            "  1.82040000e+00 1.70950000e+00 4.80724000e+00 4.21462000e+00\n",
            "  1.15282630e+04 3.20864438e+02 3.19042032e+02 5.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.00000000e+01]\n",
            " [4.08000000e+00 3.39722176e+02 3.40147373e+02 7.04982000e+01\n",
            "  6.80283718e+01 1.17379588e+04 3.57439839e+02 1.82180000e+00\n",
            "  1.87670000e+00 1.78330000e+00 1.19486600e+01 8.25238000e+00\n",
            "  1.17379588e+04 3.35362195e+02 3.11777854e+02 6.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 3.10000000e+01]\n",
            " [4.10000000e+00 3.60185035e+02 3.62870888e+02 5.40639600e+01\n",
            "  6.77866278e+01 1.16962471e+04 3.57600789e+02 1.71160000e+00\n",
            "  1.75010000e+00 1.67310000e+00 2.49856200e+01 9.59788000e+00\n",
            "  1.16962471e+04 3.60972934e+02 3.82649431e+02 0.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.20000000e+01]\n",
            " [4.11000000e+00 3.00550439e+02 3.02652458e+02 3.81930500e+01\n",
            "  6.62093486e+01 1.14240954e+04 3.59467017e+02 1.73160000e+00\n",
            "  1.77010000e+00 1.69310000e+00 9.48152000e+00 4.39351000e+00\n",
            "  1.14240954e+04 3.01841125e+02 2.99246965e+02 1.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.30000000e+01]\n",
            " [4.12000000e+00 3.02014690e+02 3.02735270e+02 3.72718600e+01\n",
            "  6.58292863e+01 1.13585175e+04 3.58908731e+02 1.58670000e+00\n",
            "  1.62520000e+00 1.54820000e+00 4.30241000e+00 3.77109000e+00\n",
            "  1.13585175e+04 3.02301382e+02 3.13063905e+02 2.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.40000000e+01]\n",
            " [4.11000000e+00 3.09560410e+02 3.10780942e+02 3.59248400e+01\n",
            "  6.59882362e+01 1.13859435e+04 3.59156383e+02 1.73610000e+00\n",
            "  1.77460000e+00 1.69760000e+00 1.34109800e+01 7.02203000e+00\n",
            "  1.13859435e+04 3.11060641e+02 3.30734565e+02 3.00000000e+00\n",
            "  1.00000000e+00 1.00000000e+00 3.50000000e+01]\n",
            " [4.10000000e+00 2.92742051e+02 2.93801762e+02 3.72732300e+01\n",
            "  6.52576822e+01 1.12598901e+04 3.63731880e+02 1.73680000e+00\n",
            "  1.77530000e+00 1.66170000e+00 1.40740500e+01 5.29593000e+00\n",
            "  1.12598901e+04 2.97064294e+02 3.14839281e+02 4.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00 3.60000000e+01]\n",
            " [4.07000000e+00 2.92923322e+02 2.94356547e+02 3.41417300e+01\n",
            "  6.44272455e+01 1.11166023e+04 3.61677666e+02 1.61110000e+00\n",
            "  1.64960000e+00 1.57260000e+00 1.24538600e+01 5.84543000e+00\n",
            "  1.11166023e+04 2.92997462e+02 3.06313487e+02 5.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.70000000e+01]\n",
            " [4.05000000e+00 3.24005013e+02 3.23738252e+02 3.50589500e+01\n",
            "  6.45291544e+01 1.11341862e+04 3.54860317e+02 1.74750000e+00\n",
            "  1.97900000e+00 1.70900000e+00 2.67208100e+01 1.77126800e+01\n",
            "  1.11341862e+04 3.15783796e+02 2.84550230e+02 6.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00 3.80000000e+01]]\n",
            "1.9964\n",
            "Training records: 1244\n",
            "Validation records: 177\n",
            "Test records: 357\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mike\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">75,776</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              â”‚            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ],
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚        \u001b[38;5;34m75,776\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              â”‚            \u001b[38;5;34m33\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">127,297</span> (497.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m127,297\u001b[0m (497.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">127,297</span> (497.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m127,297\u001b[0m (497.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 85ms/step - loss: 14.5451 - rmse: 3.7498 - val_loss: 0.9086 - val_rmse: 0.9532 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 52ms/step - loss: 2.1525 - rmse: 1.4635 - val_loss: 0.4241 - val_rmse: 0.6512 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 1.4509 - rmse: 1.2001 - val_loss: 0.3594 - val_rmse: 0.5995 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 46ms/step - loss: 1.1765 - rmse: 1.0798 - val_loss: 0.2185 - val_rmse: 0.4675 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.7302 - rmse: 0.8527 - val_loss: 0.4708 - val_rmse: 0.6862 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 73ms/step - loss: 0.7415 - rmse: 0.8602 - val_loss: 0.2664 - val_rmse: 0.5162 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 55ms/step - loss: 0.7817 - rmse: 0.8800 - val_loss: 0.2329 - val_rmse: 0.4826 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 55ms/step - loss: 0.5743 - rmse: 0.7571 - val_loss: 0.4734 - val_rmse: 0.6880 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 54ms/step - loss: 0.5577 - rmse: 0.7460 - val_loss: 0.2053 - val_rmse: 0.4531 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - loss: 0.5960 - rmse: 0.7712 - val_loss: 0.2533 - val_rmse: 0.5033 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - loss: 0.4918 - rmse: 0.7011 - val_loss: 0.2857 - val_rmse: 0.5345 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 73ms/step - loss: 0.5307 - rmse: 0.7279 - val_loss: 0.1878 - val_rmse: 0.4334 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.5749 - rmse: 0.7560 - val_loss: 0.2138 - val_rmse: 0.4623 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.5029 - rmse: 0.7082 - val_loss: 0.1870 - val_rmse: 0.4325 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 61ms/step - loss: 0.4945 - rmse: 0.7025 - val_loss: 0.5914 - val_rmse: 0.7691 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 64ms/step - loss: 0.5043 - rmse: 0.7091 - val_loss: 0.4376 - val_rmse: 0.6615 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - loss: 0.4426 - rmse: 0.6635 - val_loss: 0.2700 - val_rmse: 0.5196 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - loss: 0.4885 - rmse: 0.6984 - val_loss: 0.2504 - val_rmse: 0.5004 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - loss: 0.4774 - rmse: 0.6895 - val_loss: 0.2385 - val_rmse: 0.4884 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 92ms/step - loss: 0.4897 - rmse: 0.6983 - val_loss: 0.2277 - val_rmse: 0.4772 - learning_rate: 5.0000e-04\n",
            "Epoch 21/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 84ms/step - loss: 0.4149 - rmse: 0.6439 - val_loss: 0.2279 - val_rmse: 0.4774 - learning_rate: 5.0000e-04\n",
            "Epoch 22/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - loss: 0.3486 - rmse: 0.5891 - val_loss: 0.2428 - val_rmse: 0.4928 - learning_rate: 5.0000e-04\n",
            "Epoch 23/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - loss: 0.4028 - rmse: 0.6342 - val_loss: 0.2580 - val_rmse: 0.5079 - learning_rate: 5.0000e-04\n",
            "Epoch 24/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 70ms/step - loss: 0.3981 - rmse: 0.6295 - val_loss: 0.2954 - val_rmse: 0.5435 - learning_rate: 5.0000e-04\n",
            "\u001b[1m12/12\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.0911 - rmse: 0.2804\n",
            "Model RMSE: 0.4034\n",
            "NaÃ¯ve â€˜tomorrow = todayâ€™ RMSE: 3.6099\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "WINDOW_SIZE = 30\n",
        "\n",
        "def make_sequences(df, feature_cols, target_col):\n",
        "    X, y = [], []\n",
        "    for i in range(len(df) - WINDOW_SIZE):\n",
        "        seq = df[feature_cols].iloc[i : i + WINDOW_SIZE].values\n",
        "        X.append(seq)\n",
        "        y.append(df[target_col].iloc[i + WINDOW_SIZE])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "feature_cols = ['Composite Weather Variable - Actual', 'Demand Actual, NTS, D+1', 'Demand Forecast, NTS, hourly update', 'Interconnector - Daily Flow', 'Medium Storage - Actual Stock',\n",
        "              'Medium Storage - Stock Level at Max Flow', 'Predicted Closing Linepack (PCLP1)', \n",
        "              'SAP', 'SMPBuy',\t'SMPSell', \n",
        "              'Storage - Daily Flow','Storage - Delivery', 'Storage, Medium Range, Stock Levels', 'System Entry Flows, National, Forecast', 'System Entry Flows, National, Physical',\n",
        "              'Day of Week','Is Weekday','Next Day Is Weekday','Day of Year']\n",
        "\n",
        "\n",
        "target_col  = 'SAP'\n",
        "X, y = make_sequences(df, feature_cols, target_col)\n",
        "\n",
        "print(X[250])\n",
        "print(y[250])\n",
        "\n",
        "train_size = int(0.7 * len(X))\n",
        "val_size   = int(0.1 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val,   y_val   = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test,  y_test  = X[train_size+val_size:], y[train_size+val_size:]\n",
        "print(f\"Training records: {X_train.shape[0]}\")\n",
        "print(f\"Validation records: {X_val.shape[0]}\")\n",
        "print(f\"Test records: {X_test.shape[0]}\")\n",
        "\n",
        "#scaling\n",
        "\n",
        "n_feats = X_train.shape[2]\n",
        "scaler = StandardScaler()\n",
        "X_train_2d = X_train.reshape(-1, n_feats)\n",
        "scaler.fit(X_train_2d)\n",
        "\n",
        "def scale_split(X):\n",
        "    X_2d = X.reshape(-1, n_feats)\n",
        "    Xs = scaler.transform(X_2d)\n",
        "    return Xs.reshape(-1, WINDOW_SIZE, n_feats)\n",
        "\n",
        "X_train = scale_split(X_train)\n",
        "X_val   = scale_split(X_val)\n",
        "X_test  = scale_split(X_test)\n",
        "\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    # First LSTM layer returns full sequences so you can stack\n",
        "    layers.LSTM(128, \n",
        "                return_sequences=True, \n",
        "                input_shape=(WINDOW_SIZE, n_feats)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Second LSTM layer (doesn't return sequences)\n",
        "    layers.LSTM(64),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # A small dense â€œheadâ€ to predict next price\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(1)  # single real-valued output\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=5\n",
        "    )\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# 5. Evaluate model RMSE on test set\n",
        "eval_results = model.evaluate(X_test, y_test, batch_size=32, return_dict=True)\n",
        "model_rmse = eval_results['rmse']\n",
        "\n",
        "# 6. Compute naÃ¯ve â€œtomorrow = todayâ€ RMSE\n",
        "#    we know 'SAP' is at index:\n",
        "target_idx = feature_cols.index(target_col)\n",
        "#    take last timestep of SAP from each test sequence:\n",
        "y_pred_naive = X_test[:, -1, target_idx]\n",
        "naive_rmse = np.sqrt(np.mean((y_test - y_pred_naive)**2))\n",
        "\n",
        "# 7. Print comparison\n",
        "print(f\"Model RMSE: {model_rmse:.4f}\")\n",
        "print(f\"NaÃ¯ve â€˜tomorrow = todayâ€™ RMSE: {naive_rmse:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
