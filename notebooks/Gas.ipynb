{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "9c3764d3",
      "metadata": {
        "id": "9c3764d3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557a2bba",
      "metadata": {},
      "source": [
        "Initial steps if running on Google Colab, to download support files from GitHub and set the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "vsukzbcUFNAQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsukzbcUFNAQ",
        "outputId": "d014ee2a-a929-41f8-94f9-5b56a384a2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/gas-forecast/notebooks'\n",
            "d:\\dev\\gas-forecast\\notebooks\n"
          ]
        }
      ],
      "source": [
        "#for running on Colab\n",
        "#!git clone https://github.com/MBWestcott/gas-forecast.git\n",
        "\n",
        "# 2. Change into the repo directory\n",
        "%cd /content/gas-forecast/notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4cfcb1",
      "metadata": {},
      "source": [
        "### First download the raw data from the National Gas data portal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "69f4bd0f",
      "metadata": {
        "id": "69f4bd0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "raw_data_folder = Path(\"../data/raw/\")\n",
        "\n",
        "def download_csv(url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a CSV file from the given URL and saves it to the specified file.\n",
        "\n",
        "    :param url: URL to download the CSV data from.\n",
        "    :param output_file: Path to the local file where the CSV will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure we notice bad responses\n",
        "\n",
        "        # Write the content (CSV data) to a file in binary mode\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"CSV file has been successfully downloaded and saved as '{output_file}'.\")\n",
        "\n",
        "    except requests.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"An error occurred: {err}\")\n",
        "\n",
        "\n",
        "def download_raw_data():\n",
        "    pubIdsFile = Path(\"../PUB ids.txt\")\n",
        "    with open(pubIdsFile) as f:\n",
        "        pubIds = f.read()\n",
        "        pubIds = pubIds.replace(\"\\n\", \",\").strip()\n",
        "\n",
        "    earliest = datetime.date(2020,4,1) # Download data going back 5 years\n",
        "    \n",
        "    download_from = datetime.date.today().replace(day=1) # start first download on first day of current month\n",
        "    download_to = datetime.date.today() # end first download on today's date\n",
        "    while(download_from > earliest):\n",
        "\n",
        "        # Format the date in yyyy-mm-dd format\n",
        "        formatted_from = download_from.strftime(\"%Y-%m-%d\")\n",
        "        formatted_to = download_to.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        csv_url = f\"https://data.nationalgas.com/api/find-gas-data-download?applicableFor=Y&dateFrom={formatted_from}&dateTo={formatted_to}&dateType=GASDAY&latestFlag=Y&ids={pubIds}&type=CSV\"\n",
        "        month_format = download_from.strftime(\"%Y-%m\")\n",
        "        output_filename = raw_data_folder /  f\"{month_format}.csv\"\n",
        "\n",
        "        download_csv(csv_url, output_filename)\n",
        "        time.sleep(2) # brief courtesy sleep\n",
        "        download_to = download_from - datetime.timedelta(days=1) # next download should go up to the day before the previous download start date\n",
        "        download_from = download_to.replace(day=1) # next download should start on the first day of the month\n",
        "\n",
        "# Do the download if the raw data is not there already\n",
        "csvCount = sum(1 for f in raw_data_folder.iterdir() if f.is_file() and f.suffix == '.csv')\n",
        "if(csvCount < 60):\n",
        "    download_raw_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a322d6",
      "metadata": {},
      "source": [
        "### Load the raw data\n",
        "\n",
        "Load the raw CSVs into a single and dataframe, pivot it so that each column represents a feature.\n",
        "Rename the Applicable At date field to Gas Day, and rename the columns that are going to be reused for ground truth and time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "dd682eab",
      "metadata": {
        "id": "dd682eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1819 entries, 0 to 22\n",
            "Data columns (total 45 columns):\n",
            " #   Column                                                    Non-Null Count  Dtype         \n",
            "---  ------                                                    --------------  -----         \n",
            " 0   Gas Day                                                   1819 non-null   datetime64[ns]\n",
            " 1   Aggregate LNG Importations - Daily Flow                   1816 non-null   float64       \n",
            " 2   Beach Including Norway - Daily Flow                       1816 non-null   float64       \n",
            " 3   Beach and IOG - Beach Delivery                            1815 non-null   float64       \n",
            " 4   Beach and IOG - Daily Flow                                1816 non-null   float64       \n",
            " 5   Composite Weather Variable - Actual                       1554 non-null   float64       \n",
            " 6   Composite Weather Variable - Cold                         1818 non-null   float64       \n",
            " 7   Composite Weather Variable - Normal                       1818 non-null   float64       \n",
            " 8   Composite Weather Variable - Warm                         1817 non-null   float64       \n",
            " 9   Demand - Cold                                             1817 non-null   float64       \n",
            " 10  Demand - Cold, (excluding interconnector and storage)     1817 non-null   float64       \n",
            " 11  Demand - Warm                                             1817 non-null   float64       \n",
            " 12  Demand - Warm, (excluding interconnector and storage)     1817 non-null   float64       \n",
            " 13  Demand Actual, NTS, D+1                                   1817 non-null   float64       \n",
            " 14  Demand Forecast, NTS, hourly update                       1819 non-null   float64       \n",
            " 15  Demand, NTS, SND                                          1817 non-null   float64       \n",
            " 16  Demand, NTS, SND, (excluding interconnector and storage)  1817 non-null   float64       \n",
            " 17  Interconnector - Daily Flow                               1812 non-null   float64       \n",
            " 18  Interconnector - Delivery                                 1812 non-null   float64       \n",
            " 19  LNG Stock Level                                           1818 non-null   float64       \n",
            " 20  Long Storage - Actual Stock                               1817 non-null   float64       \n",
            " 21  Long Storage - Stock Level at Max Flow                    1819 non-null   float64       \n",
            " 22  Medium Storage - Actual Stock                             1817 non-null   float64       \n",
            " 23  Medium Storage - Stock Level at Max Flow                  1819 non-null   float64       \n",
            " 24  Predicted Closing Linepack (PCLP1)                        1819 non-null   float64       \n",
            " 25  SAP, 30 day rolling average                               1816 non-null   float64       \n",
            " 26  SAP, 7 Day rolling average                                1816 non-null   float64       \n",
            " 27  SAP                                                       1816 non-null   float64       \n",
            " 28  SMPBuy                                                    1816 non-null   float64       \n",
            " 29  SMPSell                                                   1816 non-null   float64       \n",
            " 30  Short Storage - Actual Stock                              1817 non-null   float64       \n",
            " 31  Short Storage - Stock Level at Max Flow                   1819 non-null   float64       \n",
            " 32  Storage - Daily Flow                                      1816 non-null   float64       \n",
            " 33  Storage - Delivery                                        1815 non-null   float64       \n",
            " 34  Storage, Long Range, Average flow (7 days)                1817 non-null   float64       \n",
            " 35  Storage, Long Range, Maximum potential flow               1817 non-null   float64       \n",
            " 36  Storage, Long Range, Stock Levels                         1817 non-null   float64       \n",
            " 37  Storage, Medium Range, Average flow (7 days)              1817 non-null   float64       \n",
            " 38  Storage, Medium Range, Maximum potential flow             1817 non-null   float64       \n",
            " 39  Storage, Medium Range, Stock Levels                       1817 non-null   float64       \n",
            " 40  Storage, Short Range, Average flow (7 days)               1817 non-null   float64       \n",
            " 41  Storage, Short Range, Maximum potential flow              1817 non-null   float64       \n",
            " 42  Storage, Short Range, Stock Levels                        1817 non-null   float64       \n",
            " 43  System Entry Flows, National, Forecast                    1819 non-null   float64       \n",
            " 44  System Entry Flows, National, Physical                    1819 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(44)\n",
            "memory usage: 653.7 KB\n"
          ]
        }
      ],
      "source": [
        "label_cols = [\"SAP\", \"SMPBuy\", \"SMPSell\"]\n",
        "\n",
        "def pivot(df : pd.DataFrame, cols):\n",
        "\n",
        "    #only keep the values we are interested in\n",
        "    mask = df[\"Data Item\"].isin(cols)\n",
        "\n",
        "    df_filtered = df[mask]\n",
        "\n",
        "    # if there are duplicates for the field and gas day, take the latest\n",
        "    df_latest = (\n",
        "        df_filtered\n",
        "        .sort_values(\"Applicable At\")\n",
        "        .groupby([\"Gas Day\", \"Data Item\"])\n",
        "        .last()  # this takes the row with the highest (i.e. latest) \"Applicable At\" per group\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # pivot to get 1 row per gas day\n",
        "    df_latest = df_latest.pivot(index=\"Gas Day\", columns=\"Data Item\", values=\"Value\").reset_index()\n",
        "\n",
        "    # Drop 1 column that accounts for most of the NaNs\n",
        "    #df_latest.drop(columns=[\"Composite Weather Variable - Actual\"], inplace=True)\n",
        "\n",
        "    return df_latest\n",
        "\n",
        "def load_data():\n",
        "    #Read raw CSVs\n",
        "    pathlist = list(Path(raw_data_folder).rglob('*.csv'))\n",
        "    file_count = len(pathlist)\n",
        "    dfs = []\n",
        "    files_done = 0\n",
        "    for path_obj in pathlist:\n",
        "        path = str(path_obj)\n",
        "\n",
        "        df = pd.read_csv(path,\n",
        "            parse_dates=[\"Applicable At\", \"Applicable For\", \"Generated Time\"],\n",
        "            dayfirst=True)\n",
        "\n",
        "        df.rename(columns={'Applicable For': 'Gas Day'}, inplace=True)\n",
        "        df['Gas Day'] = pd.to_datetime(df['Gas Day'], dayfirst=True)\n",
        "        # pivot to 1 row per gas day, with features as columns\n",
        "\n",
        "        daily_cols = df[\"Data Item\"].unique()\n",
        "\n",
        "        df_daily = pivot(df, daily_cols)\n",
        "        dfs.append(df_daily)\n",
        "\n",
        "        files_done += 1\n",
        "        if files_done % 10 == 0:\n",
        "            print(f\"Processed {files_done} of {file_count} raw files\")\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    #Rename the columns that are going to be reused for ground truth and time series\n",
        "    df.rename(columns={\"SAP, Actual Day\": 'SAP', \"SMP Buy, Actual Day\": 'SMPBuy', \"SMP Sell, Actual Day\": 'SMPSell'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "df.to_csv(Path(\"../data/processed/pivoted.csv\"), index=False)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c8e618",
      "metadata": {},
      "source": [
        "### Preprocess data\n",
        "\n",
        "Add the previous 5 days' prices as lag features, and 7- and 30-day rolling averages and standard deviations. Also add day of week features, and a cyclical coding of the day of year for seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "id": "267c8bfe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Composite Weather Variable - Cold</th>\n",
              "      <th>Composite Weather Variable - Normal</th>\n",
              "      <th>Composite Weather Variable - Warm</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>...</th>\n",
              "      <th>SMPSell D30 roll std</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "      <th>Next Day SAP</th>\n",
              "      <th>Next Day SMPBuy</th>\n",
              "      <th>Next Day SMPSell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>66.15110</td>\n",
              "      <td>135.26344</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>10.5824</td>\n",
              "      <td>7.85</td>\n",
              "      <td>11.36</td>\n",
              "      <td>14.94</td>\n",
              "      <td>268.090483</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0.863142</td>\n",
              "      <td>-0.504961</td>\n",
              "      <td>0.4770</td>\n",
              "      <td>0.5123</td>\n",
              "      <td>0.4417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-05-02</td>\n",
              "      <td>58.78630</td>\n",
              "      <td>131.66283</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>11.3089</td>\n",
              "      <td>7.99</td>\n",
              "      <td>11.47</td>\n",
              "      <td>15.02</td>\n",
              "      <td>244.938804</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>0.854322</td>\n",
              "      <td>-0.519744</td>\n",
              "      <td>0.4840</td>\n",
              "      <td>0.5193</td>\n",
              "      <td>0.4487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-03</td>\n",
              "      <td>56.55015</td>\n",
              "      <td>141.57363</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>11.6531</td>\n",
              "      <td>8.13</td>\n",
              "      <td>11.55</td>\n",
              "      <td>15.09</td>\n",
              "      <td>242.854588</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003748</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>0.845249</td>\n",
              "      <td>-0.534373</td>\n",
              "      <td>0.4720</td>\n",
              "      <td>0.5073</td>\n",
              "      <td>0.4367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-04</td>\n",
              "      <td>52.82721</td>\n",
              "      <td>152.87412</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>11.9252</td>\n",
              "      <td>8.26</td>\n",
              "      <td>11.65</td>\n",
              "      <td>15.14</td>\n",
              "      <td>242.098717</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>0.835925</td>\n",
              "      <td>-0.548843</td>\n",
              "      <td>0.4790</td>\n",
              "      <td>0.5143</td>\n",
              "      <td>0.4437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-05</td>\n",
              "      <td>62.21188</td>\n",
              "      <td>134.50813</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>11.4803</td>\n",
              "      <td>8.40</td>\n",
              "      <td>11.76</td>\n",
              "      <td>15.18</td>\n",
              "      <td>247.112422</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005755</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>0.826354</td>\n",
              "      <td>-0.563151</td>\n",
              "      <td>0.5017</td>\n",
              "      <td>0.5370</td>\n",
              "      <td>0.4664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "0         2020-05-01                                 66.15110   \n",
              "1         2020-05-02                                 58.78630   \n",
              "2         2020-05-03                                 56.55015   \n",
              "3         2020-05-04                                 52.82721   \n",
              "4         2020-05-05                                 62.21188   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "0                                    135.26344   \n",
              "1                                    131.66283   \n",
              "2                                    141.57363   \n",
              "3                                    152.87412   \n",
              "4                                    134.50813   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "0                               201.41454                   201.41454   \n",
              "1                               190.44913                   190.44913   \n",
              "2                               198.12378                   198.12378   \n",
              "3                               205.70133                   205.70133   \n",
              "4                               196.72001                   196.72001   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  \\\n",
              "0                                      10.5824   \n",
              "1                                      11.3089   \n",
              "2                                      11.6531   \n",
              "3                                      11.9252   \n",
              "4                                      11.4803   \n",
              "\n",
              "Data Item  Composite Weather Variable - Cold  \\\n",
              "0                                       7.85   \n",
              "1                                       7.99   \n",
              "2                                       8.13   \n",
              "3                                       8.26   \n",
              "4                                       8.40   \n",
              "\n",
              "Data Item  Composite Weather Variable - Normal  \\\n",
              "0                                        11.36   \n",
              "1                                        11.47   \n",
              "2                                        11.55   \n",
              "3                                        11.65   \n",
              "4                                        11.76   \n",
              "\n",
              "Data Item  Composite Weather Variable - Warm  Demand - Cold  ...  \\\n",
              "0                                      14.94     268.090483  ...   \n",
              "1                                      15.02     244.938804  ...   \n",
              "2                                      15.09     242.854588  ...   \n",
              "3                                      15.14     242.098717  ...   \n",
              "4                                      15.18     247.112422  ...   \n",
              "\n",
              "Data Item  SMPSell D30 roll std  Day of Week  Is Weekday  Next Day Is Weekday  \\\n",
              "0                           NaN            4           1                    0   \n",
              "1                           NaN            5           0                    0   \n",
              "2                      0.003748            6           0                    1   \n",
              "3                      0.006170            0           1                    1   \n",
              "4                      0.005755            1           1                    1   \n",
              "\n",
              "Data Item  Day of Year   sin_DoY   cos_DoY  Next Day SAP  Next Day SMPBuy  \\\n",
              "0                  122  0.863142 -0.504961        0.4770           0.5123   \n",
              "1                  123  0.854322 -0.519744        0.4840           0.5193   \n",
              "2                  124  0.845249 -0.534373        0.4720           0.5073   \n",
              "3                  125  0.835925 -0.548843        0.4790           0.5143   \n",
              "4                  126  0.826354 -0.563151        0.5017           0.5370   \n",
              "\n",
              "Data Item  Next Day SMPSell  \n",
              "0                    0.4417  \n",
              "1                    0.4487  \n",
              "2                    0.4367  \n",
              "3                    0.4437  \n",
              "4                    0.4664  \n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess(df: pd.DataFrame, add_lags=True, add_labels=True):\n",
        "\n",
        "    \"\"\"Deal with missing values, add lagged features, rolling averages and stds, Day of Week, and cyclic encoding for seasonality\"\"\"\n",
        "\n",
        "    if add_lags:\n",
        "        lag_days = 5\n",
        "        for i in range(1, lag_days+1):\n",
        "            for col in label_cols:\n",
        "                df[f\"{col} D-{i}\"] = df[col].shift(i)\n",
        "\n",
        "        # add rolling averages and stds\n",
        "        for col in label_cols:\n",
        "            for window in [7, 30]:\n",
        "                df[f'{col} D{window} roll mean'] = (\n",
        "                    df[col]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .mean()\n",
        "                    )\n",
        "                df[f'{col} D{window} roll std'] = (\n",
        "                    df[col]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .std()\n",
        "                )\n",
        "\n",
        "    # add day of week\n",
        "    df['Day of Week'] = df['Gas Day'].dt.weekday\n",
        "    df['Is Weekday'] = (df['Gas Day'].dt.weekday < 5).astype(int)\n",
        "    df['Next Day Is Weekday'] = ((df['Gas Day'] + pd.Timedelta(days=1)).dt.weekday < 5).astype(int)\n",
        "    # cyclic encoding for seasonality\n",
        "    df['Day of Year'] = df['Gas Day'].dt.dayofyear\n",
        "    df['sin_DoY'] = np.sin(2 * np.pi * df['Day of Year'] / 365)\n",
        "    df['cos_DoY'] = np.cos(2 * np.pi * df['Day of Year'] / 365)\n",
        "\n",
        "    if add_labels:\n",
        "        # Add labels for next day's actuals\n",
        "        for col in label_cols:\n",
        "            df[f\"Next Day {col}\"] = df[col].shift(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3358d8",
      "metadata": {},
      "source": [
        "### Clean missing values and outliers\n",
        "\n",
        "Most of the missing values are missing \"Composite Weather Variable - Actual\" from 2020-21. These affect around 15% of the dataset. Best way to fill in those is with the Normal forecast, which should usually be the closest. Apart from that there are very few missing readings so it is feasible to discard any remaining rows with missing data (done at the end, to avoid introducing errors into the lag features)\n",
        "\n",
        "Also remove outliers where any of the prices was 0, and one of the next day prices was more 50% away from the current day's price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "c9c69e5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1784, 78)\n",
            "(1784, 78)\n",
            "(1783, 78)\n",
            "(1783, 78)\n",
            "(1762, 78)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>SMPSell D30 roll std</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "      <th>Next Day SAP</th>\n",
              "      <th>Next Day SMPBuy</th>\n",
              "      <th>Next Day SMPSell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-05-06</td>\n",
              "      <td>50.23730</td>\n",
              "      <td>142.20118</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>12.0645</td>\n",
              "      <td>245.399580</td>\n",
              "      <td>216.150490</td>\n",
              "      <td>145.432835</td>\n",
              "      <td>116.183744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005142</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>0.816538</td>\n",
              "      <td>-0.577292</td>\n",
              "      <td>0.4834</td>\n",
              "      <td>0.5187</td>\n",
              "      <td>0.4481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-05-07</td>\n",
              "      <td>53.59770</td>\n",
              "      <td>141.87295</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>13.4655</td>\n",
              "      <td>243.927941</td>\n",
              "      <td>214.341578</td>\n",
              "      <td>145.153439</td>\n",
              "      <td>115.567076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011180</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.806480</td>\n",
              "      <td>-0.591261</td>\n",
              "      <td>0.4756</td>\n",
              "      <td>0.5109</td>\n",
              "      <td>0.4403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-05-08</td>\n",
              "      <td>51.08822</td>\n",
              "      <td>135.19872</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>15.5400</td>\n",
              "      <td>171.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010249</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0.796183</td>\n",
              "      <td>-0.605056</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.5075</td>\n",
              "      <td>0.4369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-05-09</td>\n",
              "      <td>53.28634</td>\n",
              "      <td>127.04213</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>15.0800</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009697</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0.785650</td>\n",
              "      <td>-0.618671</td>\n",
              "      <td>0.4615</td>\n",
              "      <td>0.4968</td>\n",
              "      <td>0.4262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-05-10</td>\n",
              "      <td>53.14522</td>\n",
              "      <td>127.29315</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>12.6200</td>\n",
              "      <td>264.115666</td>\n",
              "      <td>226.223933</td>\n",
              "      <td>180.336933</td>\n",
              "      <td>142.445199</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009489</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>0.774884</td>\n",
              "      <td>-0.632103</td>\n",
              "      <td>0.4569</td>\n",
              "      <td>0.4922</td>\n",
              "      <td>0.4216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "5         2020-05-06                                 50.23730   \n",
              "6         2020-05-07                                 53.59770   \n",
              "7         2020-05-08                                 51.08822   \n",
              "8         2020-05-09                                 53.28634   \n",
              "9         2020-05-10                                 53.14522   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "5                                    142.20118   \n",
              "6                                    141.87295   \n",
              "7                                    135.19872   \n",
              "8                                    127.04213   \n",
              "9                                    127.29315   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "5                               192.43848                   192.43848   \n",
              "6                               195.47065                   195.47065   \n",
              "7                               186.28694                   186.28694   \n",
              "8                               180.32847                   180.32847   \n",
              "9                               180.43837                   180.43837   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "5                                      12.0645     245.399580   \n",
              "6                                      13.4655     243.927941   \n",
              "7                                      15.5400     171.000000   \n",
              "8                                      15.0800     172.000000   \n",
              "9                                      12.6200     264.115666   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "5                                                 216.150490       \n",
              "6                                                 214.341578       \n",
              "7                                                 131.000000       \n",
              "8                                                 134.000000       \n",
              "9                                                 226.223933       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "5             145.432835   \n",
              "6             145.153439   \n",
              "7             154.000000   \n",
              "8             141.000000   \n",
              "9             180.336933   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "5                                                 116.183744      ...   \n",
              "6                                                 115.567076      ...   \n",
              "7                                                 114.000000      ...   \n",
              "8                                                 104.000000      ...   \n",
              "9                                                 142.445199      ...   \n",
              "\n",
              "Data Item  SMPSell D30 roll std  Day of Week  Is Weekday  Next Day Is Weekday  \\\n",
              "5                      0.005142            2           1                    1   \n",
              "6                      0.011180            3           1                    1   \n",
              "7                      0.010249            4           1                    0   \n",
              "8                      0.009697            5           0                    0   \n",
              "9                      0.009489            6           0                    1   \n",
              "\n",
              "Data Item  Day of Year   sin_DoY   cos_DoY  Next Day SAP  Next Day SMPBuy  \\\n",
              "5                  127  0.816538 -0.577292        0.4834           0.5187   \n",
              "6                  128  0.806480 -0.591261        0.4756           0.5109   \n",
              "7                  129  0.796183 -0.605056        0.4722           0.5075   \n",
              "8                  130  0.785650 -0.618671        0.4615           0.4968   \n",
              "9                  131  0.774884 -0.632103        0.4569           0.4922   \n",
              "\n",
              "Data Item  Next Day SMPSell  \n",
              "5                    0.4481  \n",
              "6                    0.4403  \n",
              "7                    0.4369  \n",
              "8                    0.4262  \n",
              "9                    0.4216  \n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean(df: pd.DataFrame, remove_outliers=True):\n",
        "    # fill missing CWV actuals with the normal forecast\n",
        "    df['Composite Weather Variable - Actual'] = df['Composite Weather Variable - Actual'].fillna(df['Composite Weather Variable - Normal'])\n",
        "\n",
        "    # There should be very remaining few rows that have any NaNs so we can drop any that do\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Can drop the composite weather forecasts\n",
        "    df.drop(columns=[\"Composite Weather Variable - Normal\", \"Composite Weather Variable - Cold\", \"Composite Weather Variable - Warm\"], inplace=True)\n",
        "\n",
        "    if(remove_outliers):\n",
        "        for col in label_cols:    \n",
        "            # remove outliers where any of the prices was 0\n",
        "            print(df.shape)\n",
        "            df = df[df[col] != 0]\n",
        "            print(df.shape)\n",
        "            df = df[df[f\"Next Day {col}\"] != 0]\n",
        "            print(df.shape)\n",
        "            #... and where the next day price is more than least 50% away from the current day's price\n",
        "            df = df[abs(df[col] - df[f\"Next Day {col}\"])/df[col] < 0.5]\n",
        "            print(df.shape)\n",
        "    return df    \n",
        "\n",
        "df = clean(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed_and_cleaned.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821e51d3",
      "metadata": {},
      "source": [
        "### Split the data into training and test sets\n",
        "Various ways were investigated:\n",
        "- Use the data before a cutoff date to train, and after that date to test. Designed to test whether the model will generalise to the most recent period, despite having been trained on earlier periods\n",
        "- Split the data 50/50 by date. Use all of the earlier half, and a random half of the later half, to train. Use the remaining half of the later half to test. Designed as a compromise between the above approach and a random split\n",
        "- Split the data randomly regardless of date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "f6118dc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def split_train_test_on_date(df, split_date, discard_before_date):\n",
        "    \"\"\"\n",
        "    Splits the DataFrame into training set (gas days before split date) and test set (ga days fron the split date on)\n",
        "\n",
        "    :param df: The DataFrame to split.\n",
        "    :param split_date: The date to split the DataFrame on.\n",
        "    :param discard_before_date: Discard anything before this date. Added to exclude time of Covid lockdowns.\n",
        "    :return: Tuple of (training set, testing set).\n",
        "    \"\"\"\n",
        "\n",
        "    # Split the DataFrame into training and testing sets\n",
        "    train_df = df[df['Gas Day'].between(discard_before_date, split_date, inclusive = \"neither\")]\n",
        "    test_df = df[df['Gas Day'] >= split_date]\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def split_with_test_half_of_last_half(df, discard_before_date):\n",
        "    \"\"\"\n",
        "    Use all the earlier half, and half the later half, to train\n",
        "    Use the other half of the later half to test\n",
        "    \"\"\"\n",
        "    df2 = df[df['Gas Day'] >= discard_before_date]\n",
        "    mid_date = df2['Gas Day'].mean()\n",
        "    first_half = df2[df2['Gas Day'] < mid_date]\n",
        "    second_half = df2[df2['Gas Day'] >= mid_date]\n",
        "    \n",
        "    train_df, test_df = train_test_split(second_half, test_size=0.5, shuffle=True)\n",
        "    train_df = pd.concat([first_half, train_df])\n",
        "    return train_df, test_df\n",
        "\n",
        "def n_train_n_test(df, n_train, n_test, discard_before_date):\n",
        "    \"\"\"Split based on number or fraction of rows\"\"\"\n",
        "    \n",
        "    df = df[df['Gas Day'] >= discard_before_date]\n",
        "    # Split the DataFrame into training and testing sets\n",
        "    train_df, test_df = train_test_split(df, test_size=n_test, train_size=n_train, shuffle=True)\n",
        "    \n",
        "    return train_df, test_df\n",
        "\n",
        "def get_X(df):\n",
        "    ys = [\"Next Day \" + col for col in label_cols]\n",
        "    df2 = df.drop(columns=ys)\n",
        "    df2.drop(columns=[\"Gas Day\"], inplace=True)\n",
        "    \n",
        "    return df2\n",
        "\n",
        "train, test = split_with_test_half_of_last_half(df, '2023-09-01')\n",
        "X_train = get_X(train)\n",
        "X_test = get_X(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aabed2",
      "metadata": {},
      "source": [
        "### Use Root Mean Squared Error as the measure of accuracy\n",
        "\n",
        "This is appropriate to price forecasting because it penalises larger inaccuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "9cbf2053",
      "metadata": {
        "id": "9cbf2053"
      },
      "outputs": [],
      "source": [
        "# Root mean squared error - penalises larger errors more than smaller ones\n",
        "def get_rmse(actuals, predictions):\n",
        "    rmse =  np.sqrt(np.mean((predictions - actuals)**2))\n",
        "    return round(rmse, 4)\n",
        "\n",
        "def print_model_stats(model, X):\n",
        "\n",
        "    # 1. Coefficients and intercept\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        #print(\"Coefficients:\", model.coef_)      # array of shape (n_features,)\n",
        "        cdf = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])\n",
        "        cdf = cdf.sort_values(by='Coefficients', ascending=False)\n",
        "        print(cdf)\n",
        "    if hasattr(model, \"intercept_\"):\n",
        "        print(\"Intercept:\", model.intercept_)    # scalar (or array if multi-output)\n",
        "\n",
        "    # 2. Model parameters\n",
        "    print(\"Parameters:\", model.get_params())\n",
        "\n",
        "    # 3. Linear algebra internals\n",
        "    if hasattr(model, \"rank_\"):\n",
        "        print(\"Rank of design matrix:\", model.rank_)\n",
        "    if hasattr(model, \"singular_\"):\n",
        "        print(\"Singular values of X:\", model.singular_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe56398",
      "metadata": {},
      "source": [
        "### Set up a framework to train models, and compare their performance on the test dataset against a naive predictor\n",
        "\n",
        "The naive predictor takes the current day's System Average Price and System Marginal Prices as the predictions for the next day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "id": "8a0ec407",
      "metadata": {
        "id": "8a0ec407"
      },
      "outputs": [],
      "source": [
        "TEST_ON_RANDOM = \"Random\"\n",
        "TEST_ON_LATEST = \"Latest \"\n",
        "TEST_ON_HALF_LATEST = \"Half latest\"\n",
        "\n",
        "class Context:\n",
        "    \"\"\"Context for a model evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, model_type, test_set):\n",
        "        self.model_type = model_type\n",
        "        self.test_set = test_set\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Context(model_type={self.model_type}, test_set={self.test_set})\"\n",
        "    \n",
        "class Result:\n",
        "    \"\"\"Result of a model evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, context:Context, price_label, model_rmse, naive_rmse):\n",
        "        self.context = context\n",
        "        self.price_label = price_label\n",
        "        self.model_rmse = model_rmse\n",
        "        self.naive_rmse = naive_rmse\n",
        "        self.timestamp = datetime.datetime.now()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"GasPredictResult(context={self.context}, price_label={self.price_label}, model_rmse={self.model_rmse}, naive_rmse={self.naive_rmse}, timestamp={self.timestamp})\"    \n",
        "\n",
        "def get_y(df, col):\n",
        "    return df[\"Next Day \" + col]\n",
        "\n",
        "def test_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    rmse = get_rmse(y, y_pred)\n",
        "    return rmse\n",
        "\n",
        "def train_and_test_model(model, df_train, df_test, col):\n",
        "    X_train = get_X(df_train)\n",
        "    X_test = get_X(df_test)\n",
        "    y_train = get_y(df_train, col)\n",
        "    y_test = get_y(df_test, col)\n",
        "    #scaler = StandardScaler()\n",
        "    #X_train_scaled = scaler.fit_transform(X_train)\n",
        "    #X_test_scaled = scaler.fit_transform(X_test)\n",
        "    X_train_scaled = X_train\n",
        "    X_test_scaled = X_test\n",
        "\n",
        "    model.fit(X_train_scaled, y_train)\n",
        "\n",
        "    rmse_train = test_model(model, X_train_scaled, y_train)\n",
        "    rmse_test = test_model(model, X_test_scaled, y_test)\n",
        "\n",
        "    return model, rmse_train, rmse_test\n",
        "\n",
        "def train_test_and_report_for_prices(model_factory, df_train: pd.DataFrame, df_test: pd.DataFrame, context:Context, print_model_stats=True):\n",
        "    results = []\n",
        "    for col in label_cols:\n",
        "        # Instantiate model.\n",
        "        model = model_factory()\n",
        "\n",
        "        # Train and test it\n",
        "        model, rmse_train, rmse_test = train_and_test_model(model, df_train, df_test, col)\n",
        "\n",
        "        # Print model details\n",
        "        if print_model_stats:\n",
        "            X_train = get_X(df_train)\n",
        "            print_model_stats(model, X_train)\n",
        "\n",
        "        # Get naive prediction stats for comparison\n",
        "        rmse_naive_train, rmse_naive_test = naive_predictions(df_train, df_test, col)\n",
        "\n",
        "        print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
        "        print_results(col + \" test\", rmse_naive_test, rmse_test)\n",
        "\n",
        "        testResult = Result(context, col, rmse_test, rmse_naive_test)\n",
        "        results.append(testResult)\n",
        "    return results\n",
        "\n",
        "def naive_predictions(df_train, df_test, col):\n",
        "    naive_predictions_train = df_train[col]\n",
        "    actuals_train = df_train[f\"Next Day {col}\"]\n",
        "    #mape_naive_train = get_mape(actuals_train, naive_predictions_train)\n",
        "    #print(f\"MAPE train (naive predictor) for {col}: {mape_naive_train}\")\n",
        "    rmse_naive_train = get_rmse(actuals_train, naive_predictions_train)\n",
        "    #print(f\"RMSE train (naive predictor) for {col}: {rmse_naive_train}\")\n",
        "\n",
        "    naive_predictions_test = df_test[col]\n",
        "    actuals_test = df_test[f\"Next Day {col}\"]\n",
        "    #mape_naive_test = get_mape(actuals_test, naive_predictions_test)\n",
        "    #print(f\"MAPE test (naive predictor) for {col}: {mape_naive_test}\")\n",
        "    rmse_naive_test = get_rmse(actuals_test, naive_predictions_test)\n",
        "    #print(f\"RMSE test (naive predictor) for {col}: {rmse_naive_test}\")\n",
        "    return rmse_naive_train, rmse_naive_test\n",
        "\n",
        "def print_results(case, rmse_naive, rmse_model):\n",
        "    headline = \"Worse\" if rmse_naive <= rmse_model else \"Better\"\n",
        "    print(f\"{case} - {headline} - model {rmse_model} v naive {rmse_naive}\")\n",
        "\n",
        "\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cb01d0",
      "metadata": {},
      "source": [
        "### Try linear regression models\n",
        "\n",
        "...to predict each of SAP (System Average Price), SMPBuy (System Marginal Price - Buy) and SMPSell (System Marginal Price - Sell). This generally performs worse than the naive predictor in testing, especially using a date-based split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "id": "39429681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39429681",
        "outputId": "9a1f327b-ccba-4a85-f298-0126ade5c540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression model:\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.387 v naive 0.457\n",
            "SAP test - Better - model 0.3738 v naive 0.4551\n",
            "SMPBuy train - Better - model 0.4567 v naive 0.5157\n",
            "SMPBuy test - Better - model 0.486 v naive 0.5302\n",
            "SMPSell train - Better - model 0.5181 v naive 0.5653\n",
            "SMPSell test - Worse - model 0.6889 v naive 0.683\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.4075 v naive 0.4965\n",
            "SAP test - Worse - model 0.6024 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.4878 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.8553 v naive 0.0966\n",
            "SMPSell train - Better - model 0.5906 v naive 0.6496\n",
            "SMPSell test - Worse - model 1.0567 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.4338 v naive 0.5295\n",
            "SAP test - Worse - model 0.1668 v naive 0.1217\n",
            "SMPBuy train - Better - model 0.5217 v naive 0.602\n",
            "SMPBuy test - Worse - model 0.1937 v naive 0.1433\n",
            "SMPSell train - Better - model 0.6304 v naive 0.694\n",
            "SMPSell test - Worse - model 0.2121 v naive 0.1375\n",
            "[GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SAP, model_rmse=0.3738, naive_rmse=0.4551, timestamp=2025-04-24 16:40:37.953919), GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SMPBuy, model_rmse=0.486, naive_rmse=0.5302, timestamp=2025-04-24 16:40:37.994049), GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SMPSell, model_rmse=0.6889, naive_rmse=0.683, timestamp=2025-04-24 16:40:38.022343), GasPredictResult(context=Context(model_type=Linear regression, test_set=Latest ), price_label=SAP, model_rmse=0.6024, naive_rmse=0.0806, timestamp=2025-04-24 16:40:38.054601), GasPredictResult(context=Context(model_type=Linear regression, test_set=Latest ), price_label=SMPBuy, model_rmse=0.8553, naive_rmse=0.0966, timestamp=2025-04-24 16:40:38.074917), GasPredictResult(context=Context(model_type=Linear regression, test_set=Latest ), price_label=SMPSell, model_rmse=1.0567, naive_rmse=0.0891, timestamp=2025-04-24 16:40:38.106895), GasPredictResult(context=Context(model_type=Linear regression, test_set=Half latest), price_label=SAP, model_rmse=0.1668, naive_rmse=0.1217, timestamp=2025-04-24 16:40:38.145412), GasPredictResult(context=Context(model_type=Linear regression, test_set=Half latest), price_label=SMPBuy, model_rmse=0.1937, naive_rmse=0.1433, timestamp=2025-04-24 16:40:38.224100), GasPredictResult(context=Context(model_type=Linear regression, test_set=Half latest), price_label=SMPSell, model_rmse=0.2121, naive_rmse=0.1375, timestamp=2025-04-24 16:40:38.264868)]\n"
          ]
        }
      ],
      "source": [
        "print (\"Linear regression model:\")\n",
        "model_factory = lambda: LinearRegression()\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "context = Context(\"Linear regression\", TEST_ON_RANDOM)\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "context = Context(\"Linear regression\", TEST_ON_LATEST)\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "context = Context(\"Linear regression\", TEST_ON_HALF_LATEST)\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2dd9746",
      "metadata": {},
      "source": [
        "### Try a random forest model\n",
        "Linear regression generally performed worse than the naive predictor in testing, especially using a date-based split, so let's try a random forest model. The hyperparameters for the best version were obtained by random search in the second code block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "id": "bc25ac1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc25ac1d",
        "outputId": "896fff9c-bc0c-4214-9d84-c79f832d572a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest model:\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.1873 v naive 0.4684\n",
            "SAP test - Worse - model 0.4277 v naive 0.4188\n",
            "SMPBuy train - Better - model 0.2198 v naive 0.5398\n",
            "SMPBuy test - Worse - model 0.5015 v naive 0.4528\n",
            "SMPSell train - Better - model 0.2788 v naive 0.6294\n",
            "SMPSell test - Worse - model 0.5217 v naive 0.4868\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.1972 v naive 0.4965\n",
            "SAP test - Worse - model 0.1037 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.2322 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.1225 v naive 0.0966\n",
            "SMPSell train - Better - model 0.2874 v naive 0.6496\n",
            "SMPSell test - Worse - model 0.1197 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.2074 v naive 0.5299\n",
            "SAP test - Worse - model 0.1219 v naive 0.1167\n",
            "SMPBuy train - Better - model 0.2474 v naive 0.6021\n",
            "SMPBuy test - Worse - model 0.1438 v naive 0.1428\n",
            "SMPSell train - Better - model 0.3106 v naive 0.6939\n",
            "SMPSell test - Worse - model 0.139 v naive 0.1388\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "print (\"Random forest model:\")\n",
        "#RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0) # best from random searh\n",
        "#RandomForestRegressor(n_estimators = 200, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.7, max_depth = 20, ccp_alpha = 0.0) # best for SAP: SAP test - Better - model 0.77 v naive 0.8\n",
        "model_factory = lambda: RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0)\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "context = Context(\"Random forest\", TEST_ON_RANDOM)\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "context = Context(\"Random forest\", TEST_ON_LATEST)\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "context = Context(\"Random forest\", TEST_ON_HALF_LATEST)\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24774087",
      "metadata": {},
      "source": [
        "### Next, try gradient boosting\n",
        "Again the random forest improves in test slightly on a random split but not when trained on earlier data and tested on later. Let's try tree-based gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "id": "0cecf345",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting model (XGBoost XGBRegressor):\n",
            "Using random train-test split...\n",
            "SAP train - Better - model 0.0233 v naive 0.465\n",
            "SAP test - Better - model 0.4091 v naive 0.4299\n",
            "SMPBuy train - Better - model 0.028 v naive 0.5117\n",
            "SMPBuy test - Better - model 0.5307 v naive 0.5418\n",
            "SMPSell train - Better - model 0.0286 v naive 0.6091\n",
            "SMPSell test - Worse - model 0.5973 v naive 0.5589\n",
            "Using pure date-based train-test split...\n",
            "SAP train - Better - model 0.0332 v naive 0.4965\n",
            "SAP test - Worse - model 0.116 v naive 0.0806\n",
            "SMPBuy train - Better - model 0.0373 v naive 0.5645\n",
            "SMPBuy test - Worse - model 0.1506 v naive 0.0966\n",
            "SMPSell train - Better - model 0.0401 v naive 0.6496\n",
            "SMPSell test - Worse - model 0.1448 v naive 0.0891\n",
            "Using half of the last half for testing...\n",
            "SAP train - Better - model 0.0287 v naive 0.5299\n",
            "SAP test - Worse - model 0.1216 v naive 0.1166\n",
            "SMPBuy train - Better - model 0.032 v naive 0.6023\n",
            "SMPBuy test - Worse - model 0.1469 v naive 0.1394\n",
            "SMPSell train - Better - model 0.0356 v naive 0.6941\n",
            "SMPSell test - Worse - model 0.1456 v naive 0.136\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBRegressor\n",
        "\n",
        "print(\"Gradient Boosting model (XGBoost XGBRegressor):\")\n",
        "\n",
        "model_factory = lambda: XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(\"Using random train-test split...\")\n",
        "context = Context(\"Gradient boosting\", TEST_ON_RANDOM)\n",
        "train, test = n_train_n_test(df, n_train=0.75, n_test=0.25, discard_before_date='2021-03-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using pure date-based train-test split...\")\n",
        "context = Context(\"Gradient boosting\", TEST_ON_LATEST)\n",
        "train, test = split_train_test_on_date(df, '2024-04-01', '2020-10-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using half of the last half for testing...\")\n",
        "context = Context(\"Gradient boosting\", TEST_ON_HALF_LATEST)\n",
        "train, test = split_with_test_half_of_last_half(df, '2021-04-01')\n",
        "all_results += train_test_and_report_for_prices(model_factory, train, test, context, print_model_stats=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab84acc",
      "metadata": {},
      "source": [
        "### Try Recurrent Neural Network\n",
        "\n",
        "The gradient booster likewise did not perform any better than the naive predictor, especially when trained on the earlier data and tested on the later data. Let's try a neural net. For a simple time series, a Temporal Convolutional Net would be the obvious choice, but in this case we have a lot of market fundamentals to use as additional features so a RNN seems the better fit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2834c00b",
      "metadata": {},
      "source": [
        "(1) Because of how the inputs need to be shaped into sequences, we'll load the data again, skipping the manually-engineered lag features and next-day labels. We'll still fill in missing actual Composite Weather Variables with the normal forecast, but won't delete the few with outlying prices in case the RNN is sophisticated enough to make good use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "id": "44d0ce63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>Storage, Short Range, Maximum potential flow</th>\n",
              "      <th>Storage, Short Range, Stock Levels</th>\n",
              "      <th>System Entry Flows, National, Forecast</th>\n",
              "      <th>System Entry Flows, National, Physical</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>66.15110</td>\n",
              "      <td>135.26344</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>10.5824</td>\n",
              "      <td>268.090483</td>\n",
              "      <td>240.620483</td>\n",
              "      <td>160.371026</td>\n",
              "      <td>132.901026</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>247.492971</td>\n",
              "      <td>253.383732</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0.863142</td>\n",
              "      <td>-0.504961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-05-02</td>\n",
              "      <td>58.78630</td>\n",
              "      <td>131.66283</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>11.3089</td>\n",
              "      <td>244.938804</td>\n",
              "      <td>217.103350</td>\n",
              "      <td>145.468324</td>\n",
              "      <td>117.632869</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>208.688512</td>\n",
              "      <td>229.860588</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>0.854322</td>\n",
              "      <td>-0.519744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-03</td>\n",
              "      <td>56.55015</td>\n",
              "      <td>141.57363</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>11.6531</td>\n",
              "      <td>242.854588</td>\n",
              "      <td>214.663679</td>\n",
              "      <td>144.375923</td>\n",
              "      <td>116.185014</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>221.676044</td>\n",
              "      <td>252.336835</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>0.845249</td>\n",
              "      <td>-0.534373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-04</td>\n",
              "      <td>52.82721</td>\n",
              "      <td>152.87412</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>11.9252</td>\n",
              "      <td>242.098717</td>\n",
              "      <td>213.561444</td>\n",
              "      <td>144.746098</td>\n",
              "      <td>116.208825</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>206.768342</td>\n",
              "      <td>231.939225</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>0.835925</td>\n",
              "      <td>-0.548843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-05</td>\n",
              "      <td>62.21188</td>\n",
              "      <td>134.50813</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>11.4803</td>\n",
              "      <td>247.112422</td>\n",
              "      <td>218.209695</td>\n",
              "      <td>145.956044</td>\n",
              "      <td>117.053317</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>201.921238</td>\n",
              "      <td>189.255071</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>0.826354</td>\n",
              "      <td>-0.563151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "0         2020-05-01                                 66.15110   \n",
              "1         2020-05-02                                 58.78630   \n",
              "2         2020-05-03                                 56.55015   \n",
              "3         2020-05-04                                 52.82721   \n",
              "4         2020-05-05                                 62.21188   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "0                                    135.26344   \n",
              "1                                    131.66283   \n",
              "2                                    141.57363   \n",
              "3                                    152.87412   \n",
              "4                                    134.50813   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "0                               201.41454                   201.41454   \n",
              "1                               190.44913                   190.44913   \n",
              "2                               198.12378                   198.12378   \n",
              "3                               205.70133                   205.70133   \n",
              "4                               196.72001                   196.72001   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "0                                      10.5824     268.090483   \n",
              "1                                      11.3089     244.938804   \n",
              "2                                      11.6531     242.854588   \n",
              "3                                      11.9252     242.098717   \n",
              "4                                      11.4803     247.112422   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "0                                                 240.620483       \n",
              "1                                                 217.103350       \n",
              "2                                                 214.663679       \n",
              "3                                                 213.561444       \n",
              "4                                                 218.209695       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "0             160.371026   \n",
              "1             145.468324   \n",
              "2             144.375923   \n",
              "3             144.746098   \n",
              "4             145.956044   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "0                                                 132.901026      ...   \n",
              "1                                                 117.632869      ...   \n",
              "2                                                 116.185014      ...   \n",
              "3                                                 116.208825      ...   \n",
              "4                                                 117.053317      ...   \n",
              "\n",
              "Data Item  Storage, Short Range, Maximum potential flow  \\\n",
              "0                                                   0.0   \n",
              "1                                                   0.0   \n",
              "2                                                   0.0   \n",
              "3                                                   0.0   \n",
              "4                                                   0.0   \n",
              "\n",
              "Data Item  Storage, Short Range, Stock Levels  \\\n",
              "0                                         0.0   \n",
              "1                                         0.0   \n",
              "2                                         0.0   \n",
              "3                                         0.0   \n",
              "4                                         0.0   \n",
              "\n",
              "Data Item  System Entry Flows, National, Forecast  \\\n",
              "0                                      247.492971   \n",
              "1                                      208.688512   \n",
              "2                                      221.676044   \n",
              "3                                      206.768342   \n",
              "4                                      201.921238   \n",
              "\n",
              "Data Item  System Entry Flows, National, Physical  Day of Week  Is Weekday  \\\n",
              "0                                      253.383732            4           1   \n",
              "1                                      229.860588            5           0   \n",
              "2                                      252.336835            6           0   \n",
              "3                                      231.939225            0           1   \n",
              "4                                      189.255071            1           1   \n",
              "\n",
              "Data Item  Next Day Is Weekday  Day of Year   sin_DoY   cos_DoY  \n",
              "0                            0          122  0.863142 -0.504961  \n",
              "1                            0          123  0.854322 -0.519744  \n",
              "2                            1          124  0.845249 -0.534373  \n",
              "3                            1          125  0.835925 -0.548843  \n",
              "4                            1          126  0.826354 -0.563151  \n",
              "\n",
              "[5 rows x 48 columns]"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Reload with minimal preprocessing and cleaning\n",
        "df = load_data()\n",
        "df = preprocess(df, add_lags=False, add_labels=False)\n",
        "df = clean(df, remove_outliers=False)\n",
        "df = df.sort_values('Gas Day').reset_index(drop=True) # Should already be sorted, but just in case\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e48096",
      "metadata": {},
      "source": [
        "(2) Make the sequences, covering 30 days of the salient features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "62f199b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "WINDOW_SIZE = 30\n",
        "\n",
        "feature_cols = ['Composite Weather Variable - Actual', 'Demand Actual, NTS, D+1', 'Demand Forecast, NTS, hourly update', 'Interconnector - Daily Flow', 'Medium Storage - Actual Stock',\n",
        "              'Medium Storage - Stock Level at Max Flow', 'Predicted Closing Linepack (PCLP1)', \n",
        "              'SAP', 'SMPBuy',\t'SMPSell', \n",
        "              'Storage - Daily Flow','Storage - Delivery', 'Storage, Medium Range, Stock Levels', 'System Entry Flows, National, Forecast', 'System Entry Flows, National, Physical',\n",
        "              'Day of Week','Is Weekday','Next Day Is Weekday','Day of Year']\n",
        "\n",
        "def make_sequences(df, feature_cols):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(df) - WINDOW_SIZE):\n",
        "        X.append(df[feature_cols].iloc[i : i + WINDOW_SIZE].values)\n",
        "        Y.append(df[label_cols].iloc[i + WINDOW_SIZE].values) # using SAP, SMPBuy and SMPSell as labels as before\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, y = make_sequences(df, feature_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5b5e0e",
      "metadata": {},
      "source": [
        "(3) Split sequentially into training, validate and test sets, so that the new gas days introduced at each stage are later than the days already seen. Then scale the sets individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "id": "0659b02a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training records: 1244\n",
            "Validation records: 177\n",
            "Test records: 357\n"
          ]
        }
      ],
      "source": [
        "train_size = int(0.7 * len(X))\n",
        "val_size   = int(0.1 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_val,   y_val   = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test,  y_test  = X[train_size+val_size:], y[train_size+val_size:]\n",
        "print(f\"Training records: {X_train.shape[0]}\")\n",
        "print(f\"Validation records: {X_val.shape[0]}\")\n",
        "print(f\"Test records: {X_test.shape[0]}\")\n",
        "\n",
        "\n",
        "n_feats = X_train.shape[2]\n",
        "scaler = StandardScaler()\n",
        "X_train_2d = X_train.reshape(-1, n_feats)\n",
        "scaler.fit(X_train_2d)\n",
        "\n",
        "def scale_split(X):\n",
        "    X_2d = X.reshape(-1, n_feats)\n",
        "    Xs = scaler.transform(X_2d)\n",
        "    return Xs.reshape(-1, WINDOW_SIZE, n_feats)\n",
        "\n",
        "#Take a copy of the unscaled test data for comparison against the naive predictor\n",
        "X_test_unscaled = X_test.copy()\n",
        "\n",
        "X_train = scale_split(X_train)\n",
        "X_val   = scale_split(X_val)\n",
        "X_test  = scale_split(X_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "593fa9eb",
      "metadata": {},
      "source": [
        "(4) Train and test the model - unfortunately it doesn't fit into the framework for sklearn-type models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "id": "aec8afd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mike\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m6,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 59ms/step - loss: 25.6282 - rmse: 5.0574 - val_loss: 3.6550 - val_rmse: 1.9118 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 11.6664 - rmse: 3.4005 - val_loss: 2.4421 - val_rmse: 1.5627 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 4.2651 - rmse: 2.0574 - val_loss: 1.3342 - val_rmse: 1.1551 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 2.2940 - rmse: 1.5113 - val_loss: 0.8676 - val_rmse: 0.9314 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 1.4136 - rmse: 1.1876 - val_loss: 0.8562 - val_rmse: 0.9253 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 1.0811 - rmse: 1.0380 - val_loss: 0.6339 - val_rmse: 0.7962 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 1.1201 - rmse: 1.0536 - val_loss: 0.5899 - val_rmse: 0.7680 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.8722 - rmse: 0.9324 - val_loss: 0.5241 - val_rmse: 0.7240 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.7361 - rmse: 0.8574 - val_loss: 0.4316 - val_rmse: 0.6570 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.5916 - rmse: 0.7632 - val_loss: 0.3741 - val_rmse: 0.6117 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.5674 - rmse: 0.7525 - val_loss: 0.3415 - val_rmse: 0.5844 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4570 - rmse: 0.6746 - val_loss: 0.3135 - val_rmse: 0.5599 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.5768 - rmse: 0.7581 - val_loss: 0.2546 - val_rmse: 0.5046 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4072 - rmse: 0.6356 - val_loss: 0.2423 - val_rmse: 0.4922 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3964 - rmse: 0.6275 - val_loss: 0.2124 - val_rmse: 0.4608 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.4645 - rmse: 0.6789 - val_loss: 0.2102 - val_rmse: 0.4585 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.4370 - rmse: 0.6598 - val_loss: 0.1843 - val_rmse: 0.4293 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.3581 - rmse: 0.5947 - val_loss: 0.1490 - val_rmse: 0.3860 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4251 - rmse: 0.6508 - val_loss: 0.1642 - val_rmse: 0.4053 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.3532 - rmse: 0.5924 - val_loss: 0.1925 - val_rmse: 0.4387 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.3889 - rmse: 0.6230 - val_loss: 0.1383 - val_rmse: 0.3719 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3134 - rmse: 0.5588 - val_loss: 0.1220 - val_rmse: 0.3492 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.3521 - rmse: 0.5918 - val_loss: 0.1481 - val_rmse: 0.3848 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.3128 - rmse: 0.5577 - val_loss: 0.1624 - val_rmse: 0.4029 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.3161 - rmse: 0.5600 - val_loss: 0.1235 - val_rmse: 0.3514 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.3328 - rmse: 0.5759 - val_loss: 0.1470 - val_rmse: 0.3834 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.3577 - rmse: 0.5957 - val_loss: 0.1309 - val_rmse: 0.3618 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.2830 - rmse: 0.5285 - val_loss: 0.1390 - val_rmse: 0.3728 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3519 - rmse: 0.5926 - val_loss: 0.1434 - val_rmse: 0.3787 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.2850 - rmse: 0.5330 - val_loss: 0.1412 - val_rmse: 0.3758 - learning_rate: 5.0000e-04\n",
            "Epoch 31/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.2871 - rmse: 0.5350 - val_loss: 0.1408 - val_rmse: 0.3752 - learning_rate: 5.0000e-04\n",
            "Epoch 32/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.3274 - rmse: 0.5717 - val_loss: 0.1477 - val_rmse: 0.3843 - learning_rate: 5.0000e-04\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0975 - rmse: 0.2958\n",
            "Overall RMSE: 0.4088\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 48ms/step\n",
            "SAP RMSE: 0.3677\n",
            "SAP naive predictor RMSE: 0.0801\n",
            "SMPBuy RMSE: 0.3929\n",
            "SMPBuy naive predictor RMSE: 0.0936\n",
            "SMPSell RMSE: 0.4601\n",
            "SMPSell naive predictor RMSE: 0.0893\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def make_rnn():\n",
        "    #model = models.Sequential([\n",
        "        #layers.LSTM(128, return_sequences=True, input_shape=(WINDOW_SIZE, n_feats)),\n",
        "        #layers.Dropout(0.02),\n",
        "        #layers.LSTM(64),\n",
        "        #layers.Dropout(0.02),\n",
        "        #layers.Dense(32, activation='relu'),\n",
        "        #layers.Dense(3, name='multi_output')   # predicts [SAP, SMPBuy, SMPSell]\n",
        "    #])\n",
        "    model = models.Sequential([\n",
        "        # Single, small LSTM — no return_sequences, so it only outputs the last hidden state\n",
        "        layers.LSTM(32, input_shape=(WINDOW_SIZE, n_feats)),\n",
        "\n",
        "        # (Optional) small dense “bottleneck” to pick up any non-linear mix\n",
        "        layers.Dense(16, activation='relu'),\n",
        "\n",
        "        # Multi-output head predicts [SAP, SMPBuy, SMPSell]\n",
        "        layers.Dense(3, name='multi_output')\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def train_and_test_rnn(model, context:Context):\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate model RMSE on test set\n",
        "    eval_results = model.evaluate(X_test, y_test, return_dict=True)\n",
        "    model_rmse = eval_results['rmse']\n",
        "    print(f\"Overall RMSE: {model_rmse:.4f}\")\n",
        "\n",
        "    results = []\n",
        "    #Individual RMSE for each target\n",
        "    y_pred = model.predict(X_test)\n",
        "    for i, name in enumerate(label_cols):\n",
        "        #get model RMSE for each target\n",
        "        rmse = get_rmse(y_test[:,i], y_pred[:,i])\n",
        "        print(f\"{name} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # get naive predictor RMSE based on the unscaled inputs\n",
        "        feat_idx = feature_cols.index(name)\n",
        "        y_pred_naive = X_test_unscaled[:, -1, feat_idx]\n",
        "        y_true = y_test[:, i]\n",
        "        naive_rmse = get_rmse(y_true, y_pred_naive)\n",
        "        print(f\"{name} naive predictor RMSE: {naive_rmse:.4f}\")\n",
        "\n",
        "        # add stats\n",
        "        testResult = Result(context, name, rmse, naive_rmse)\n",
        "\n",
        "        # add to the running list of results\n",
        "        results.append(testResult)\n",
        "\n",
        "    return results\n",
        "\n",
        "model = make_rnn()\n",
        "context = Context(\"RNN\", TEST_ON_LATEST)\n",
        "all_results += train_and_test_rnn(model, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f23169",
      "metadata": {},
      "source": [
        "### Finally, try reframing RNN with a residual model\n",
        "\n",
        "The RNN is still testing worse than naive predictor, indicating that the network is not learning anything from the additional fields that adds anything to the current day prices.\n",
        "As a final option, try a residual model that predicts the delta from the current day's price\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "id": "9ac9c053",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ lstm_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ last_vals (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ delta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dense_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ last_vals[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ delta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_7       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m19\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m6,656\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ lstm_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ last_vals (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input_layer_7[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ delta (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │         \u001b[38;5;34m51\u001b[0m │ dense_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ last_vals[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ delta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - loss: 22.6008 - rmse: 4.7458 - val_loss: 7.9864 - val_rmse: 2.8260 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - loss: 10.6863 - rmse: 3.2601 - val_loss: 2.0446 - val_rmse: 1.4299 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.0966 - rmse: 1.7544 - val_loss: 1.1042 - val_rmse: 1.0508 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.0733 - rmse: 1.0344 - val_loss: 0.8299 - val_rmse: 0.9110 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 1.0092 - rmse: 0.9961 - val_loss: 1.0283 - val_rmse: 1.0141 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.7420 - rmse: 0.8596 - val_loss: 1.1450 - val_rmse: 1.0700 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.6416 - rmse: 0.7998 - val_loss: 1.1750 - val_rmse: 1.0840 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.6041 - rmse: 0.7745 - val_loss: 1.3104 - val_rmse: 1.1447 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.5542 - rmse: 0.7431 - val_loss: 1.3715 - val_rmse: 1.1711 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4803 - rmse: 0.6923 - val_loss: 1.5217 - val_rmse: 1.2336 - learning_rate: 5.0000e-04\n",
            "Epoch 11/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - loss: 0.4826 - rmse: 0.6938 - val_loss: 1.5033 - val_rmse: 1.2261 - learning_rate: 5.0000e-04\n",
            "Epoch 12/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.4898 - rmse: 0.6997 - val_loss: 1.5177 - val_rmse: 1.2320 - learning_rate: 5.0000e-04\n",
            "Epoch 13/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4565 - rmse: 0.6747 - val_loss: 1.6002 - val_rmse: 1.2650 - learning_rate: 5.0000e-04\n",
            "Epoch 14/100\n",
            "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4613 - rmse: 0.6778 - val_loss: 1.6693 - val_rmse: 1.2920 - learning_rate: 5.0000e-04\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.4098 - rmse: 0.6099\n",
            "Overall RMSE: 0.7559\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step\n",
            "SAP RMSE: 0.7026\n",
            "SAP naive predictor RMSE: 0.0801\n",
            "SMPBuy RMSE: 0.6860\n",
            "SMPBuy naive predictor RMSE: 0.0936\n",
            "SMPSell RMSE: 0.8658\n",
            "SMPSell naive predictor RMSE: 0.0893\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def make_rnn_residual():\n",
        "    # 1) Inputs\n",
        "    inputs = layers.Input(shape=(WINDOW_SIZE, n_feats))\n",
        "\n",
        "    # 2) Core LSTM\n",
        "    x = layers.LSTM(32)(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "\n",
        "    # 3) Delta prediction head (predict tomorrow’s Δ for each series)\n",
        "    delta = layers.Dense(3, name='delta')(x)  \n",
        "    #   outputs [ΔSAP, ΔSMPBuy, ΔSMPSell]\n",
        "    idxs_of_labels = [feature_cols.index(c) for c in label_cols]\n",
        "    # 4) Grab today's values from the last timestep of the sequence\n",
        "    #    This gives shape (batch, 3) corresponding to [SAP_t, SMPBuy_t, SMPSell_t].\n",
        "    last_vals = layers.Lambda(lambda z: tf.gather(z[:, -1, :], idxs_of_labels, axis=1),\n",
        "                            name='last_vals')(inputs)\n",
        "\n",
        "    # 5) Add skip-connection: tomorrow = today + predicted Δ\n",
        "    outputs = layers.Add(name='residual_output')([last_vals, delta])\n",
        "\n",
        "    # 6) Assemble and compile\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse',\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = make_rnn_residual()\n",
        "context = Context(\"Residual RNN\", TEST_ON_LATEST)\n",
        "all_results += train_and_test_rnn(model, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ed5add",
      "metadata": {},
      "source": [
        "### Pick the best performing model type\n",
        "No model outperformed the naive predictor so I'll have to choose the least bad one to tune based on the gathered results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "id": "192bf53f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest (SAP): difference over naive RMSE = -0.0231\n",
            "Random forest (SMPBuy): difference over naive RMSE = -0.0259\n",
            "Random forest (SMPSell): difference over naive RMSE = -0.0306\n",
            "Gradient boosting (SAP): difference over naive RMSE = -0.0354\n",
            "Gradient boosting (SMPBuy): difference over naive RMSE = -0.0540\n",
            "Gradient boosting (SMPSell): difference over naive RMSE = -0.0557\n",
            "RNN (SAP): difference over naive RMSE = -0.2876\n",
            "RNN (SMPBuy): difference over naive RMSE = -0.2993\n",
            "RNN (SMPSell): difference over naive RMSE = -0.3708\n",
            "Linear regression (SAP): difference over naive RMSE = -0.5218\n",
            "Residual RNN (SMPBuy): difference over naive RMSE = -0.5924\n",
            "Residual RNN (SAP): difference over naive RMSE = -0.6225\n",
            "Linear regression (SMPBuy): difference over naive RMSE = -0.7587\n",
            "Residual RNN (SMPSell): difference over naive RMSE = -0.7765\n",
            "Linear regression (SMPSell): difference over naive RMSE = -0.9676\n"
          ]
        }
      ],
      "source": [
        "# sort by the delta (model_rmse - naive_rmse)\n",
        "#Filter results to only include \"Latest\" data test set\n",
        "filtered_results = [r for r in all_results if r.context.test_set == TEST_ON_LATEST]\n",
        "\n",
        "sorted_results = sorted(filtered_results, key=lambda r: r.naive_rmse - r.model_rmse, reverse=True)\n",
        "\n",
        "for r in sorted_results:\n",
        "    difference = r.naive_rmse - r.model_rmse\n",
        "    print(f\"{r.context.model_type} ({r.price_label}): difference over naive RMSE = {difference:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b02542e",
      "metadata": {},
      "source": [
        "Random forest came out best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a51ccc4e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a51ccc4e",
        "outputId": "066a975b-5ff8-4b2a-b20d-d88898dce52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
            "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.7, 'max_depth': None, 'ccp_alpha': 0.001}\n",
            "Best CV RMSE on train set: 0.4899\n",
            "SAP test - Worse - model 0.1277 v naive 0.1166\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        }
      ],
      "source": [
        "#Random search for hyperparameter tuning\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators':     [100, 200, 500],\n",
        "    'max_depth':        [None, 10, 20],\n",
        "    'min_samples_split':[2],\n",
        "    'min_samples_leaf': [2],\n",
        "    'max_features':     [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0],\n",
        "    'ccp_alpha':        [0.0, 0.001]\n",
        "}\n",
        "#param_grid = {\n",
        "#    'n_estimators':     [100, 200],\n",
        "#    'max_depth':        [None, 10, 20],\n",
        "#    'min_samples_split':[2, 5],\n",
        "#    'min_samples_leaf': [1, 2],\n",
        "#    'max_features':     ['sqrt'],\n",
        "#    'ccp_alpha':        [0.001, 0.01]\n",
        "#}\n",
        "X_train = get_X(train)\n",
        "for col in label_cols:\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "        \n",
        "        oob_score=True   # optional: get out‑of‑bag score on your train set\n",
        "    )\n",
        "    #grid = GridSearchCV(\n",
        "    #    estimator=rf,\n",
        "    #    param_grid=param_grid,\n",
        "    #    cv=5,                             # 5‑fold CV on X_train/y_train\n",
        "    #    scoring='neg_root_mean_squared_error',\n",
        "    #    n_jobs=-1,\n",
        "    #    verbose=3\n",
        "    #)\n",
        "    rand_search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=param_grid,             \n",
        "        n_iter=50,                                  \n",
        "        cv=5,\n",
        "        scoring='neg_root_mean_squared_error',      \n",
        "        n_jobs=-1,\n",
        "        verbose=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    y_train = get_y(train, col)\n",
        "    rand_search.fit(X_train, y_train)\n",
        "\n",
        "    print(\"Best hyperparameters:\", rand_search.best_params_)\n",
        "    print(\"Best CV RMSE on train set: {:.4f}\".format(-rand_search.best_score_))\n",
        "\n",
        "\n",
        "    X_test = get_X(test)\n",
        "    y_test = get_y(test, col)\n",
        "    # -----------------------------------------------------------------------------\n",
        "    # 5. Evaluate the best model on the TEST set\n",
        "    # -----------------------------------------------------------------------------\n",
        "    best_model = rand_search.best_estimator_\n",
        "\n",
        "    rmse_test = test_model(best_model, X_test, y_test)\n",
        "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)\n",
        "\n",
        "    print_results(col + \" test\", rmse_naive_test, rmse_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81bc167d",
      "metadata": {},
      "source": [
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 1.0, 'max_depth': 20, 'ccp_alpha': 0.0}\n",
        "Best CV RMSE on train set: 0.7879\n",
        "SAP test - Worse - model 0.78 v naive 0.78\n",
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.7, 'max_depth': None, 'ccp_alpha': 0.001}\n",
        "Best CV RMSE on train set: 0.8540\n",
        "SMPBuy test - Worse - model 0.86 v naive 0.83\n",
        "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n",
        "Best hyperparameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.5, 'max_depth': None, 'ccp_alpha': 0.001}\n",
        "Best CV RMSE on train set: 0.8906\n",
        "SMPSell test - Worse - model 0.9 v naive 0.83"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
