{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9c3764d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "2a877cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from the given URL and saves it to the specified file.\n",
    "    \n",
    "    :param url: URL to download the CSV data from.\n",
    "    :param output_file: Path to the local file where the CSV will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "\n",
    "        # Write the content (CSV data) to a file in binary mode\n",
    "        with open(output_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"CSV file has been successfully downloaded and saved as '{output_file}'.\")\n",
    "        \n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "69f4bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = \"..\\\\data\\\\raw\\\\\"\n",
    "# download the raw data from national gas data portal\n",
    "def download_raw_data():\n",
    "    with open(\"..\\\\PUB ids.txt\") as f:\n",
    "        pubIds = f.read()\n",
    "        pubIds = pubIds.replace(\"\\n\", \",\").strip() \n",
    "    \n",
    "    earliest = datetime.date(2020,4,1) # Download data going back 5 years\n",
    "    # Loop from week 0 (today) to week 13 (13 weeks ago)\n",
    "    download_from = datetime.date.today().replace(day=1) # start first download on first day of current month\n",
    "    download_to = datetime.date.today() # end first download on today's date\n",
    "    while(download_from > earliest):\n",
    "        \n",
    "        # Format the date in yyyy-mm-dd format\n",
    "        formatted_from = download_from.strftime(\"%Y-%m-%d\")\n",
    "        formatted_to = download_to.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        csv_url = f\"https://data.nationalgas.com/api/find-gas-data-download?applicableFor=Y&dateFrom={formatted_from}&dateTo={formatted_to}&dateType=GASDAY&latestFlag=Y&ids={pubIds}&type=CSV\"\n",
    "        month_format = download_from.strftime(\"%Y-%m\")\n",
    "        output_filename = f\"{raw_data_folder}{month_format}.csv\"\n",
    "\n",
    "        download_csv(csv_url, output_filename)\n",
    "        time.sleep(3) # brief courtesy sleep\n",
    "        download_to = download_from - datetime.timedelta(days=1) # next download should go up to the day before the previous download start date\n",
    "        download_from = download_to.replace(day=1) # next download should start on the first day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "dd682eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"SAP\", \"SMPBuy\", \"SMPSell\"]\n",
    "\n",
    "def pivot(df, cols):\n",
    "\n",
    "    #only keep the values we are interested in\n",
    "    mask = df[\"Data Item\"].isin(cols)\n",
    "\n",
    "    df_filtered = df[mask]  \n",
    "\n",
    "    # if there are duplicates for the field and gas day, take the latest\n",
    "    df_latest = (\n",
    "        df_filtered\n",
    "        .sort_values(\"Applicable At\")\n",
    "        .groupby([\"Gas Day\", \"Data Item\"])\n",
    "        .last()  # this takes the row with the highest (i.e. latest) \"Applicable At\" per group\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # pivot to get 1 row per gas day\n",
    "    df_latest = df_latest.pivot(index=\"Gas Day\", columns=\"Data Item\", values=\"Value\").reset_index()\n",
    "    \n",
    "    # Drop 1 column that accounts for most of the NaNs\n",
    "    df_latest.drop(columns=[\"Composite Weather Variable - Actual\"], inplace=True)\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "def load_data():\n",
    "    #Read raw CSVs\n",
    "    pathlist = list(Path(raw_data_folder).rglob('*.csv'))\n",
    "    file_count = len(pathlist)\n",
    "    dfs = []\n",
    "    files_done = 0\n",
    "    for path_obj in pathlist:\n",
    "        path = str(path_obj)   \n",
    "        \n",
    "        df = pd.read_csv(path,\n",
    "            parse_dates=[\"Applicable At\", \"Applicable For\", \"Generated Time\"],\n",
    "            dayfirst=True)\n",
    "\n",
    "        df.rename(columns={'Applicable For': 'Gas Day'}, inplace=True)\n",
    "        df['Gas Day'] = pd.to_datetime(df['Gas Day'], dayfirst=True)\n",
    "        # daily summary columns: \n",
    "\n",
    "        daily_cols = df[\"Data Item\"].unique()\n",
    "        # print(daily_cols)\n",
    "        # Get price and demand columns, to use as tommorow's ground truth, and with 1-3 days lag\n",
    "\n",
    "        #label_cols = [\"SAP, Actual Day\", \"SMP Buy, Actual Day\", \"SMP Sell, Actual Day\", \"Demand Actual, NTS, D+1\"]\n",
    "\n",
    "        #df_labels = pivot(df, label_cols)\n",
    "        \n",
    "\n",
    "        df_daily = pivot(df, daily_cols)\n",
    "        dfs.append(df_daily)\n",
    "        \n",
    "        files_done += 1\n",
    "        if files_done % 10 == 0:\n",
    "            print(f\"Processed {files_done} of {file_count} raw files\")\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    #Rename the columns that are going to be reused for ground truth and time series\n",
    "    df.rename(columns={\"SAP, Actual Day\": 'SAP', \"SMP Buy, Actual Day\": 'SMPBuy', \"SMP Sell, Actual Day\": 'SMPSell'}, inplace=True)\n",
    "    \n",
    "    # add lagged features\n",
    "    lag_days = 5\n",
    "    for i in range(1, lag_days+1):\n",
    "        for col in label_cols:\n",
    "            df[f\"{col} D-{i}\"] = df[col].shift(i)\n",
    "\n",
    "\n",
    "    # add rolling averages and stds\n",
    "    for col in label_cols:\n",
    "        for window in [7, 30]:\n",
    "            df[f'{col} D{window} roll mean'] = (\n",
    "                df[col]\n",
    "                .shift(1)               # so today's feature doesn't include today's price\n",
    "                .rolling(window=window, min_periods=1)  # you can require fewer points if you like\n",
    "                .mean()\n",
    "                )\n",
    "            df[f'{col} D{window} roll std'] = (\n",
    "                df[col]\n",
    "                .shift(1)               # so today's feature doesn't include today's price\n",
    "                .rolling(window=window, min_periods=1)  # you can require fewer points if you like\n",
    "                .std()\n",
    "            )\n",
    "\n",
    "    # add day of week\n",
    "    df['Day of Week'] = df['Gas Day'].dt.weekday\n",
    "    \n",
    "    # cyclic encoding for seasonality\n",
    "    df['Day of Year'] = df['Gas Day'].dt.dayofyear\n",
    "    df['sin_DoY'] = np.sin(2 * np.pi * df['Day of Year'] / 365)\n",
    "    df['cos_DoY'] = np.cos(2 * np.pi * df['Day of Year'] / 365)\n",
    "\n",
    "    # Add labels for next day's actuals\n",
    "    for col in label_cols:\n",
    "        df[f\"Next Day {col}\"] = df[col].shift(-1)\n",
    "\n",
    "    # There should be very few rows that have any NaNs so we can drop any that do\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_train_test(df, split_date, discard_before_date):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training set (gas days before split date) and test set (ga days fron the split date on)\n",
    "    \n",
    "    :param df: The DataFrame to split.\n",
    "    :param split_date: The date to split the DataFrame on.\n",
    "    :param discard_before_date: Discard anything before this date. Added to exclude time of Covid lockdowns.\n",
    "    :return: Tuple of (training set, testing set).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    train_df = df[df['Gas Day'].between(discard_before_date, split_date, inclusive = \"neither\")] \n",
    "    test_df = df[df['Gas Day'] >= split_date]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def split_with_test_half_of_last_half(df, discard_before_date):\n",
    "    df2 = df[df['Gas Day'] >= discard_before_date]\n",
    "    mid_date = df2['Gas Day'].mean()\n",
    "    first_half = df2[df2['Gas Day'] < mid_date]\n",
    "    second_half = df2[df2['Gas Day'] >= mid_date]\n",
    "    #Use all the earlier half, and half the later half, to train\n",
    "    # Use the other half of the later half to test\n",
    "    train_df, test_df = train_test_split(second_half, test_size=0.5, shuffle=True)\n",
    "    train_df = pd.concat([first_half, train_df])\n",
    "    return train_df, test_df\n",
    "\n",
    "def n_train_n_test(df, n_train, n_test, discard_before_date):\n",
    "    df.to_csv(\"..\\\\data\\\\processed\\\\all.csv\", index=False)\n",
    "    df = df[df['Gas Day'] >= discard_before_date]\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=n_test, train_size=n_train, shuffle=True)\n",
    "    train_df.to_csv(\"..\\\\data\\\\processed\\\\train.csv\", index=False)\n",
    "    test_df.to_csv(\"..\\\\data\\\\processed\\\\test.csv\", index=False)\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_X(df):\n",
    "    ys = [\"Next Day \" + col for col in label_cols]\n",
    "    df2 = df.drop(columns=ys)\n",
    "    df2.drop(columns=[\"Gas Day\"], inplace=True)\n",
    "    # experimentally - just include the price time series columns\n",
    "    #for col in df2.columns.tolist():      # iterate over a copy of the column list\n",
    "    #    if not is_price_column(col):             # if the substring isn’t found\n",
    "    #        df2.drop(columns=col, inplace=True)\n",
    "    #df2 = df2[label_cols]\n",
    "    #df2 = df2[[\"SAP\"]]\n",
    "    return df2    \n",
    "\n",
    "def is_price_column(column_name):\n",
    "    if \"SMP\" in column_name or \"SAP\" in column_name:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_y(df, col):\n",
    "    return df[\"Next Day \" + col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "9cbf2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error - penalises larger errors more than smaller ones\n",
    "def get_rmse(actuals, predictions):    \n",
    "    rmse =  np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    return round(rmse, 2)\n",
    "\n",
    "#Mean absolute percentage error\n",
    "def get_mape(actuals, predictions):\n",
    "    mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100\n",
    "    return round(mape, 2)\n",
    "\n",
    "\n",
    "def print_model_stats(model, X):\n",
    "\n",
    "    # 1. Coefficients and intercept\n",
    "    if hasattr(model, \"coef_\"):\n",
    "        #print(\"Coefficients:\", model.coef_)      # array of shape (n_features,)\n",
    "        cdf = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])\n",
    "        cdf = cdf.sort_values(by='Coefficients', ascending=False)\n",
    "        print(cdf)\n",
    "    if hasattr(model, \"intercept_\"):\n",
    "        print(\"Intercept:\", model.intercept_)    # scalar (or array if multi-output)\n",
    "\n",
    "    # 2. Model parameters\n",
    "    print(\"Parameters:\", model.get_params())\n",
    "\n",
    "    # 3. Data‐related attributes\n",
    "    #print(\"Number of features seen during fit:\", model.n_features_in_)\n",
    "    #if hasattr(model, \"feature_names_in_\"):\n",
    "    #    print(\"Feature names:\", model.feature_names_in_)\n",
    "\n",
    "    # 4. Linear algebra internals (rarely needed)\n",
    "    if hasattr(model, \"rank_\"):\n",
    "        print(\"Rank of design matrix:\", model.rank_)\n",
    "    if hasattr(model, \"singular_\"):\n",
    "        print(\"Singular values of X:\", model.singular_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a46e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 of 60 raw files\n",
      "Processed 20 of 60 raw files\n",
      "Processed 30 of 60 raw files\n",
      "Processed 40 of 60 raw files\n",
      "Processed 50 of 60 raw files\n",
      "Processed 60 of 60 raw files\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "download_raw_data() # uncomment this to download the data again\n",
    "df = load_data()\n",
    "\n",
    "# split on date, or random proportions\n",
    "#train, test = split_train_test(df, '2024-10-01', '2021-03-01')\n",
    "#train, test = n_train_n_test(df, n_train=250, n_test=50, discard_before_date='2021-03-01')\n",
    "train, test = n_train_n_test(df, n_train=0.7, n_test=0.3, discard_before_date='2021-03-01')\n",
    "#train = train[train['Gas Day'] > '2021-03-01'] \n",
    "#test = test[test['Gas Day'] > '2021-03-01'] \n",
    "#train, test = split_with_test_half_of_last_half(df, '2021-03-01')\n",
    "X_train = get_X(train)\n",
    "X_test = get_X(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8a0ec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous day's actual as a naive predictor\n",
    "\n",
    "def test_model(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = get_rmse(y, y_pred)\n",
    "    return rmse\n",
    "    \n",
    "def train_and_test_model(model, df_train, df_test, col):\n",
    "    X_train = get_X(df_train)\n",
    "    X_test = get_X(df_test)\n",
    "    y_train = get_y(df_train, col)\n",
    "    y_test = get_y(df_test, col)\n",
    "    #scaler = StandardScaler()\n",
    "    #X_train_scaled = scaler.fit_transform(X_train)\n",
    "    #X_test_scaled = scaler.fit_transform(X_test)\n",
    "    X_train_scaled = X_train\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    rmse_train = test_model(model, X_train_scaled, y_train)\n",
    "    rmse_test = test_model(model, X_test_scaled, y_test)\n",
    "    \n",
    "    return model, rmse_train, rmse_test\n",
    "\n",
    "def naive_predictions(df_train, df_test, col):\n",
    "    naive_predictions_train = df_train[col]\n",
    "    actuals_train = df_train[f\"Next Day {col}\"]\n",
    "    #mape_naive_train = get_mape(actuals_train, naive_predictions_train)\n",
    "    #print(f\"MAPE train (naive predictor) for {col}: {mape_naive_train}\")\n",
    "    rmse_naive_train = get_rmse(actuals_train, naive_predictions_train)\n",
    "    #print(f\"RMSE train (naive predictor) for {col}: {rmse_naive_train}\")\n",
    "\n",
    "    naive_predictions_test = df_test[col]\n",
    "    actuals_test = df_test[f\"Next Day {col}\"]\n",
    "    #mape_naive_test = get_mape(actuals_test, naive_predictions_test)\n",
    "    #print(f\"MAPE test (naive predictor) for {col}: {mape_naive_test}\")\n",
    "    rmse_naive_test = get_rmse(actuals_test, naive_predictions_test)\n",
    "    #print(f\"RMSE test (naive predictor) for {col}: {rmse_naive_test}\")\n",
    "    return rmse_naive_train, rmse_naive_test\n",
    "\n",
    "def print_results(case, rmse_naive, rmse_model):\n",
    "    headline = \"Worse\" if rmse_naive <= rmse_model else \"Better\"\n",
    "    print(f\"{case} - {headline} - model {rmse_model} v naive {rmse_naive}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "39429681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model:\n",
      "SAP train - Better - model 0.42 v naive 0.5\n",
      "SAP test - Worse - model 0.58 v naive 0.57\n",
      "SMPBuy train - Better - model 0.51 v naive 0.59\n",
      "SMPBuy test - Worse - model 0.72 v naive 0.67\n",
      "SMPSell train - Better - model 0.62 v naive 0.72\n",
      "SMPSell test - Worse - model 0.81 v naive 0.69\n"
     ]
    }
   ],
   "source": [
    "print (\"Linear regression model:\")\n",
    "for col in label_cols:\n",
    "    # Instantiate linear regression model.\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train and test it    \n",
    "    model, rmse_train, rmse_test = train_and_test_model(model, train, test, col)\n",
    "\n",
    "    # Print model details\n",
    "    X_train = get_X(train)\n",
    "    #print_model_stats(model, X_train)\n",
    "\n",
    "    # Get naive prediction stats for comparison\n",
    "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)   \n",
    "    \n",
    "    print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
    "    print_results(col + \" test\", rmse_naive_test, rmse_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bc25ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model:\n",
      "Train rows: 1047\n",
      "Test rows: 450\n",
      "Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "SAP train - Better - model 0.18 v naive 0.5\n",
      "SAP test - Worse - model 0.58 v naive 0.57\n",
      "Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "SMPBuy train - Better - model 0.22 v naive 0.59\n",
      "SMPBuy test - Worse - model 0.71 v naive 0.67\n",
      "Parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "SMPSell train - Better - model 0.27 v naive 0.72\n",
      "SMPSell test - Worse - model 0.75 v naive 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "print (\"Random forest model:\")\n",
    "print(\"Train rows:\", train.shape[0])\n",
    "print(\"Test rows:\", test.shape[0])\n",
    "for col in label_cols:\n",
    "    # Instantiate linear regression model.\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # Train and test it    \n",
    "    model, rmse_train, rmse_test = train_and_test_model(model, train, test, col)\n",
    "\n",
    "    # Print model details\n",
    "    X_train = get_X(train)\n",
    "    print_model_stats(model, X_train)\n",
    "\n",
    "    # Get naive prediction stats for comparison\n",
    "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)   \n",
    "    \n",
    "    print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
    "    print_results(col + \" test\", rmse_naive_test, rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "a51ccc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best hyperparameters: {'ccp_alpha': 0.001, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best CV RMSE on train set: 0.5316\n",
      "SAP test - Worse - model 0.73 v naive 0.57\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best hyperparameters: {'ccp_alpha': 0.001, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best CV RMSE on train set: 0.5316\n",
      "SMPBuy test - Worse - model 0.85 v naive 0.67\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
      "Best hyperparameters: {'ccp_alpha': 0.001, 'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best CV RMSE on train set: 0.5316\n",
      "SMPSell test - Worse - model 0.85 v naive 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#param_grid = {\n",
    "#    'n_estimators':     [100, 200, 500],\n",
    "#    'max_depth':        [None, 10, 20, 30],\n",
    "#    'min_samples_split':[2, 5, 10],\n",
    "#    'min_samples_leaf': [1, 2, 5],\n",
    "#    'max_features':     ['auto', 'sqrt', 0.3],\n",
    "#    'ccp_alpha':        [0.0, 0.001, 0.01]\n",
    "#}\n",
    "param_grid = {\n",
    "    'n_estimators':     [100, 200],\n",
    "    'max_depth':        [None, 10, 20],\n",
    "    'min_samples_split':[2, 5],\n",
    "    'min_samples_leaf': [1, 2],\n",
    "    'max_features':     ['sqrt'],\n",
    "    'ccp_alpha':        [0.001, 0.01]\n",
    "}\n",
    "for col in label_cols:\n",
    "        \n",
    "    rf = RandomForestRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        oob_score=True   # optional: get out‑of‑bag score on your train set\n",
    "    )\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=rf,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,                             # 5‑fold CV on X_train/y_train\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=3\n",
    "    )\n",
    "\n",
    "    X_train = get_X(train)\n",
    "    y_train = get_y(train, \"SAP\")\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best hyperparameters:\", grid.best_params_)\n",
    "    print(\"Best CV RMSE on train set: {:.4f}\".format(-grid.best_score_))\n",
    "\n",
    "\n",
    "    X_test = get_X(test)\n",
    "    y_test = get_y(test, col)\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 5. Evaluate the best model on the TEST set\n",
    "    # -----------------------------------------------------------------------------\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    rmse_test = test_model(best_model, X_test, y_test)\n",
    "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)   \n",
    "\n",
    "    print_results(col + \" test\", rmse_naive_test, rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f8c25",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m TEST_SIZE  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# split features/target\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     35\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# scale features\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'price'"
     ]
    }
   ],
   "source": [
    "# 1. Install the TCN layer\n",
    "#!pip install keras-tcn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tcn import TCN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 2. Suppose you already have a DataFrame `df` with:\n",
    "#    - 'Gas Day' datetime index or column\n",
    "#    - features (lags, rolling stats, calendar encodings, …)\n",
    "#    - target column 'price'\n",
    "\n",
    "# If 'Gas Day' is a column:\n",
    "# df['Gas Day'] = pd.to_datetime(df['Gas Day'])\n",
    "# df.set_index('Gas Day', inplace=True)\n",
    "\n",
    "# 3. Prepare sequences for TCN\n",
    "def make_sequences(X, y, seq_len):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - seq_len):\n",
    "        Xs.append(X[i : i + seq_len].values)\n",
    "        ys.append(y[i + seq_len])\n",
    "    return np.stack(Xs), np.array(ys)\n",
    "\n",
    "# parameters\n",
    "SEQ_LEN    = 30          # e.g. use past 30 days to predict next-day price\n",
    "TEST_SIZE  = 0.2\n",
    "\n",
    "# split features/target\n",
    "y = df['price']\n",
    "X = df.drop(columns=['price'])\n",
    "\n",
    "# scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns, index=X.index)\n",
    "\n",
    "# build sequences\n",
    "X_seq, y_seq = make_sequences(X_scaled, y, SEQ_LEN)\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_seq, y_seq, test_size=TEST_SIZE, shuffle=False\n",
    ")\n",
    "\n",
    "# 4. Build a simple TCN model\n",
    "n_features = X_train.shape[2]\n",
    "\n",
    "inputs = Input(shape=(SEQ_LEN, n_features))\n",
    "# TCN defaults: 64 filters, kernel_size=3, 8 stacks with exponentially increasing dilation\n",
    "tcn_layer = TCN(return_sequences=False)(inputs)\n",
    "output    = Dense(1)(tcn_layer)\n",
    "\n",
    "model = Model(inputs, output)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "\n",
    "# 5. Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=50,\n",
    "    batch_size=16,\n",
    "    callbacks=[\n",
    "        # optional: early stopping\n",
    "        # tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6. Evaluate & predict\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Test MAE: {mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
