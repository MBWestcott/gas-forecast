{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c3764d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a877cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_csv(url, output_file):\n",
    "    \"\"\"\n",
    "    Downloads a CSV file from the given URL and saves it to the specified file.\n",
    "    \n",
    "    :param url: URL to download the CSV data from.\n",
    "    :param output_file: Path to the local file where the CSV will be saved.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Send a GET request to the URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "\n",
    "        # Write the content (CSV data) to a file in binary mode\n",
    "        with open(output_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        \n",
    "        print(f\"CSV file has been successfully downloaded and saved as '{output_file}'.\")\n",
    "        \n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f4bd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_folder = \"..\\\\data\\\\raw\\\\\"\n",
    "# download the raw data from national gas data portal\n",
    "def download_raw_data():\n",
    "    with open(\"..\\\\PUB ids.txt\") as f:\n",
    "        pubIds = f.read()\n",
    "        pubIds = pubIds.replace(\"\\n\", \",\").strip() \n",
    "    \n",
    "    earliest = datetime.date(2020,4,1) # Download data going back 5 years\n",
    "    # Loop from week 0 (today) to week 13 (13 weeks ago)\n",
    "    download_from = datetime.date.today().replace(day=1) # start first download on first day of current month\n",
    "    download_to = datetime.date.today() # end first download on today's date\n",
    "    while(download_from > earliest):\n",
    "        \n",
    "        # Format the date in yyyy-mm-dd format\n",
    "        formatted_from = download_from.strftime(\"%Y-%m-%d\")\n",
    "        formatted_to = download_to.strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "        csv_url = f\"https://data.nationalgas.com/api/find-gas-data-download?applicableFor=Y&dateFrom={formatted_from}&dateTo={formatted_to}&dateType=GASDAY&latestFlag=Y&ids={pubIds}&type=CSV\"\n",
    "        month_format = download_from.strftime(\"%Y-%m\")\n",
    "        output_filename = f\"{raw_data_folder}{month_format}.csv\"\n",
    "\n",
    "        download_csv(csv_url, output_filename)\n",
    "        time.sleep(3) # brief courtesy sleep\n",
    "        download_to = download_from - datetime.timedelta(days=1) # next download should go up to the day before the previous download start date\n",
    "        download_from = download_to.replace(day=1) # next download should start on the first day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd682eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_cols = [\"SAP\", \"SMPBuy\", \"SMPSell\", \"Demand\"]\n",
    "\n",
    "def pivot(df, cols):\n",
    "\n",
    "    #only keep the values we are interested in\n",
    "    mask = df[\"Data Item\"].isin(cols)\n",
    "\n",
    "    df_filtered = df[mask]  \n",
    "\n",
    "    # if there are duplicates for the field and gas day, take the latest\n",
    "    df_latest = (\n",
    "        df_filtered\n",
    "        .sort_values(\"Applicable At\")\n",
    "        .groupby([\"Gas Day\", \"Data Item\"])\n",
    "        .last()  # this takes the row with the highest (i.e. latest) \"Applicable At\" per group\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # pivot to get 1 row per gas day\n",
    "    df_latest = df_latest.pivot(index=\"Gas Day\", columns=\"Data Item\", values=\"Value\").reset_index()\n",
    "    \n",
    "    # Drop 1 column that accounts for most of the NaNs\n",
    "    df_latest.drop(columns=[\"Composite Weather Variable - Actual\"], inplace=True)\n",
    "\n",
    "    return df_latest\n",
    "\n",
    "def load_data():\n",
    "    #Read raw CSVs\n",
    "    pathlist = list(Path(raw_data_folder).rglob('*.csv'))\n",
    "    file_count = len(pathlist)\n",
    "    dfs = []\n",
    "    files_done = 0\n",
    "    for path_obj in pathlist:\n",
    "        path = str(path_obj)   \n",
    "        \n",
    "        df = pd.read_csv(path,\n",
    "            parse_dates=[\"Applicable At\", \"Applicable For\", \"Generated Time\"],\n",
    "            dayfirst=True)\n",
    "\n",
    "        df.rename(columns={'Applicable For': 'Gas Day'}, inplace=True)\n",
    "        df['Gas Day'] = pd.to_datetime(df['Gas Day'], dayfirst=True)\n",
    "        # daily summary columns: \n",
    "\n",
    "        daily_cols = df[\"Data Item\"].unique()\n",
    "        # print(daily_cols)\n",
    "        # Get price and demand columns, to use as tommorow's ground truth, and with 1-3 days lag\n",
    "\n",
    "        #label_cols = [\"SAP, Actual Day\", \"SMP Buy, Actual Day\", \"SMP Sell, Actual Day\", \"Demand Actual, NTS, D+1\"]\n",
    "\n",
    "        #df_labels = pivot(df, label_cols)\n",
    "        \n",
    "\n",
    "        df_daily = pivot(df, daily_cols)\n",
    "        dfs.append(df_daily)\n",
    "        \n",
    "        files_done += 1\n",
    "        if files_done % 10 == 0:\n",
    "            print(f\"Processed {files_done} of {file_count} raw files\")\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    #Rename the columns that are going to be reused for ground truth and time series\n",
    "    df.rename(columns={\"SAP, Actual Day\": 'SAP', \"SMP Buy, Actual Day\": 'SMPBuy', \"SMP Sell, Actual Day\": 'SMPSell', \"Demand Actual, NTS, D+1\":\"Demand\"}, inplace=True)\n",
    "    \n",
    "    # add lagged features\n",
    "    lag_days = 5\n",
    "    for i in range(1, lag_days+1):\n",
    "        for col in label_cols:\n",
    "            df[f\"{col} D-{i}\"] = df[col].shift(i)\n",
    "\n",
    "\n",
    "    # add rolling averages and stds\n",
    "    for col in label_cols:\n",
    "        for window in [7, 30]:\n",
    "            df[f'{col} D{window} roll mean'] = (\n",
    "                df[col]\n",
    "                .shift(1)               # so today's feature doesn't include today's price\n",
    "                .rolling(window=window, min_periods=1)  # you can require fewer points if you like\n",
    "                .mean()\n",
    "                )\n",
    "            df[f'{col} D{window} roll std'] = (\n",
    "                df[col]\n",
    "                .shift(1)               # so today's feature doesn't include today's price\n",
    "                .rolling(window=window, min_periods=1)  # you can require fewer points if you like\n",
    "                .std()\n",
    "            )\n",
    "\n",
    "    # add day of week\n",
    "    df['Day of Week'] = df['Gas Day'].dt.weekday\n",
    "    \n",
    "    # Add labels for next day's actuals\n",
    "    for col in label_cols:\n",
    "        df[f\"Next Day {col}\"] = df[col].shift(-1)\n",
    "\n",
    "    # There should be very few rows that have any NaNs so we can drop any that do\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "def split_train_test(df, split_date):\n",
    "    \"\"\"\n",
    "    Splits the DataFrame into training set (gas days before split date) and test set (ga days fron the split date on)\n",
    "    \n",
    "    :param df: The DataFrame to split.\n",
    "    :param split_date: The date to split the DataFrame on.\n",
    "    :return: Tuple of (training set, testing set).\n",
    "    \"\"\"\n",
    "\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    train_df = df[df['Gas Day'] < split_date]\n",
    "    test_df = df[df['Gas Day'] >= split_date]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def n_train_n_test(df, n_train, n_test):\n",
    "    df.to_csv(\"..\\\\data\\\\processed\\\\all.csv\", index=False)\n",
    "    # Split the DataFrame into training and testing sets\n",
    "    train_df, test_df = train_test_split(df, test_size=n_test, train_size=n_train, shuffle=False)\n",
    "    train_df.to_csv(\"..\\\\data\\\\processed\\\\train.csv\", index=False)\n",
    "    test_df.to_csv(\"..\\\\data\\\\processed\\\\test.csv\", index=False)\n",
    "    return train_df, test_df\n",
    "\n",
    "def get_X(df):\n",
    "    ys = [\"Next Day \" + col for col in label_cols]\n",
    "    df2 = df.drop(columns=ys)\n",
    "    df2.drop(columns=[\"Gas Day\"], inplace=True)\n",
    "    # experimentally - just include the price time series columns\n",
    "    #for col in df2.columns.tolist():      # iterate over a copy of the column list\n",
    "    #    if not is_price_column(col):             # if the substring isn’t found\n",
    "    #        df2.drop(columns=col, inplace=True)\n",
    "    #df2 = df2[label_cols]\n",
    "    #df2 = df2[[\"SAP\"]]\n",
    "    return df2    \n",
    "\n",
    "def is_price_column(column_name):\n",
    "    if \"SMP\" in column_name or \"SAP\" in column_name:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_y(df, col):\n",
    "    return df[\"Next Day \" + col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9cbf2053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root mean squared error - penalises larger errors more than smaller ones\n",
    "def get_rmse(actuals, predictions):    \n",
    "    rmse =  np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    return round(rmse, 2)\n",
    "\n",
    "#Mean absolute percentage error\n",
    "def get_mape(actuals, predictions):\n",
    "    mape = np.mean(np.abs((predictions - actuals) / actuals)) * 100\n",
    "    return round(mape, 2)\n",
    "\n",
    "\n",
    "def print_model_stats(model, X):\n",
    "\n",
    "    cdf = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])\n",
    "    print(cdf)\n",
    "    # 1. Coefficients and intercept\n",
    "    print(\"Coefficients:\", model.coef_)      # array of shape (n_features,)\n",
    "    print(\"Intercept:\", model.intercept_)    # scalar (or array if multi-output)\n",
    "\n",
    "    # 2. Model parameters\n",
    "    print(\"Parameters:\", model.get_params())\n",
    "\n",
    "    # 3. Data‐related attributes\n",
    "    print(\"Number of features seen during fit:\", model.n_features_in_)\n",
    "    if hasattr(model, \"feature_names_in_\"):\n",
    "        print(\"Feature names:\", model.feature_names_in_)\n",
    "\n",
    "    # 4. Linear algebra internals (rarely needed)\n",
    "    print(\"Rank of design matrix:\", model.rank_)\n",
    "    print(\"Singular values of X:\", model.singular_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38a46e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 of 60 raw files\n",
      "Processed 20 of 60 raw files\n",
      "Processed 30 of 60 raw files\n",
      "Processed 40 of 60 raw files\n",
      "Processed 50 of 60 raw files\n",
      "Processed 60 of 60 raw files\n"
     ]
    }
   ],
   "source": [
    "#load data\n",
    "#download_raw_data() # uncomment this to download the data again\n",
    "df = load_data()\n",
    "\n",
    "# split on date, or random proportions\n",
    "#train, test = split_train_test(df, '2024-04-01')\n",
    "train, test = train_test_split(df, test_size=0.3, train_size=0.7, shuffle=True)\n",
    "X_train = get_X(train)\n",
    "X_test = get_X(test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8a0ec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the previous day's actual as a naive predictor\n",
    "\n",
    "def train_and_test_model(model, df_train, df_test, col):\n",
    "    X_train = get_X(df_train)\n",
    "    X_test = get_X(df_test)\n",
    "    y_train = get_y(df_train, col)\n",
    "    y_test = get_y(df_test, col)\n",
    "    #scaler = StandardScaler()\n",
    "    #X_train_scaled = scaler.fit_transform(X_train)\n",
    "    #X_test_scaled = scaler.fit_transform(X_test)\n",
    "    X_train_scaled = X_train\n",
    "    X_test_scaled = X_test\n",
    "\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred_train = model.predict(X_train_scaled)\n",
    "    rmse_train = get_rmse(y_train, y_pred_train)\n",
    "\n",
    "    y_pred_test = model.predict(X_test_scaled)\n",
    "    rmse_test = get_rmse(y_test, y_pred_test)\n",
    "\n",
    "    return model, rmse_train, rmse_test\n",
    "\n",
    "def naive_predictions(df_train, df_test, col):\n",
    "    naive_predictions_train = df_train[col]\n",
    "    actuals_train = df_train[f\"Next Day {col}\"]\n",
    "    #mape_naive_train = get_mape(actuals_train, naive_predictions_train)\n",
    "    #print(f\"MAPE train (naive predictor) for {col}: {mape_naive_train}\")\n",
    "    rmse_naive_train = get_rmse(actuals_train, naive_predictions_train)\n",
    "    #print(f\"RMSE train (naive predictor) for {col}: {rmse_naive_train}\")\n",
    "\n",
    "    naive_predictions_test = df_test[col]\n",
    "    actuals_test = df_test[f\"Next Day {col}\"]\n",
    "    #mape_naive_test = get_mape(actuals_test, naive_predictions_test)\n",
    "    #print(f\"MAPE test (naive predictor) for {col}: {mape_naive_test}\")\n",
    "    rmse_naive_test = get_rmse(actuals_test, naive_predictions_test)\n",
    "    #print(f\"RMSE test (naive predictor) for {col}: {rmse_naive_test}\")\n",
    "    return rmse_naive_train, rmse_naive_test\n",
    "\n",
    "def print_results(case, rmse_naive, rmse_model):\n",
    "    headline = \"Worse\" if rmse_naive <= rmse_model else \"Better\"\n",
    "    print(f\"{case} - {headline} - model {rmse_model} v naive {rmse_naive}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "39429681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model:\n",
      "SAP train - Better - model 0.43 v naive 0.48\n",
      "SAP test - Better - model 0.44 v naive 0.46\n",
      "SMPBuy train - Better - model 0.51 v naive 0.56\n",
      "SMPBuy test - Worse - model 0.58 v naive 0.58\n",
      "SMPSell train - Better - model 0.61 v naive 0.68\n",
      "SMPSell test - Worse - model 0.63 v naive 0.58\n",
      "Demand train - Better - model 16.09 v naive 18.11\n",
      "Demand test - Better - model 17.83 v naive 20.22\n"
     ]
    }
   ],
   "source": [
    "print (\"Linear regression model:\")\n",
    "for col in label_cols:\n",
    "    # Instantiate linear regression model.\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Train and test it    \n",
    "    model, rmse_train, rmse_test = train_and_test_model(model, train, test, col)\n",
    "\n",
    "    # Print model details\n",
    "    X_train = get_X(train)\n",
    "    #print_model_stats(model, X_train)\n",
    "\n",
    "    # Get naive prediction stats for comparison\n",
    "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)   \n",
    "    \n",
    "    print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
    "    print_results(col + \" test\", rmse_naive_test, rmse_test)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bc25ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest model:\n",
      "SAP train - Better - model 0.18 v naive 0.48\n",
      "SAP test - Better - model 0.43 v naive 0.46\n",
      "SMPBuy train - Better - model 0.21 v naive 0.56\n",
      "SMPBuy test - Worse - model 0.58 v naive 0.58\n",
      "SMPSell train - Better - model 0.26 v naive 0.68\n",
      "SMPSell test - Worse - model 0.63 v naive 0.58\n",
      "Demand train - Better - model 6.44 v naive 18.11\n",
      "Demand test - Better - model 17.56 v naive 20.22\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "print (\"Random forest model:\")\n",
    "for col in label_cols:\n",
    "    # Instantiate linear regression model.\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # Train and test it    \n",
    "    model, rmse_train, rmse_test = train_and_test_model(model, train, test, col)\n",
    "\n",
    "    # Print model details\n",
    "    X_train = get_X(train)\n",
    "    #print_model_stats(model, X_train)\n",
    "\n",
    "    # Get naive prediction stats for comparison\n",
    "    rmse_naive_train, rmse_naive_test = naive_predictions(train, test, col)   \n",
    "    \n",
    "    print_results(col + \" train\", rmse_naive_train, rmse_train)\n",
    "    print_results(col + \" test\", rmse_naive_test, rmse_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
