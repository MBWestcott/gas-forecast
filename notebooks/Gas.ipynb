{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94d228fc",
      "metadata": {},
      "source": [
        "## UK natural gas system price prediction project\n",
        "\n",
        "The purpose of this project is to investigate how well machine learning can predict a commodity price, given just a few market fundamentals, and previous prices, as features. I have chosen the UK natural gas market because the key data on supply, demand and prices are freely available and up to date via https://data.nationalgas.com/\n",
        "\n",
        "The goal is to predict the next day's daily System Average Price and System Marginal (Buy and Sell) Prices. The System Average Price is the volume weighted average price of trades on the UK natural gas On-the-Day Commodity Market - i.e. gas for immediate delivery. The System Marginal Price (Buy) is related to the day's highest price, and is the price that suppliers must pay for the balance of gas used by their customers, if that is more than the amount they have supplied to the system (a \"short imbalance\") The System Marginal Price (Sell) is related to the day's lowest price. The System Marginal Price (Sell) is related to the day's lowest price, and is the price that suppliers receive for any surplus gas that they have supplied to the system, which their customers have not used (a \"long imbalance\"). All prices are in pence per kilowatt-hour (p/kWh).\n",
        "\n",
        "The dataset is drawn from the five year history available at https://data.nationalgas.com/, focusing on the fields that make up the Daily Summary Report, with the data for the three target prices coming from \n",
        "\n",
        "Model performance will be measured based on Root Mean Squared Error (RMSE), as compared to the RMSE of a naive predictor that simply assumes that the next day's price will be the same as the current day's price. RMSE has been chosen as most suitable to price prediction because it penalises larger errors more harshly than smaller ones."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bbfe169",
      "metadata": {},
      "source": [
        "### Initial setup steps\n",
        "\n",
        "First we'll make sure the required libraries are available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "9c3764d3",
      "metadata": {
        "id": "9c3764d3"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import datetime\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "#!pip install scikit-optimize # needed on Google Colab\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557a2bba",
      "metadata": {},
      "source": [
        "Initial steps if running on Google Colab, to download support files from GitHub and set the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "vsukzbcUFNAQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsukzbcUFNAQ",
        "outputId": "d014ee2a-a929-41f8-94f9-5b56a384a2ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WinError 3] The system cannot find the path specified: '/content/gas-forecast/notebooks'\n",
            "d:\\dev\\gas-forecast\\notebooks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'gas-forecast'...\n"
          ]
        }
      ],
      "source": [
        "#for running on Colab\n",
        "!git clone https://github.com/MBWestcott/gas-forecast.git\n",
        "\n",
        "# 2. Change into the repo directory\n",
        "%cd /content/gas-forecast/notebooks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a4cfcb1",
      "metadata": {},
      "source": [
        "### First download the raw data from the National Gas data portal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "69f4bd0f",
      "metadata": {
        "id": "69f4bd0f"
      },
      "outputs": [],
      "source": [
        "raw_data_folder = Path(\"../data/raw/\")\n",
        "\n",
        "def download_csv(url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a CSV file from the given URL and saves it to the specified file.\n",
        "\n",
        "    :param url: URL to download the CSV data from.\n",
        "    :param output_file: Path to the local file where the CSV will be saved.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Ensure we notice bad responses\n",
        "\n",
        "        # Write the content (CSV data) to a file in binary mode\n",
        "        with open(output_file, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "\n",
        "        print(f\"CSV file has been successfully downloaded and saved as '{output_file}'.\")\n",
        "\n",
        "    except requests.HTTPError as http_err:\n",
        "        print(f\"HTTP error occurred: {http_err}\")\n",
        "    except Exception as err:\n",
        "        print(f\"An error occurred: {err}\")\n",
        "\n",
        "\n",
        "def download_raw_data():\n",
        "    pubIdsFile = Path(\"../PUB ids.txt\")\n",
        "    with open(pubIdsFile) as f:\n",
        "        pubIds = f.read()\n",
        "        pubIds = pubIds.replace(\"\\n\", \",\").strip()\n",
        "\n",
        "    earliest = datetime.date(2020,4,1) # Download data going back 5 years\n",
        "    \n",
        "    download_from = datetime.date.today().replace(day=1) # start first download on first day of current month\n",
        "    download_to = datetime.date.today() # end first download on today's date\n",
        "    while(download_from > earliest):\n",
        "\n",
        "        # Format the date in yyyy-mm-dd format\n",
        "        formatted_from = download_from.strftime(\"%Y-%m-%d\")\n",
        "        formatted_to = download_to.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "        csv_url = f\"https://data.nationalgas.com/api/find-gas-data-download?applicableFor=Y&dateFrom={formatted_from}&dateTo={formatted_to}&dateType=GASDAY&latestFlag=Y&ids={pubIds}&type=CSV\"\n",
        "        month_format = download_from.strftime(\"%Y-%m\")\n",
        "        output_filename = raw_data_folder /  f\"{month_format}.csv\"\n",
        "\n",
        "        download_csv(csv_url, output_filename)\n",
        "        time.sleep(2) # brief courtesy sleep\n",
        "        download_to = download_from - datetime.timedelta(days=1) # next download should go up to the day before the previous download start date\n",
        "        download_from = download_to.replace(day=1) # next download should start on the first day of the month\n",
        "\n",
        "# Do the download if the raw data is not there already\n",
        "csvCount = sum(1 for f in raw_data_folder.iterdir() if f.is_file() and f.suffix == '.csv')\n",
        "if(csvCount < 60):\n",
        "    download_raw_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a322d6",
      "metadata": {},
      "source": [
        "### Load the raw data\n",
        "\n",
        "Load the raw CSVs into a single and dataframe, pivot it so that each column represents a feature.\n",
        "Rename the Applicable At date field to Gas Day, and rename the columns that are going to be reused for ground truth and time series"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd682eab",
      "metadata": {
        "id": "dd682eab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1819 entries, 0 to 22\n",
            "Data columns (total 45 columns):\n",
            " #   Column                                                    Non-Null Count  Dtype         \n",
            "---  ------                                                    --------------  -----         \n",
            " 0   Gas Day                                                   1819 non-null   datetime64[ns]\n",
            " 1   Aggregate LNG Importations - Daily Flow                   1816 non-null   float64       \n",
            " 2   Beach Including Norway - Daily Flow                       1816 non-null   float64       \n",
            " 3   Beach and IOG - Beach Delivery                            1815 non-null   float64       \n",
            " 4   Beach and IOG - Daily Flow                                1816 non-null   float64       \n",
            " 5   Composite Weather Variable - Actual                       1554 non-null   float64       \n",
            " 6   Composite Weather Variable - Cold                         1818 non-null   float64       \n",
            " 7   Composite Weather Variable - Normal                       1818 non-null   float64       \n",
            " 8   Composite Weather Variable - Warm                         1817 non-null   float64       \n",
            " 9   Demand - Cold                                             1817 non-null   float64       \n",
            " 10  Demand - Cold, (excluding interconnector and storage)     1817 non-null   float64       \n",
            " 11  Demand - Warm                                             1817 non-null   float64       \n",
            " 12  Demand - Warm, (excluding interconnector and storage)     1817 non-null   float64       \n",
            " 13  Demand Actual, NTS, D+1                                   1817 non-null   float64       \n",
            " 14  Demand Forecast, NTS, hourly update                       1819 non-null   float64       \n",
            " 15  Demand, NTS, SND                                          1817 non-null   float64       \n",
            " 16  Demand, NTS, SND, (excluding interconnector and storage)  1817 non-null   float64       \n",
            " 17  Interconnector - Daily Flow                               1812 non-null   float64       \n",
            " 18  Interconnector - Delivery                                 1812 non-null   float64       \n",
            " 19  LNG Stock Level                                           1818 non-null   float64       \n",
            " 20  Long Storage - Actual Stock                               1817 non-null   float64       \n",
            " 21  Long Storage - Stock Level at Max Flow                    1819 non-null   float64       \n",
            " 22  Medium Storage - Actual Stock                             1817 non-null   float64       \n",
            " 23  Medium Storage - Stock Level at Max Flow                  1819 non-null   float64       \n",
            " 24  Predicted Closing Linepack (PCLP1)                        1819 non-null   float64       \n",
            " 25  SAP, 30 day rolling average                               1816 non-null   float64       \n",
            " 26  SAP, 7 Day rolling average                                1816 non-null   float64       \n",
            " 27  SAP                                                       1816 non-null   float64       \n",
            " 28  SMPBuy                                                    1816 non-null   float64       \n",
            " 29  SMPSell                                                   1816 non-null   float64       \n",
            " 30  Short Storage - Actual Stock                              1817 non-null   float64       \n",
            " 31  Short Storage - Stock Level at Max Flow                   1819 non-null   float64       \n",
            " 32  Storage - Daily Flow                                      1816 non-null   float64       \n",
            " 33  Storage - Delivery                                        1815 non-null   float64       \n",
            " 34  Storage, Long Range, Average flow (7 days)                1817 non-null   float64       \n",
            " 35  Storage, Long Range, Maximum potential flow               1817 non-null   float64       \n",
            " 36  Storage, Long Range, Stock Levels                         1817 non-null   float64       \n",
            " 37  Storage, Medium Range, Average flow (7 days)              1817 non-null   float64       \n",
            " 38  Storage, Medium Range, Maximum potential flow             1817 non-null   float64       \n",
            " 39  Storage, Medium Range, Stock Levels                       1817 non-null   float64       \n",
            " 40  Storage, Short Range, Average flow (7 days)               1817 non-null   float64       \n",
            " 41  Storage, Short Range, Maximum potential flow              1817 non-null   float64       \n",
            " 42  Storage, Short Range, Stock Levels                        1817 non-null   float64       \n",
            " 43  System Entry Flows, National, Forecast                    1819 non-null   float64       \n",
            " 44  System Entry Flows, National, Physical                    1819 non-null   float64       \n",
            "dtypes: datetime64[ns](1), float64(44)\n",
            "memory usage: 653.7 KB\n"
          ]
        }
      ],
      "source": [
        "price_targets = [\"SAP\", \"SMPBuy\", \"SMPSell\"]\n",
        "\n",
        "def pivot(df : pd.DataFrame, cols):\n",
        "\n",
        "    #only keep the values we are interested in\n",
        "    mask = df[\"Data Item\"].isin(cols)\n",
        "\n",
        "    df_filtered = df[mask]\n",
        "\n",
        "    # if there are duplicates for the field and gas day, take the latest\n",
        "    df_latest = (\n",
        "        df_filtered\n",
        "        .sort_values(\"Applicable At\")\n",
        "        .groupby([\"Gas Day\", \"Data Item\"])\n",
        "        .last()  # this takes the row with the highest (i.e. latest) \"Applicable At\" per group\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # pivot to get 1 row per gas day\n",
        "    df_latest = df_latest.pivot(index=\"Gas Day\", columns=\"Data Item\", values=\"Value\").reset_index()\n",
        "\n",
        "    df_latest = df_latest.sort_values(\"Gas Day\", ascending=True)\n",
        "\n",
        "    return df_latest\n",
        "\n",
        "def load_data():\n",
        "    #Read raw CSVs\n",
        "    pathlist = list(Path(raw_data_folder).rglob('*.csv'))\n",
        "    file_count = len(pathlist)\n",
        "    dfs = []\n",
        "    files_done = 0\n",
        "    for path_obj in pathlist:\n",
        "        path = str(path_obj)\n",
        "\n",
        "        df = pd.read_csv(path,\n",
        "            parse_dates=[\"Applicable At\", \"Applicable For\", \"Generated Time\"],\n",
        "            dayfirst=True)\n",
        "\n",
        "        df.rename(columns={'Applicable For': 'Gas Day'}, inplace=True)\n",
        "        df['Gas Day'] = pd.to_datetime(df['Gas Day'], dayfirst=True)\n",
        "        # pivot to 1 row per gas day, with features as columns\n",
        "\n",
        "        daily_cols = df[\"Data Item\"].unique()\n",
        "\n",
        "        df_daily = pivot(df, daily_cols)\n",
        "        dfs.append(df_daily)\n",
        "\n",
        "        files_done += 1\n",
        "        if files_done % 10 == 0:\n",
        "            print(f\"Processed {files_done} of {file_count} raw files\")\n",
        "\n",
        "    df = pd.concat(dfs)\n",
        "\n",
        "    #Rename the columns that are going to be reused for ground truth and time series\n",
        "    df.rename(columns={\"SAP, Actual Day\": 'SAP', \"SMP Buy, Actual Day\": 'SMPBuy', \"SMP Sell, Actual Day\": 'SMPSell'}, inplace=True)\n",
        "    return df\n",
        "\n",
        "df = load_data()\n",
        "df.to_csv(Path(\"../data/processed/pivoted.csv\"), index=False)\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52c8e618",
      "metadata": {},
      "source": [
        "### Preprocess data\n",
        "\n",
        "Add the previous 5 days' prices as lag features, and 7- and 30-day rolling averages and standard deviations. Also add day of week features, and a cyclical coding of the day of year for seasonality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "267c8bfe",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Composite Weather Variable - Cold</th>\n",
              "      <th>Composite Weather Variable - Normal</th>\n",
              "      <th>Composite Weather Variable - Warm</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>...</th>\n",
              "      <th>SMPSell D30 roll std</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "      <th>Next Day SAP</th>\n",
              "      <th>Next Day SMPBuy</th>\n",
              "      <th>Next Day SMPSell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2020-05-01</td>\n",
              "      <td>66.15110</td>\n",
              "      <td>135.26344</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>201.41454</td>\n",
              "      <td>10.5824</td>\n",
              "      <td>7.85</td>\n",
              "      <td>11.36</td>\n",
              "      <td>14.94</td>\n",
              "      <td>268.090483</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>122</td>\n",
              "      <td>0.863142</td>\n",
              "      <td>-0.504961</td>\n",
              "      <td>0.4770</td>\n",
              "      <td>0.5123</td>\n",
              "      <td>0.4417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2020-05-02</td>\n",
              "      <td>58.78630</td>\n",
              "      <td>131.66283</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>190.44913</td>\n",
              "      <td>11.3089</td>\n",
              "      <td>7.99</td>\n",
              "      <td>11.47</td>\n",
              "      <td>15.02</td>\n",
              "      <td>244.938804</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>123</td>\n",
              "      <td>0.854322</td>\n",
              "      <td>-0.519744</td>\n",
              "      <td>0.4840</td>\n",
              "      <td>0.5193</td>\n",
              "      <td>0.4487</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2020-05-03</td>\n",
              "      <td>56.55015</td>\n",
              "      <td>141.57363</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>198.12378</td>\n",
              "      <td>11.6531</td>\n",
              "      <td>8.13</td>\n",
              "      <td>11.55</td>\n",
              "      <td>15.09</td>\n",
              "      <td>242.854588</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003748</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>124</td>\n",
              "      <td>0.845249</td>\n",
              "      <td>-0.534373</td>\n",
              "      <td>0.4720</td>\n",
              "      <td>0.5073</td>\n",
              "      <td>0.4367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2020-05-04</td>\n",
              "      <td>52.82721</td>\n",
              "      <td>152.87412</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>205.70133</td>\n",
              "      <td>11.9252</td>\n",
              "      <td>8.26</td>\n",
              "      <td>11.65</td>\n",
              "      <td>15.14</td>\n",
              "      <td>242.098717</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006170</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>0.835925</td>\n",
              "      <td>-0.548843</td>\n",
              "      <td>0.4790</td>\n",
              "      <td>0.5143</td>\n",
              "      <td>0.4437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2020-05-05</td>\n",
              "      <td>62.21188</td>\n",
              "      <td>134.50813</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>196.72001</td>\n",
              "      <td>11.4803</td>\n",
              "      <td>8.40</td>\n",
              "      <td>11.76</td>\n",
              "      <td>15.18</td>\n",
              "      <td>247.112422</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005755</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>126</td>\n",
              "      <td>0.826354</td>\n",
              "      <td>-0.563151</td>\n",
              "      <td>0.5017</td>\n",
              "      <td>0.5370</td>\n",
              "      <td>0.4664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "0         2020-05-01                                 66.15110   \n",
              "1         2020-05-02                                 58.78630   \n",
              "2         2020-05-03                                 56.55015   \n",
              "3         2020-05-04                                 52.82721   \n",
              "4         2020-05-05                                 62.21188   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "0                                    135.26344   \n",
              "1                                    131.66283   \n",
              "2                                    141.57363   \n",
              "3                                    152.87412   \n",
              "4                                    134.50813   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "0                               201.41454                   201.41454   \n",
              "1                               190.44913                   190.44913   \n",
              "2                               198.12378                   198.12378   \n",
              "3                               205.70133                   205.70133   \n",
              "4                               196.72001                   196.72001   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  \\\n",
              "0                                      10.5824   \n",
              "1                                      11.3089   \n",
              "2                                      11.6531   \n",
              "3                                      11.9252   \n",
              "4                                      11.4803   \n",
              "\n",
              "Data Item  Composite Weather Variable - Cold  \\\n",
              "0                                       7.85   \n",
              "1                                       7.99   \n",
              "2                                       8.13   \n",
              "3                                       8.26   \n",
              "4                                       8.40   \n",
              "\n",
              "Data Item  Composite Weather Variable - Normal  \\\n",
              "0                                        11.36   \n",
              "1                                        11.47   \n",
              "2                                        11.55   \n",
              "3                                        11.65   \n",
              "4                                        11.76   \n",
              "\n",
              "Data Item  Composite Weather Variable - Warm  Demand - Cold  ...  \\\n",
              "0                                      14.94     268.090483  ...   \n",
              "1                                      15.02     244.938804  ...   \n",
              "2                                      15.09     242.854588  ...   \n",
              "3                                      15.14     242.098717  ...   \n",
              "4                                      15.18     247.112422  ...   \n",
              "\n",
              "Data Item  SMPSell D30 roll std  Day of Week  Is Weekday  Next Day Is Weekday  \\\n",
              "0                           NaN            4           1                    0   \n",
              "1                           NaN            5           0                    0   \n",
              "2                      0.003748            6           0                    1   \n",
              "3                      0.006170            0           1                    1   \n",
              "4                      0.005755            1           1                    1   \n",
              "\n",
              "Data Item  Day of Year   sin_DoY   cos_DoY  Next Day SAP  Next Day SMPBuy  \\\n",
              "0                  122  0.863142 -0.504961        0.4770           0.5123   \n",
              "1                  123  0.854322 -0.519744        0.4840           0.5193   \n",
              "2                  124  0.845249 -0.534373        0.4720           0.5073   \n",
              "3                  125  0.835925 -0.548843        0.4790           0.5143   \n",
              "4                  126  0.826354 -0.563151        0.5017           0.5370   \n",
              "\n",
              "Data Item  Next Day SMPSell  \n",
              "0                    0.4417  \n",
              "1                    0.4487  \n",
              "2                    0.4367  \n",
              "3                    0.4437  \n",
              "4                    0.4664  \n",
              "\n",
              "[5 rows x 81 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess(df: pd.DataFrame, add_lags=True, add_labels=True):\n",
        "\n",
        "    \"\"\"Deal with missing values, add lagged features, rolling averages and stds, Day of Week, and cyclic encoding for seasonality\"\"\"\n",
        "\n",
        "    if add_lags:\n",
        "        lag_days = 5\n",
        "        for i in range(1, lag_days+1):\n",
        "            for pt in price_targets:\n",
        "                df[f\"{pt} D-{i}\"] = df[pt].shift(i)\n",
        "\n",
        "        # add rolling averages and stds\n",
        "        for pt in price_targets:\n",
        "            for window in [7, 30]:\n",
        "                df[f'{pt} D{window} roll mean'] = (\n",
        "                    df[pt]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .mean()\n",
        "                    )\n",
        "                df[f'{pt} D{window} roll std'] = (\n",
        "                    df[pt]\n",
        "                    .shift(1)               # so today's feature doesn't include today's price\n",
        "                    .rolling(window=window, min_periods=1)  \n",
        "                    .std()\n",
        "                )\n",
        "\n",
        "    # add day of week\n",
        "    df['Day of Week'] = df['Gas Day'].dt.weekday\n",
        "    df['Is Weekday'] = (df['Gas Day'].dt.weekday < 5).astype(int)\n",
        "    df['Next Day Is Weekday'] = ((df['Gas Day'] + pd.Timedelta(days=1)).dt.weekday < 5).astype(int)\n",
        "    # cyclic encoding for seasonality\n",
        "    df['Day of Year'] = df['Gas Day'].dt.dayofyear\n",
        "    df['sin_DoY'] = np.sin(2 * np.pi * df['Day of Year'] / 365)\n",
        "    df['cos_DoY'] = np.cos(2 * np.pi * df['Day of Year'] / 365)\n",
        "\n",
        "    if add_labels:\n",
        "        # Add labels for next day's actuals\n",
        "        for pt in price_targets:\n",
        "            df[f\"Next Day {pt}\"] = df[pt].shift(-1)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = preprocess(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff3358d8",
      "metadata": {},
      "source": [
        "### Clean missing values and outliers\n",
        "\n",
        "Most of the missing values are missing \"Composite Weather Variable - Actual\" from 2020-21. These affect around 15% of the dataset. Best way to fill in those is with the Normal forecast, which should usually be the closest. Apart from that there are very few missing readings so it is feasible to discard any remaining rows with missing data (done at the end, to avoid introducing errors into the lag features)\n",
        "\n",
        "Also remove outliers where any of the prices was 0, and one of the next day prices was more 50% away from the current day's price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c69e5a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1784, 78)\n",
            "(1784, 78)\n",
            "(1783, 78)\n",
            "(1783, 78)\n",
            "(1762, 78)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>SMPSell D30 roll std</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "      <th>Next Day SAP</th>\n",
              "      <th>Next Day SMPBuy</th>\n",
              "      <th>Next Day SMPSell</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2020-05-06</td>\n",
              "      <td>50.23730</td>\n",
              "      <td>142.20118</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>192.43848</td>\n",
              "      <td>12.0645</td>\n",
              "      <td>245.399580</td>\n",
              "      <td>216.150490</td>\n",
              "      <td>145.432835</td>\n",
              "      <td>116.183744</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005142</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>127</td>\n",
              "      <td>0.816538</td>\n",
              "      <td>-0.577292</td>\n",
              "      <td>0.4834</td>\n",
              "      <td>0.5187</td>\n",
              "      <td>0.4481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>2020-05-07</td>\n",
              "      <td>53.59770</td>\n",
              "      <td>141.87295</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>195.47065</td>\n",
              "      <td>13.4655</td>\n",
              "      <td>243.927941</td>\n",
              "      <td>214.341578</td>\n",
              "      <td>145.153439</td>\n",
              "      <td>115.567076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011180</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>128</td>\n",
              "      <td>0.806480</td>\n",
              "      <td>-0.591261</td>\n",
              "      <td>0.4756</td>\n",
              "      <td>0.5109</td>\n",
              "      <td>0.4403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>2020-05-08</td>\n",
              "      <td>51.08822</td>\n",
              "      <td>135.19872</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>186.28694</td>\n",
              "      <td>15.5400</td>\n",
              "      <td>171.000000</td>\n",
              "      <td>131.000000</td>\n",
              "      <td>154.000000</td>\n",
              "      <td>114.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010249</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>129</td>\n",
              "      <td>0.796183</td>\n",
              "      <td>-0.605056</td>\n",
              "      <td>0.4722</td>\n",
              "      <td>0.5075</td>\n",
              "      <td>0.4369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2020-05-09</td>\n",
              "      <td>53.28634</td>\n",
              "      <td>127.04213</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>180.32847</td>\n",
              "      <td>15.0800</td>\n",
              "      <td>172.000000</td>\n",
              "      <td>134.000000</td>\n",
              "      <td>141.000000</td>\n",
              "      <td>104.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009697</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>130</td>\n",
              "      <td>0.785650</td>\n",
              "      <td>-0.618671</td>\n",
              "      <td>0.4615</td>\n",
              "      <td>0.4968</td>\n",
              "      <td>0.4262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020-05-10</td>\n",
              "      <td>53.14522</td>\n",
              "      <td>127.29315</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>180.43837</td>\n",
              "      <td>12.6200</td>\n",
              "      <td>264.115666</td>\n",
              "      <td>226.223933</td>\n",
              "      <td>180.336933</td>\n",
              "      <td>142.445199</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009489</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>0.774884</td>\n",
              "      <td>-0.632103</td>\n",
              "      <td>0.4569</td>\n",
              "      <td>0.4922</td>\n",
              "      <td>0.4216</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 78 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "5         2020-05-06                                 50.23730   \n",
              "6         2020-05-07                                 53.59770   \n",
              "7         2020-05-08                                 51.08822   \n",
              "8         2020-05-09                                 53.28634   \n",
              "9         2020-05-10                                 53.14522   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "5                                    142.20118   \n",
              "6                                    141.87295   \n",
              "7                                    135.19872   \n",
              "8                                    127.04213   \n",
              "9                                    127.29315   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "5                               192.43848                   192.43848   \n",
              "6                               195.47065                   195.47065   \n",
              "7                               186.28694                   186.28694   \n",
              "8                               180.32847                   180.32847   \n",
              "9                               180.43837                   180.43837   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "5                                      12.0645     245.399580   \n",
              "6                                      13.4655     243.927941   \n",
              "7                                      15.5400     171.000000   \n",
              "8                                      15.0800     172.000000   \n",
              "9                                      12.6200     264.115666   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "5                                                 216.150490       \n",
              "6                                                 214.341578       \n",
              "7                                                 131.000000       \n",
              "8                                                 134.000000       \n",
              "9                                                 226.223933       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "5             145.432835   \n",
              "6             145.153439   \n",
              "7             154.000000   \n",
              "8             141.000000   \n",
              "9             180.336933   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "5                                                 116.183744      ...   \n",
              "6                                                 115.567076      ...   \n",
              "7                                                 114.000000      ...   \n",
              "8                                                 104.000000      ...   \n",
              "9                                                 142.445199      ...   \n",
              "\n",
              "Data Item  SMPSell D30 roll std  Day of Week  Is Weekday  Next Day Is Weekday  \\\n",
              "5                      0.005142            2           1                    1   \n",
              "6                      0.011180            3           1                    1   \n",
              "7                      0.010249            4           1                    0   \n",
              "8                      0.009697            5           0                    0   \n",
              "9                      0.009489            6           0                    1   \n",
              "\n",
              "Data Item  Day of Year   sin_DoY   cos_DoY  Next Day SAP  Next Day SMPBuy  \\\n",
              "5                  127  0.816538 -0.577292        0.4834           0.5187   \n",
              "6                  128  0.806480 -0.591261        0.4756           0.5109   \n",
              "7                  129  0.796183 -0.605056        0.4722           0.5075   \n",
              "8                  130  0.785650 -0.618671        0.4615           0.4968   \n",
              "9                  131  0.774884 -0.632103        0.4569           0.4922   \n",
              "\n",
              "Data Item  Next Day SMPSell  \n",
              "5                    0.4481  \n",
              "6                    0.4403  \n",
              "7                    0.4369  \n",
              "8                    0.4262  \n",
              "9                    0.4216  \n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean(df: pd.DataFrame, remove_outliers=True):\n",
        "    # fill missing CWV actuals with the normal forecast\n",
        "    df['Composite Weather Variable - Actual'] = df['Composite Weather Variable - Actual'].fillna(df['Composite Weather Variable - Normal'])\n",
        "\n",
        "    # There should be very remaining few rows that have any NaNs so we can drop any that do\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "    # Can drop the composite weather forecasts\n",
        "    df.drop(columns=[\"Composite Weather Variable - Normal\", \"Composite Weather Variable - Cold\", \"Composite Weather Variable - Warm\"], inplace=True)\n",
        "\n",
        "    if(remove_outliers):\n",
        "        for pt in price_targets:    \n",
        "            # remove outliers where any of the prices was 0\n",
        "            print(df.shape)\n",
        "            df = df[df[pt] != 0]\n",
        "            print(df.shape)\n",
        "            df = df[df[f\"Next Day {pt}\"] != 0]\n",
        "            print(df.shape)\n",
        "            #... and where the next day price is more than least 50% away from the current day's price\n",
        "            df = df[abs(df[pt] - df[f\"Next Day {pt}\"])/df[pt] < 0.5]\n",
        "            print(df.shape)\n",
        "    return df    \n",
        "\n",
        "df = clean(df)\n",
        "df.to_csv(Path(\"../data/processed/preprocessed_and_cleaned.csv\"), index=False)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821e51d3",
      "metadata": {},
      "source": [
        "### Split the data into training and test sets\n",
        "Using two configurations:\n",
        "- Split the data by date - earliest portion to train, then later portion to validate, and the last to test. Designed to test whether the model will generalise to the most recent period, despite having been trained on earlier periods\n",
        "- Split the data randomly regardless of date\n",
        "\n",
        "By default, discard the earliest data from training, which coincided with Covid restrictions as experimentally this seems to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6118dc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def split_sequential(df, n_train = 0.7, n_validate = 0.2, n_test = 0.1, discard_before_date = '2021-04-01'):\n",
        "    \"\"\"Split based on date\"\"\"\n",
        "    # Convert the 'Gas Day' column to datetime if it's not already  \n",
        "    \n",
        "    df_sorted = df.sort_values(\"Gas Day\", ascending=True)\n",
        "    df_filtered = df_sorted[df_sorted['Gas Day'] >= discard_before_date]\n",
        "    train_df, vt_df = train_test_split(df_filtered, test_size=n_validate + n_test, train_size=n_train, shuffle=False)\n",
        "    validate_df, test_df = train_test_split(vt_df, test_size=n_test/(n_validate + n_test), train_size=n_validate/(n_validate + n_test), shuffle=False)\n",
        "    \n",
        "    return train_df, validate_df, test_df\n",
        "\n",
        "def split_random(df, n_train = 0.7, n_validate = 0.2, n_test = 0.1, discard_before_date = '2021-04-01'):\n",
        "    \"\"\"Split based on number or fraction of rows\"\"\"\n",
        "    \n",
        "    df_filtered = df[df['Gas Day'] >= discard_before_date]\n",
        "    # Split the DataFrame into training and testing sets\n",
        "    train_df, vt_df = train_test_split(df_filtered, test_size=n_validate + n_test, train_size=n_train, shuffle=True)\n",
        "    validate_df, test_df = train_test_split(vt_df, test_size=n_test/(n_validate + n_test), train_size=n_validate/(n_validate + n_test), shuffle=True)\n",
        "    \n",
        "    return train_df, validate_df, test_df\n",
        "\n",
        "def get_X(df):\n",
        "    ys = [\"Next Day \" + col for col in price_targets]\n",
        "    df2 = df.drop(columns=ys)\n",
        "    df2.drop(columns=[\"Gas Day\"], inplace=True)\n",
        "    \n",
        "    return df2\n",
        "\n",
        "#train, validate, test = split_sequential(df,0.7, 0.2, 0.1, '2023-09-01')\n",
        "#X_train = get_X(train)\n",
        "#X_test = get_X(test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59aabed2",
      "metadata": {},
      "source": [
        "### Use Root Mean Squared Error as the measure of accuracy\n",
        "\n",
        "This is appropriate to price forecasting because it penalises larger inaccuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "9cbf2053",
      "metadata": {
        "id": "9cbf2053"
      },
      "outputs": [],
      "source": [
        "# Root mean squared error - penalises larger errors more than smaller ones\n",
        "def get_rmse(actuals, predictions):\n",
        "    rmse =  np.sqrt(np.mean((predictions - actuals)**2))\n",
        "    return round(rmse, 4)\n",
        "\n",
        "def print_model_stats(model, X):\n",
        "\n",
        "    # 1. Coefficients and intercept\n",
        "    if hasattr(model, \"coef_\"):\n",
        "        #print(\"Coefficients:\", model.coef_)      # array of shape (n_features,)\n",
        "        cdf = pd.DataFrame(model.coef_, X.columns, columns=['Coefficients'])\n",
        "        cdf = cdf.sort_values(by='Coefficients', ascending=False)\n",
        "        print(cdf)\n",
        "    if hasattr(model, \"intercept_\"):\n",
        "        print(\"Intercept:\", model.intercept_)    # scalar (or array if multi-output)\n",
        "\n",
        "    # 2. Model parameters\n",
        "    print(\"Parameters:\", model.get_params())\n",
        "\n",
        "    # 3. Linear algebra internals\n",
        "    if hasattr(model, \"rank_\"):\n",
        "        print(\"Rank of design matrix:\", model.rank_)\n",
        "    if hasattr(model, \"singular_\"):\n",
        "        print(\"Singular values of X:\", model.singular_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe56398",
      "metadata": {},
      "source": [
        "### Set up a framework to train models, and compare their performance on the test dataset against a naive predictor\n",
        "\n",
        "The naive predictor takes the current day's System Average Price and System Marginal Prices as the predictions for the next day"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a0ec407",
      "metadata": {
        "id": "8a0ec407"
      },
      "outputs": [],
      "source": [
        "SPLIT_RANDOM = \"Random\"\n",
        "SPLIT_SEQUENTIAL = \"Sequential\"\n",
        "\n",
        "class Context:\n",
        "    \"\"\"Context for a model evaluation\"\"\"\n",
        "\n",
        "    def __init__(self, model_type, test_set):\n",
        "        self.model_type = model_type\n",
        "        self.test_set = test_set\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Context(model_type={self.model_type}, test_set={self.test_set})\"\n",
        "    \n",
        "class Result:\n",
        "    \"\"\"Result of a model evaluation\"\"\"\n",
        "    \n",
        "    def __init__(self, context:Context, price_label, model_rmse, naive_rmse):\n",
        "        self.context = context\n",
        "        self.price_label = price_label\n",
        "        self.model_rmse = model_rmse\n",
        "        self.naive_rmse = naive_rmse\n",
        "        self.timestamp = datetime.datetime.now()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"GasPredictResult(context={self.context}, price_label={self.price_label}, model_rmse={self.model_rmse}, naive_rmse={self.naive_rmse}, timestamp={self.timestamp})\"    \n",
        "\n",
        "def get_y(df, col):\n",
        "    return df[\"Next Day \" + col]\n",
        "\n",
        "def validate_model(model, X, y):\n",
        "    y_pred = model.predict(X)\n",
        "    rmse = get_rmse(y, y_pred)\n",
        "    return rmse\n",
        "\n",
        "def train_and_validate_model(model, df_train, df_validate, col):\n",
        "    X_train = get_X(df_train)\n",
        "    X_validate = get_X(df_validate)\n",
        "    y_train = get_y(df_train, col)\n",
        "    y_validate = get_y(df_validate, col)\n",
        "    #scaler = StandardScaler()\n",
        "    #X_train_scaled = scaler.fit_transform(X_train)\n",
        "    #X_test_scaled = scaler.fit_transform(X_test)\n",
        "    #X_train_scaled = X_train\n",
        "    #X_validate_scaled = X_validate\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    rmse_train = validate_model(model, X_train, y_train)\n",
        "    rmse_validate = validate_model(model, X_validate, y_validate)\n",
        "\n",
        "    return model, rmse_train, rmse_validate\n",
        "\n",
        "def train_validate_and_report_for_prices(model_factory, df_train: pd.DataFrame, df_validate: pd.DataFrame, context:Context, print_model_stats=True):\n",
        "    results = []\n",
        "    for pt in price_targets:\n",
        "        # Instantiate model.\n",
        "        model = model_factory()\n",
        "\n",
        "        # Train and test it\n",
        "        model, rmse_train, rmse_validate = train_and_validate_model(model, df_train, df_validate, pt)\n",
        "\n",
        "        # Print model details\n",
        "        if print_model_stats:\n",
        "            X_train = get_X(df_train)\n",
        "            print_model_stats(model, X_train)\n",
        "\n",
        "        # Get naive prediction stats for comparison\n",
        "        rmse_naive_train = naive_predictions(df_train, pt)\n",
        "        rmse_naive_validate = naive_predictions(df_validate, pt)\n",
        "\n",
        "        print_results(pt + \" train\", rmse_naive_train, rmse_train)\n",
        "        print_results(pt + \" validate\", rmse_naive_validate, rmse_validate)\n",
        "\n",
        "        testResult = Result(context, pt, rmse_validate, rmse_naive_validate)\n",
        "        results.append(testResult)\n",
        "    return results\n",
        "\n",
        "def naive_predictions(df, priceTarget):\n",
        "    naive_predictions = df[priceTarget]\n",
        "    actuals = df[f\"Next Day {priceTarget}\"]\n",
        "    return get_rmse(actuals, naive_predictions)\n",
        "\n",
        "def print_results(case, rmse_naive, rmse_model):\n",
        "    headline = \"Worse\" if rmse_naive <= rmse_model else \"Better\"\n",
        "    print(f\"{case} - {headline} - model {rmse_model} v naive {rmse_naive}\")\n",
        "\n",
        "\n",
        "all_results = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75cb01d0",
      "metadata": {},
      "source": [
        "### Try linear regression models\n",
        "\n",
        "...to predict each of SAP (System Average Price), SMPBuy (System Marginal Price - Buy) and SMPSell (System Marginal Price - Sell). This generally performs worse than the naive predictor in testing, especially using a date-based split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "39429681",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39429681",
        "outputId": "9a1f327b-ccba-4a85-f298-0126ade5c540"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear regression model:\n",
            "Using random train-validate-test split...\n",
            "SAP train - Better - model 0.3678 v naive 0.4575\n",
            "SAP validate - Better - model 0.4737 v naive 0.4908\n",
            "SMPBuy train - Better - model 0.4424 v naive 0.5233\n",
            "SMPBuy validate - Better - model 0.5469 v naive 0.5526\n",
            "SMPSell train - Better - model 0.5243 v naive 0.6093\n",
            "SMPSell validate - Worse - model 0.7367 v naive 0.633\n",
            "Using sequential train-validate-test split...\n",
            "SAP train - Better - model 0.4455 v naive 0.549\n",
            "SAP validate - Worse - model 0.2533 v naive 0.0724\n",
            "SMPBuy train - Better - model 0.5346 v naive 0.6244\n",
            "SMPBuy validate - Worse - model 0.3255 v naive 0.0824\n",
            "SMPSell train - Better - model 0.6468 v naive 0.7189\n",
            "SMPSell validate - Worse - model 0.4198 v naive 0.0738\n",
            "[GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SAP, model_rmse=0.4737, naive_rmse=0.4908, timestamp=2025-04-25 09:31:22.311062), GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SMPBuy, model_rmse=0.5469, naive_rmse=0.5526, timestamp=2025-04-25 09:31:22.345149), GasPredictResult(context=Context(model_type=Linear regression, test_set=Random), price_label=SMPSell, model_rmse=0.7367, naive_rmse=0.633, timestamp=2025-04-25 09:31:22.364297), GasPredictResult(context=Context(model_type=Linear regression, test_set=Sequential), price_label=SAP, model_rmse=0.2533, naive_rmse=0.0724, timestamp=2025-04-25 09:31:22.395547), GasPredictResult(context=Context(model_type=Linear regression, test_set=Sequential), price_label=SMPBuy, model_rmse=0.3255, naive_rmse=0.0824, timestamp=2025-04-25 09:31:22.411174), GasPredictResult(context=Context(model_type=Linear regression, test_set=Sequential), price_label=SMPSell, model_rmse=0.4198, naive_rmse=0.0738, timestamp=2025-04-25 09:31:22.451247)]\n"
          ]
        }
      ],
      "source": [
        "print (\"Linear regression model:\")\n",
        "model_factory = lambda: LinearRegression()\n",
        "\n",
        "print(\"Using random train-validate-test split...\")\n",
        "context = Context(\"Linear regression\", SPLIT_RANDOM)\n",
        "train, validate, test = split_random(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using sequential train-validate-test split...\")\n",
        "context = Context(\"Linear regression\", SPLIT_SEQUENTIAL)\n",
        "train, validate, test = split_sequential(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n",
        "\n",
        "print(all_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2dd9746",
      "metadata": {},
      "source": [
        "### Try a random forest model\n",
        "Linear regression generally performed worse than the naive predictor in testing, especially using a date-based split, so let's try a random forest model. The hyperparameters for the best version were obtained by random search in the second code block below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "bc25ac1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc25ac1d",
        "outputId": "896fff9c-bc0c-4214-9d84-c79f832d572a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest model:\n",
            "Using random train-validate-test split...\n",
            "SAP train - Better - model 0.1835 v naive 0.4222\n",
            "SAP validate - Worse - model 0.5861 v naive 0.5534\n",
            "SMPBuy train - Better - model 0.223 v naive 0.4987\n",
            "SMPBuy validate - Worse - model 0.6367 v naive 0.5954\n",
            "SMPSell train - Better - model 0.2741 v naive 0.586\n",
            "SMPSell validate - Worse - model 0.7464 v naive 0.659\n",
            "Using sequential train-validate-test split...\n",
            "SAP train - Better - model 0.2155 v naive 0.549\n",
            "SAP validate - Worse - model 0.0912 v naive 0.0724\n",
            "SMPBuy train - Better - model 0.2582 v naive 0.6244\n",
            "SMPBuy validate - Worse - model 0.1083 v naive 0.0824\n",
            "SMPSell train - Better - model 0.3196 v naive 0.7189\n",
            "SMPSell validate - Worse - model 0.1095 v naive 0.0738\n"
          ]
        }
      ],
      "source": [
        "print (\"Random forest model:\")\n",
        "#RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0) # best from random searh\n",
        "#RandomForestRegressor(n_estimators = 200, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.7, max_depth = 20, ccp_alpha = 0.0) # best for SAP: SAP test - Better - model 0.77 v naive 0.8\n",
        "model_factory = lambda: RandomForestRegressor(n_estimators = 500, min_samples_split = 2, min_samples_leaf= 2, max_features = 0.9, max_depth = 20, ccp_alpha = 0.0)\n",
        "\n",
        "print(\"Using random train-validate-test split...\")\n",
        "context = Context(\"Random forest\", SPLIT_RANDOM)\n",
        "train, validate, test = split_random(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using sequential train-validate-test split...\")\n",
        "context = Context(\"Random forest\", SPLIT_SEQUENTIAL)\n",
        "train, validate, test = split_sequential(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24774087",
      "metadata": {},
      "source": [
        "### Next, try gradient boosting\n",
        "Again the random forest improves in test slightly on a random split but not when trained on earlier data and tested on later. Let's try tree-based gradient boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0cecf345",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting model (XGBoost XGBRegressor):\n",
            "Using random train-validate-test split...\n",
            "SAP train - Better - model 0.0231 v naive 0.4558\n",
            "SAP validate - Better - model 0.386 v naive 0.4248\n",
            "SMPBuy train - Better - model 0.0254 v naive 0.5266\n",
            "SMPBuy validate - Better - model 0.4474 v naive 0.4827\n",
            "SMPSell train - Better - model 0.0321 v naive 0.573\n",
            "SMPSell validate - Better - model 0.5738 v naive 0.6944\n",
            "Using sequential train-validate-test split...\n",
            "SAP train - Better - model 0.0275 v naive 0.549\n",
            "SAP validate - Worse - model 0.1391 v naive 0.0724\n",
            "SMPBuy train - Better - model 0.0322 v naive 0.6244\n",
            "SMPBuy validate - Worse - model 0.1433 v naive 0.0824\n",
            "SMPSell train - Better - model 0.0346 v naive 0.7189\n",
            "SMPSell validate - Worse - model 0.1146 v naive 0.0738\n"
          ]
        }
      ],
      "source": [
        "print(\"Gradient Boosting model (XGBoost XGBRegressor):\")\n",
        "\n",
        "model_factory = lambda: XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.7,\n",
        "        reg_alpha=0.0,\n",
        "        reg_lambda=1.0,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "print(\"Using random train-validate-test split...\")\n",
        "context = Context(\"Gradient boosting\", SPLIT_RANDOM)\n",
        "train, validate, test = split_random(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n",
        "\n",
        "print(\"Using sequential train-validate-test split...\")\n",
        "context = Context(\"Gradient boosting\", SPLIT_SEQUENTIAL)\n",
        "train, validate, test = split_sequential(df)\n",
        "all_results += train_validate_and_report_for_prices(model_factory, train, validate, context, print_model_stats=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dab84acc",
      "metadata": {},
      "source": [
        "### Try Recurrent Neural Network\n",
        "\n",
        "The gradient booster likewise did not perform any better than the naive predictor, especially when trained on the earlier data and tested on the later data. Let's try a neural net. For a simple time series, a Temporal Convolutional Net would be the obvious choice, but in this case we have a lot of market fundamentals to use as additional features so a RNN seems the better fit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2834c00b",
      "metadata": {},
      "source": [
        "(1) Because of how the inputs need to be shaped into sequences, we'll load the data again, skipping the manually-engineered lag features and next-day labels. We'll still fill in missing actual Composite Weather Variables with the normal forecast, but won't delete the few with outlying prices in case the RNN is sophisticated enough to make good use of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "44d0ce63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Data Item</th>\n",
              "      <th>Gas Day</th>\n",
              "      <th>Aggregate LNG Importations - Daily Flow</th>\n",
              "      <th>Beach Including Norway - Daily Flow</th>\n",
              "      <th>Beach and IOG - Beach Delivery</th>\n",
              "      <th>Beach and IOG - Daily Flow</th>\n",
              "      <th>Composite Weather Variable - Actual</th>\n",
              "      <th>Demand - Cold</th>\n",
              "      <th>Demand - Cold, (excluding interconnector and storage)</th>\n",
              "      <th>Demand - Warm</th>\n",
              "      <th>Demand - Warm, (excluding interconnector and storage)</th>\n",
              "      <th>...</th>\n",
              "      <th>Storage, Short Range, Maximum potential flow</th>\n",
              "      <th>Storage, Short Range, Stock Levels</th>\n",
              "      <th>System Entry Flows, National, Forecast</th>\n",
              "      <th>System Entry Flows, National, Physical</th>\n",
              "      <th>Day of Week</th>\n",
              "      <th>Is Weekday</th>\n",
              "      <th>Next Day Is Weekday</th>\n",
              "      <th>Day of Year</th>\n",
              "      <th>sin_DoY</th>\n",
              "      <th>cos_DoY</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>2021-04-01</td>\n",
              "      <td>56.64959</td>\n",
              "      <td>147.32989</td>\n",
              "      <td>202.42128</td>\n",
              "      <td>202.42128</td>\n",
              "      <td>8.04000</td>\n",
              "      <td>305.628045</td>\n",
              "      <td>299.428045</td>\n",
              "      <td>192.178205</td>\n",
              "      <td>185.978205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>216.581800</td>\n",
              "      <td>235.986349</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>91</td>\n",
              "      <td>0.999991</td>\n",
              "      <td>0.004304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>2021-04-02</td>\n",
              "      <td>56.98982</td>\n",
              "      <td>166.66129</td>\n",
              "      <td>222.13281</td>\n",
              "      <td>222.13281</td>\n",
              "      <td>8.42962</td>\n",
              "      <td>294.933461</td>\n",
              "      <td>288.293461</td>\n",
              "      <td>184.176358</td>\n",
              "      <td>177.536358</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>226.122795</td>\n",
              "      <td>221.070202</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>92</td>\n",
              "      <td>0.999917</td>\n",
              "      <td>-0.012910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>334</th>\n",
              "      <td>2021-04-03</td>\n",
              "      <td>59.86041</td>\n",
              "      <td>163.58733</td>\n",
              "      <td>221.85874</td>\n",
              "      <td>221.85874</td>\n",
              "      <td>7.78921</td>\n",
              "      <td>282.792862</td>\n",
              "      <td>275.712862</td>\n",
              "      <td>171.236588</td>\n",
              "      <td>164.156588</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>232.033606</td>\n",
              "      <td>249.997365</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>93</td>\n",
              "      <td>0.999546</td>\n",
              "      <td>-0.030120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>335</th>\n",
              "      <td>2021-04-04</td>\n",
              "      <td>56.81811</td>\n",
              "      <td>162.65719</td>\n",
              "      <td>217.84170</td>\n",
              "      <td>217.84170</td>\n",
              "      <td>8.09275</td>\n",
              "      <td>280.273502</td>\n",
              "      <td>272.753502</td>\n",
              "      <td>168.677337</td>\n",
              "      <td>161.157337</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>219.171588</td>\n",
              "      <td>210.539110</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>94</td>\n",
              "      <td>0.998880</td>\n",
              "      <td>-0.047321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>336</th>\n",
              "      <td>2021-04-05</td>\n",
              "      <td>58.17918</td>\n",
              "      <td>155.78494</td>\n",
              "      <td>212.49572</td>\n",
              "      <td>212.49572</td>\n",
              "      <td>6.20191</td>\n",
              "      <td>297.694091</td>\n",
              "      <td>289.734091</td>\n",
              "      <td>185.852197</td>\n",
              "      <td>177.892197</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>221.809610</td>\n",
              "      <td>212.792903</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>95</td>\n",
              "      <td>0.997917</td>\n",
              "      <td>-0.064508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 48 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Data Item    Gas Day  Aggregate LNG Importations - Daily Flow  \\\n",
              "332       2021-04-01                                 56.64959   \n",
              "333       2021-04-02                                 56.98982   \n",
              "334       2021-04-03                                 59.86041   \n",
              "335       2021-04-04                                 56.81811   \n",
              "336       2021-04-05                                 58.17918   \n",
              "\n",
              "Data Item  Beach Including Norway - Daily Flow  \\\n",
              "332                                  147.32989   \n",
              "333                                  166.66129   \n",
              "334                                  163.58733   \n",
              "335                                  162.65719   \n",
              "336                                  155.78494   \n",
              "\n",
              "Data Item  Beach and IOG - Beach Delivery  Beach and IOG - Daily Flow  \\\n",
              "332                             202.42128                   202.42128   \n",
              "333                             222.13281                   222.13281   \n",
              "334                             221.85874                   221.85874   \n",
              "335                             217.84170                   217.84170   \n",
              "336                             212.49572                   212.49572   \n",
              "\n",
              "Data Item  Composite Weather Variable - Actual  Demand - Cold  \\\n",
              "332                                    8.04000     305.628045   \n",
              "333                                    8.42962     294.933461   \n",
              "334                                    7.78921     282.792862   \n",
              "335                                    8.09275     280.273502   \n",
              "336                                    6.20191     297.694091   \n",
              "\n",
              "Data Item  Demand - Cold, (excluding interconnector and storage)  \\\n",
              "332                                               299.428045       \n",
              "333                                               288.293461       \n",
              "334                                               275.712862       \n",
              "335                                               272.753502       \n",
              "336                                               289.734091       \n",
              "\n",
              "Data Item  Demand - Warm  \\\n",
              "332           192.178205   \n",
              "333           184.176358   \n",
              "334           171.236588   \n",
              "335           168.677337   \n",
              "336           185.852197   \n",
              "\n",
              "Data Item  Demand - Warm, (excluding interconnector and storage)  ...  \\\n",
              "332                                               185.978205      ...   \n",
              "333                                               177.536358      ...   \n",
              "334                                               164.156588      ...   \n",
              "335                                               161.157337      ...   \n",
              "336                                               177.892197      ...   \n",
              "\n",
              "Data Item  Storage, Short Range, Maximum potential flow  \\\n",
              "332                                                 0.0   \n",
              "333                                                 0.0   \n",
              "334                                                 0.0   \n",
              "335                                                 0.0   \n",
              "336                                                 0.0   \n",
              "\n",
              "Data Item  Storage, Short Range, Stock Levels  \\\n",
              "332                                       0.0   \n",
              "333                                       0.0   \n",
              "334                                       0.0   \n",
              "335                                       0.0   \n",
              "336                                       0.0   \n",
              "\n",
              "Data Item  System Entry Flows, National, Forecast  \\\n",
              "332                                    216.581800   \n",
              "333                                    226.122795   \n",
              "334                                    232.033606   \n",
              "335                                    219.171588   \n",
              "336                                    221.809610   \n",
              "\n",
              "Data Item  System Entry Flows, National, Physical  Day of Week  Is Weekday  \\\n",
              "332                                    235.986349            3           1   \n",
              "333                                    221.070202            4           1   \n",
              "334                                    249.997365            5           0   \n",
              "335                                    210.539110            6           0   \n",
              "336                                    212.792903            0           1   \n",
              "\n",
              "Data Item  Next Day Is Weekday  Day of Year   sin_DoY   cos_DoY  \n",
              "332                          1           91  0.999991  0.004304  \n",
              "333                          0           92  0.999917 -0.012910  \n",
              "334                          0           93  0.999546 -0.030120  \n",
              "335                          1           94  0.998880 -0.047321  \n",
              "336                          1           95  0.997917 -0.064508  \n",
              "\n",
              "[5 rows x 48 columns]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Reload with minimal preprocessing and cleaning\n",
        "df = load_data()\n",
        "df = preprocess(df, add_lags=False, add_labels=False)\n",
        "df = clean(df, remove_outliers=False)\n",
        "df = df.sort_values('Gas Day').reset_index(drop=True) # Should already be sorted, but just in case\n",
        "df = df[df['Gas Day'] >= '2021-04-01'] # discard the earliest data, as per the train/val/test split default\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42e48096",
      "metadata": {},
      "source": [
        "(2) Make the sequences, covering 30 days of the salient features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62f199b7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "WINDOW_SIZE = 30\n",
        "\n",
        "feature_cols = ['Composite Weather Variable - Actual', 'Demand Actual, NTS, D+1', 'Demand Forecast, NTS, hourly update', 'Interconnector - Daily Flow', 'Medium Storage - Actual Stock',\n",
        "              'Medium Storage - Stock Level at Max Flow', 'Predicted Closing Linepack (PCLP1)', \n",
        "              'SAP', 'SMPBuy',\t'SMPSell', \n",
        "              'Storage - Daily Flow','Storage - Delivery', 'Storage, Medium Range, Stock Levels', 'System Entry Flows, National, Forecast', 'System Entry Flows, National, Physical',\n",
        "              'Day of Week','Is Weekday','Next Day Is Weekday','Day of Year']\n",
        "\n",
        "def make_sequences(df, feature_cols):\n",
        "    X, Y = [], []\n",
        "    for i in range(len(df) - WINDOW_SIZE):\n",
        "        X.append(df[feature_cols].iloc[i : i + WINDOW_SIZE].values)\n",
        "        Y.append(df[price_targets].iloc[i + WINDOW_SIZE].values) # using SAP, SMPBuy and SMPSell as labels as before\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, y = make_sequences(df, feature_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5b5e0e",
      "metadata": {},
      "source": [
        "(3) Split sequentially into training, validate and test sets, so that the new gas days introduced at each stage are later than the days already seen. Then scale the sets individually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "0659b02a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training records: 1012\n",
            "Validation records: 289\n",
            "Test records: 145\n"
          ]
        }
      ],
      "source": [
        "train_size = int(0.7 * len(X))\n",
        "val_size   = int(0.2 * len(X))\n",
        "\n",
        "X_train, y_train = X[:train_size], y[:train_size]\n",
        "X_validate,   y_validate   = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
        "X_test,  y_test  = X[train_size+val_size:], y[train_size+val_size:]\n",
        "print(f\"Training records: {X_train.shape[0]}\")\n",
        "print(f\"Validation records: {X_validate.shape[0]}\")\n",
        "print(f\"Test records: {X_test.shape[0]}\")\n",
        "\n",
        "\n",
        "n_feats = X_train.shape[2]\n",
        "scaler = StandardScaler()\n",
        "X_train_2d = X_train.reshape(-1, n_feats)\n",
        "scaler.fit(X_train_2d)\n",
        "\n",
        "def scale_split(X):\n",
        "    X_2d = X.reshape(-1, n_feats)\n",
        "    Xs = scaler.transform(X_2d)\n",
        "    return Xs.reshape(-1, WINDOW_SIZE, n_feats)\n",
        "\n",
        "#Take a copy of the unscaled test data for comparison against the naive predictor\n",
        "X_validate_unscaled = X_validate.copy()\n",
        "\n",
        "X_train = scale_split(X_train)\n",
        "X_validate   = scale_split(X_validate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "593fa9eb",
      "metadata": {},
      "source": [
        "(4) Train and test the model - unfortunately it doesn't fit into the framework for sklearn-type models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aec8afd0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\mike\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m6,656\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ multi_output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │            \u001b[38;5;34m51\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 81ms/step - loss: 33.2486 - rmse: 5.7651 - val_loss: 4.4083 - val_rmse: 2.0996 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - loss: 23.1846 - rmse: 4.8112 - val_loss: 1.5409 - val_rmse: 1.2413 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 8.7249 - rmse: 2.9276 - val_loss: 0.9626 - val_rmse: 0.9811 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - loss: 3.9230 - rmse: 1.9558 - val_loss: 0.5775 - val_rmse: 0.7599 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.9024 - rmse: 1.3775 - val_loss: 0.3993 - val_rmse: 0.6319 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.4181 - rmse: 1.1892 - val_loss: 0.3200 - val_rmse: 0.5657 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.2340 - rmse: 1.1093 - val_loss: 0.2439 - val_rmse: 0.4938 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.1444 - rmse: 1.0611 - val_loss: 0.2422 - val_rmse: 0.4922 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.7879 - rmse: 0.8837 - val_loss: 0.1989 - val_rmse: 0.4460 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.8901 - rmse: 0.9390 - val_loss: 0.3028 - val_rmse: 0.5503 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.6528 - rmse: 0.8027 - val_loss: 0.1825 - val_rmse: 0.4272 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 0.7354 - rmse: 0.8568 - val_loss: 0.1836 - val_rmse: 0.4285 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.6247 - rmse: 0.7893 - val_loss: 0.1666 - val_rmse: 0.4081 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.5243 - rmse: 0.7221 - val_loss: 0.2207 - val_rmse: 0.4698 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.5560 - rmse: 0.7448 - val_loss: 0.1727 - val_rmse: 0.4155 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 0.6240 - rmse: 0.7845 - val_loss: 0.1690 - val_rmse: 0.4111 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 0.5147 - rmse: 0.7148 - val_loss: 0.1261 - val_rmse: 0.3550 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step - loss: 0.5146 - rmse: 0.7165 - val_loss: 0.1443 - val_rmse: 0.3798 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - loss: 0.5236 - rmse: 0.7226 - val_loss: 0.1451 - val_rmse: 0.3809 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.5197 - rmse: 0.7199 - val_loss: 0.1184 - val_rmse: 0.3441 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - loss: 0.4857 - rmse: 0.6959 - val_loss: 0.1185 - val_rmse: 0.3443 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.5080 - rmse: 0.7115 - val_loss: 0.1482 - val_rmse: 0.3850 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.4699 - rmse: 0.6853 - val_loss: 0.1036 - val_rmse: 0.3219 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4321 - rmse: 0.6542 - val_loss: 0.1387 - val_rmse: 0.3724 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.4358 - rmse: 0.6587 - val_loss: 0.1073 - val_rmse: 0.3276 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.4264 - rmse: 0.6496 - val_loss: 0.1176 - val_rmse: 0.3429 - learning_rate: 0.0010\n",
            "Epoch 27/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.3783 - rmse: 0.6138 - val_loss: 0.0938 - val_rmse: 0.3063 - learning_rate: 0.0010\n",
            "Epoch 28/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - loss: 0.4521 - rmse: 0.6708 - val_loss: 0.1256 - val_rmse: 0.3544 - learning_rate: 0.0010\n",
            "Epoch 29/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4170 - rmse: 0.6448 - val_loss: 0.1107 - val_rmse: 0.3327 - learning_rate: 0.0010\n",
            "Epoch 30/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 0.4399 - rmse: 0.6621 - val_loss: 0.1480 - val_rmse: 0.3847 - learning_rate: 0.0010\n",
            "Epoch 31/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - loss: 0.3956 - rmse: 0.6284 - val_loss: 0.1274 - val_rmse: 0.3569 - learning_rate: 0.0010\n",
            "Epoch 32/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.4392 - rmse: 0.6613 - val_loss: 0.1343 - val_rmse: 0.3664 - learning_rate: 0.0010\n",
            "Epoch 33/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - loss: 0.4571 - rmse: 0.6716 - val_loss: 0.1202 - val_rmse: 0.3466 - learning_rate: 5.0000e-04\n",
            "Epoch 34/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 45ms/step - loss: 0.4100 - rmse: 0.6399 - val_loss: 0.1066 - val_rmse: 0.3265 - learning_rate: 5.0000e-04\n",
            "Epoch 35/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.3956 - rmse: 0.6280 - val_loss: 0.1107 - val_rmse: 0.3327 - learning_rate: 5.0000e-04\n",
            "Epoch 36/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3636 - rmse: 0.6016 - val_loss: 0.1184 - val_rmse: 0.3442 - learning_rate: 5.0000e-04\n",
            "Epoch 37/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.3014 - rmse: 0.5455 - val_loss: 0.1220 - val_rmse: 0.3492 - learning_rate: 5.0000e-04\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.1001 - rmse: 0.3160\n",
            "Overall RMSE: 0.3063\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step\n",
            "SAP RMSE: 0.3267\n",
            "SAP naive predictor RMSE: 0.0723\n",
            "SMPBuy RMSE: 0.3196\n",
            "SMPBuy naive predictor RMSE: 0.0823\n",
            "SMPSell RMSE: 0.2695\n",
            "SMPSell naive predictor RMSE: 0.0736\n"
          ]
        }
      ],
      "source": [
        "def make_rnn():\n",
        "    #model = models.Sequential([\n",
        "        #layers.LSTM(128, return_sequences=True, input_shape=(WINDOW_SIZE, n_feats)),\n",
        "        #layers.Dropout(0.02),\n",
        "        #layers.LSTM(64),\n",
        "        #layers.Dropout(0.02),\n",
        "        #layers.Dense(32, activation='relu'),\n",
        "        #layers.Dense(3, name='multi_output')   # predicts [SAP, SMPBuy, SMPSell]\n",
        "    #])\n",
        "    model = models.Sequential([\n",
        "        # Single, small LSTM — no return_sequences, so it only outputs the last hidden state\n",
        "        layers.LSTM(32, input_shape=(WINDOW_SIZE, n_feats)),\n",
        "\n",
        "        # (Optional) small dense “bottleneck” to pick up any non-linear mix\n",
        "        layers.Dense(16, activation='relu'),\n",
        "\n",
        "        # Multi-output head predicts [SAP, SMPBuy, SMPSell]\n",
        "        layers.Dense(3, name='multi_output')\n",
        "    ])\n",
        "\n",
        "\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss='mse',\n",
        "    metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def train_and_validate_rnn(model, context:Context):\n",
        "\n",
        "    callbacks = [\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True\n",
        "        ),\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss',\n",
        "            factor=0.5,\n",
        "            patience=5\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_validate, y_validate),\n",
        "        epochs=100,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    # 5. Evaluate model RMSE on validation set\n",
        "    eval_results = model.evaluate(X_validate, y_validate, return_dict=True)\n",
        "    model_rmse = eval_results['rmse']\n",
        "    print(f\"Overall RMSE: {model_rmse:.4f}\")\n",
        "\n",
        "    results = []\n",
        "    #Individual RMSE for each target\n",
        "    y_pred = model.predict(X_validate)\n",
        "    for i, name in enumerate(price_targets):\n",
        "        #get model RMSE for each target\n",
        "        rmse = get_rmse(y_validate[:,i], y_pred[:,i])\n",
        "        print(f\"{name} RMSE: {rmse:.4f}\")\n",
        "\n",
        "        # get naive predictor RMSE based on the unscaled inputs\n",
        "        feat_idx = feature_cols.index(name)\n",
        "        y_pred_naive = X_validate_unscaled[:, -1, feat_idx]\n",
        "        y_true = y_validate[:, i]\n",
        "        naive_rmse = get_rmse(y_true, y_pred_naive)\n",
        "        print(f\"{name} naive predictor RMSE: {naive_rmse:.4f}\")\n",
        "\n",
        "        # add stats\n",
        "        testResult = Result(context, name, rmse, naive_rmse)\n",
        "\n",
        "        # add to the running list of results\n",
        "        results.append(testResult)\n",
        "\n",
        "    return results\n",
        "\n",
        "model = make_rnn()\n",
        "context = Context(\"RNN\", SPLIT_SEQUENTIAL)\n",
        "all_results += train_and_validate_rnn(model, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12f23169",
      "metadata": {},
      "source": [
        "### Finally, try reframing RNN with a residual model\n",
        "\n",
        "The RNN is still testing worse than naive predictor, indicating that the network is not learning anything from the additional fields that adds anything to the current day prices.\n",
        "As a final option, try a residual model that predicts the delta from the current day's price\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac9c053",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\mike\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:219: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">6,656</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ last_vals (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ delta (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">51</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_output     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ last_vals[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ delta[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m19\u001b[0m)    │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │      \u001b[38;5;34m6,656\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │        \u001b[38;5;34m528\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ last_vals (\u001b[38;5;33mLambda\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ delta (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │         \u001b[38;5;34m51\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ residual_output     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ last_vals[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ delta[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7,235</span> (28.26 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7,235\u001b[0m (28.26 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 31ms/step - loss: 33.4401 - rmse: 5.7796 - val_loss: 12.2580 - val_rmse: 3.5011 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 27.9117 - rmse: 5.2822 - val_loss: 10.6101 - val_rmse: 3.2573 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 26.0676 - rmse: 5.1006 - val_loss: 5.3083 - val_rmse: 2.3040 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 14.3820 - rmse: 3.7763 - val_loss: 1.0811 - val_rmse: 1.0397 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 4.4569 - rmse: 2.1105 - val_loss: 0.9065 - val_rmse: 0.9521 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 2.9813 - rmse: 1.7248 - val_loss: 0.6376 - val_rmse: 0.7985 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 2.8445 - rmse: 1.6835 - val_loss: 0.2457 - val_rmse: 0.4957 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 1.7962 - rmse: 1.3388 - val_loss: 0.1097 - val_rmse: 0.3312 - learning_rate: 0.0010\n",
            "Epoch 9/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.7035 - rmse: 1.3015 - val_loss: 0.0582 - val_rmse: 0.2413 - learning_rate: 0.0010\n",
            "Epoch 10/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1.4351 - rmse: 1.1967 - val_loss: 0.0355 - val_rmse: 0.1884 - learning_rate: 0.0010\n",
            "Epoch 11/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 1.1381 - rmse: 1.0653 - val_loss: 0.0339 - val_rmse: 0.1840 - learning_rate: 0.0010\n",
            "Epoch 12/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - loss: 0.8620 - rmse: 0.9277 - val_loss: 0.0293 - val_rmse: 0.1711 - learning_rate: 0.0010\n",
            "Epoch 13/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.7819 - rmse: 0.8818 - val_loss: 0.0289 - val_rmse: 0.1700 - learning_rate: 0.0010\n",
            "Epoch 14/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.7824 - rmse: 0.8800 - val_loss: 0.0301 - val_rmse: 0.1735 - learning_rate: 0.0010\n",
            "Epoch 15/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.6730 - rmse: 0.8191 - val_loss: 0.0315 - val_rmse: 0.1776 - learning_rate: 0.0010\n",
            "Epoch 16/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - loss: 0.6816 - rmse: 0.8246 - val_loss: 0.0291 - val_rmse: 0.1705 - learning_rate: 0.0010\n",
            "Epoch 17/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - loss: 0.6503 - rmse: 0.8057 - val_loss: 0.0280 - val_rmse: 0.1672 - learning_rate: 0.0010\n",
            "Epoch 18/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - loss: 0.6371 - rmse: 0.7949 - val_loss: 0.0255 - val_rmse: 0.1596 - learning_rate: 0.0010\n",
            "Epoch 19/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.5999 - rmse: 0.7743 - val_loss: 0.0344 - val_rmse: 0.1856 - learning_rate: 0.0010\n",
            "Epoch 20/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.5711 - rmse: 0.7548 - val_loss: 0.0233 - val_rmse: 0.1527 - learning_rate: 0.0010\n",
            "Epoch 21/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 36ms/step - loss: 0.5986 - rmse: 0.7712 - val_loss: 0.0271 - val_rmse: 0.1645 - learning_rate: 0.0010\n",
            "Epoch 22/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.5311 - rmse: 0.7262 - val_loss: 0.0236 - val_rmse: 0.1536 - learning_rate: 0.0010\n",
            "Epoch 23/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.6089 - rmse: 0.7793 - val_loss: 0.0255 - val_rmse: 0.1597 - learning_rate: 0.0010\n",
            "Epoch 24/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - loss: 0.5890 - rmse: 0.7663 - val_loss: 0.0286 - val_rmse: 0.1690 - learning_rate: 0.0010\n",
            "Epoch 25/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.5281 - rmse: 0.7264 - val_loss: 0.0239 - val_rmse: 0.1545 - learning_rate: 0.0010\n",
            "Epoch 26/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.4905 - rmse: 0.6991 - val_loss: 0.0250 - val_rmse: 0.1581 - learning_rate: 5.0000e-04\n",
            "Epoch 27/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - loss: 0.5125 - rmse: 0.7152 - val_loss: 0.0256 - val_rmse: 0.1601 - learning_rate: 5.0000e-04\n",
            "Epoch 28/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - loss: 0.4739 - rmse: 0.6871 - val_loss: 0.0263 - val_rmse: 0.1621 - learning_rate: 5.0000e-04\n",
            "Epoch 29/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - loss: 0.5535 - rmse: 0.7405 - val_loss: 0.0266 - val_rmse: 0.1631 - learning_rate: 5.0000e-04\n",
            "Epoch 30/100\n",
            "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.4895 - rmse: 0.6991 - val_loss: 0.0279 - val_rmse: 0.1669 - learning_rate: 5.0000e-04\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0242 - rmse: 0.1544 \n",
            "Overall RMSE: 0.1527\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 43ms/step\n",
            "SAP RMSE: 0.1435\n",
            "SAP naive predictor RMSE: 0.0723\n",
            "SMPBuy RMSE: 0.1495\n",
            "SMPBuy naive predictor RMSE: 0.0823\n",
            "SMPSell RMSE: 0.1645\n",
            "SMPSell naive predictor RMSE: 0.0736\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def make_rnn_residual():\n",
        "    # 1) Inputs\n",
        "    inputs = layers.Input(shape=(WINDOW_SIZE, n_feats))\n",
        "\n",
        "    # 2) Core LSTM\n",
        "    x = layers.LSTM(32)(inputs)\n",
        "    x = layers.Dense(16, activation='relu')(x)\n",
        "\n",
        "    # 3) Delta prediction head (predict tomorrow’s Δ for each series)\n",
        "    delta = layers.Dense(3, name='delta')(x)  \n",
        "    #   outputs [ΔSAP, ΔSMPBuy, ΔSMPSell]\n",
        "    idxs_of_labels = [feature_cols.index(pt) for pt in price_targets]\n",
        "    # 4) Grab today's values from the last timestep of the sequence\n",
        "    #    This gives shape (batch, 3) corresponding to [SAP_t, SMPBuy_t, SMPSell_t].\n",
        "    last_vals = layers.Lambda(lambda z: tf.gather(z[:, -1, :], idxs_of_labels, axis=1),\n",
        "                            name='last_vals')(inputs)\n",
        "\n",
        "    # 5) Add skip-connection: tomorrow = today + predicted Δ\n",
        "    outputs = layers.Add(name='residual_output')([last_vals, delta])\n",
        "\n",
        "    # 6) Assemble and compile\n",
        "    model = models.Model(inputs, outputs)\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='mse',\n",
        "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')]\n",
        "    )\n",
        "\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "model = make_rnn_residual()\n",
        "context = Context(\"Residual RNN\", SPLIT_SEQUENTIAL)\n",
        "all_results += train_and_validate_rnn(model, context)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32ed5add",
      "metadata": {},
      "source": [
        "### Pick the best performing model type\n",
        "No model outperformed the naive predictor so I'll have to choose the least bad one to tune based on the gathered results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "192bf53f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random forest (SAP): difference over naive RMSE = -0.0188\n",
            "Random forest (SMPBuy): difference over naive RMSE = -0.0259\n",
            "Random forest (SMPSell): difference over naive RMSE = -0.0357\n",
            "Gradient boosting (SMPSell): difference over naive RMSE = -0.0408\n",
            "Gradient boosting (SMPBuy): difference over naive RMSE = -0.0609\n",
            "Gradient boosting (SAP): difference over naive RMSE = -0.0667\n",
            "Residual RNN (SMPBuy): difference over naive RMSE = -0.0672\n",
            "Residual RNN (SAP): difference over naive RMSE = -0.0712\n",
            "Residual RNN (SMPSell): difference over naive RMSE = -0.0909\n",
            "Linear regression (SAP): difference over naive RMSE = -0.1809\n",
            "RNN (SMPSell): difference over naive RMSE = -0.1959\n",
            "RNN (SMPBuy): difference over naive RMSE = -0.2373\n",
            "Linear regression (SMPBuy): difference over naive RMSE = -0.2431\n",
            "RNN (SAP): difference over naive RMSE = -0.2544\n",
            "Linear regression (SMPSell): difference over naive RMSE = -0.3460\n"
          ]
        }
      ],
      "source": [
        "# sort by the delta (model_rmse - naive_rmse)\n",
        "#Filter results to only include \"Latest\" data test set\n",
        "filtered_results = [r for r in all_results if r.context.test_set == SPLIT_SEQUENTIAL]\n",
        "\n",
        "sorted_results = sorted(filtered_results, key=lambda r: r.naive_rmse - r.model_rmse, reverse=True)\n",
        "\n",
        "for r in sorted_results:\n",
        "    difference = r.naive_rmse - r.model_rmse\n",
        "    print(f\"{r.context.model_type} ({r.price_label}): difference over naive RMSE = {difference:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b02542e",
      "metadata": {},
      "source": [
        "Random forest came out best so I'll try to improve on the hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ecec5c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up search framework in order to try bayesian and random search optimization\n",
        "def search_hyperparams(search, df_train, df_validate, priceTarget):\n",
        "    X_train = get_X(df_train)\n",
        "    y_train = get_y(df_train, priceTarget)\n",
        "    \n",
        "    # Run the hyperparameter search\n",
        "    start = time.perf_counter()\n",
        "    search.fit(X_train, y_train)\n",
        "    end = time.perf_counter()\n",
        "    \n",
        "    elapsed_minutes = (end - start) / 60\n",
        "    print(f\"Search took {elapsed_minutes:.4f} minutes\")\n",
        "\n",
        "    print(\"Best hyperparameters:\", search.best_params_)\n",
        "    print(\"Best CV RMSE on train set: {:.4f}\".format(-search.best_score_))\n",
        "\n",
        "    # Get the best model and evaluate it on the validation set        \n",
        "    \n",
        "    X_validate = get_X(df_validate)\n",
        "    y_validate = get_y(df_validate, priceTarget)\n",
        "\n",
        "    best_model = search.best_estimator_\n",
        "\n",
        "    rmse_validate = validate_model(best_model, X_validate, y_validate)\n",
        "    \n",
        "    # Get naive predictor RMSE for comparison\n",
        "    rmse_naive_validate = naive_predictions(df_validate, priceTarget)\n",
        "    \n",
        "    print_results(priceTarget + \" validate\", rmse_naive_validate, rmse_validate)\n",
        "    return best_model, search.best_params_, rmse_validate, rmse_naive_validate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5188f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 10 of 60 raw files\n",
            "Processed 20 of 60 raw files\n",
            "Processed 30 of 60 raw files\n",
            "Processed 40 of 60 raw files\n",
            "Processed 50 of 60 raw files\n",
            "Processed 60 of 60 raw files\n",
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1802, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1789, 78)\n",
            "(1784, 78)\n",
            "(1784, 78)\n",
            "(1783, 78)\n",
            "(1783, 78)\n",
            "(1762, 78)\n",
            "SAP: Random search\n",
            "Fitting 5 folds for each of 50 candidates, totalling 250 fits\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[24], line 37\u001b[0m\n\u001b[0;32m     25\u001b[0m rand_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     26\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mrf,\n\u001b[0;32m     27\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mrandom_search_grid,             \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m\n\u001b[0;32m     34\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Random search\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 37\u001b[0m best_model, best_params, rmse_validate, rmse_naive_validate \u001b[38;5;241m=\u001b[39m search_hyperparams(rand_search, df, df, col)\n\u001b[0;32m     38\u001b[0m hyperparam_results[col \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Random\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: best_model,\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: best_params,\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse_validate,\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnaive_rmse\u001b[39m\u001b[38;5;124m'\u001b[39m: rmse_naive_validate\n\u001b[0;32m     43\u001b[0m }\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Persist the best model\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[23], line 8\u001b[0m, in \u001b[0;36msearch_hyperparams\u001b[1;34m(search, df_train, df_validate, col)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Run the hyperparameter search\u001b[39;00m\n\u001b[0;32m      7\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m----> 8\u001b[0m search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m      9\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m     11\u001b[0m elapsed_minutes \u001b[38;5;241m=\u001b[39m (end \u001b[38;5;241m-\u001b[39m start) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m60\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1960\u001b[0m         ParameterSampler(\n\u001b[0;32m   1961\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_distributions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_state\n\u001b[0;32m   1962\u001b[0m         )\n\u001b[0;32m   1963\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    965\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    966\u001b[0m         clone(base_estimator),\n\u001b[0;32m    967\u001b[0m         X,\n\u001b[0;32m    968\u001b[0m         y,\n\u001b[0;32m    969\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    970\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    971\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[0;32m    972\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[0;32m    973\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[0;32m    974\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[0;32m    975\u001b[0m     )\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params),\n\u001b[0;32m    978\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39msplitter\u001b[38;5;241m.\u001b[39msplit)),\n\u001b[0;32m    979\u001b[0m     )\n\u001b[0;32m    980\u001b[0m )\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\mike\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# reload the data for the sklearn-style framework\n",
        "df = load_data()\n",
        "df = preprocess(df)\n",
        "df = clean(df)\n",
        "train, validate, test = split_sequential(df)\n",
        "models_dir = Path('..') / 'models'\n",
        "\n",
        "# Start with a wide range of candidates\n",
        "random_search_grid = {\n",
        "    'n_estimators':     [100, 200, 500],\n",
        "    'max_depth':        [None, 10, 20],\n",
        "    'min_samples_split':[2,5],\n",
        "    'min_samples_leaf': [1,2],\n",
        "    'max_features':     [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 1.0],\n",
        "    'ccp_alpha':        [0.0, 0.001]\n",
        "}\n",
        "hyperparam_results = {}\n",
        "for pt in price_targets:\n",
        "    rf = RandomForestRegressor(\n",
        "            random_state=42,\n",
        "            n_jobs=-1,  \n",
        "            oob_score=True\n",
        "        )\n",
        "    # Try random search\n",
        "    rand_search = RandomizedSearchCV(\n",
        "        estimator=rf,\n",
        "        param_distributions=random_search_grid,             \n",
        "        n_iter=50,                                  \n",
        "        cv=5,\n",
        "        scoring='neg_root_mean_squared_error',      \n",
        "        n_jobs=-1,\n",
        "        verbose=3,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    print(f\"{pt}: Random search\")\n",
        "    best_model, best_params, rmse_validate, rmse_naive_validate = search_hyperparams(rand_search, train, validate, pt)\n",
        "    hyperparam_results[pt + \" Random\"]= {\n",
        "        'model': best_model,\n",
        "        'params': best_params,\n",
        "        'model_rmse': rmse_validate,\n",
        "        'naive_rmse': rmse_naive_validate\n",
        "    }\n",
        "    # Persist the best model\n",
        "    file_path = models_dir / f\"{pt}_random_best_rf.joblib\"\n",
        "    joblib.dump(model, file_path)\n",
        "\n",
        "    # Try bayesian seatch\n",
        "    bayes_search_spaces = {\n",
        "        'n_estimators':      Integer(100, 500),\n",
        "        'max_depth':         Integer(10, 50),\n",
        "        'min_samples_split': Integer(2, 5),\n",
        "        'min_samples_leaf':  Integer(1, 2),\n",
        "        'max_features':      Real(0.1, 1.0),\n",
        "        'ccp_alpha':         Real(0.0, 0.01)\n",
        "    }\n",
        "\n",
        "    rf = RandomForestRegressor(\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            oob_score=True\n",
        "        )\n",
        "\n",
        "    bayes_search = BayesSearchCV(\n",
        "        estimator=rf,\n",
        "        search_spaces=bayes_search_spaces,\n",
        "        n_iter=50,\n",
        "        cv=5,\n",
        "        scoring='neg_root_mean_squared_error',\n",
        "        n_jobs=-1,\n",
        "        verbose=3,\n",
        "        random_state=51\n",
        "    )\n",
        "\n",
        "    print(f\"{pt}: Bayesian search\")\n",
        "    best_model, best_params, rmse_validate, rmse_naive_validate = search_hyperparams(bayes_search, train, validate, pt)\n",
        "    hyperparam_results[pt + \" Bayesian\"]= {\n",
        "        'model': best_model,\n",
        "        'params': best_params,\n",
        "        'model_rmse': rmse_validate,\n",
        "        'naive_rmse': rmse_naive_validate\n",
        "    }\n",
        "    # Persist the best model\n",
        "    file_path = models_dir / f\"{pt}_bayesian_best_rf.joblib\"\n",
        "    joblib.dump(model, file_path)\n",
        "\n",
        "# Print out details of the best estimators from each search\n",
        "sorted_results = sorted(\n",
        "    hyperparam_results.items(),\n",
        "    key=lambda kv: kv[1]['naive_rmse'] - kv[1]['model_rmse'],\n",
        "    reverse=True,\n",
        ")\n",
        "for run_name, result in sorted_results:\n",
        "    difference = result['naive_rmse'] - result['model_rmse']\n",
        "    print(f\"{run_name}: difference over naive RMSE = {difference:.4f} with parameters: {result['params']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e76c6b",
      "metadata": {},
      "source": [
        "Based on the output, the best-tuned estimator for each price target came from the random search (despite the Bayesian search being given a search space covering all the random search grid, and taking 3 times as much time). Now to reload the best estimator for each price target and finally test against the latest 10%  of data which has been held out so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f2d4a5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = get_X(test)\n",
        "model_rmses = []\n",
        "naive_rmses = []\n",
        "for pt in price_targets:\n",
        "    file_path = models_dir / f\"{pt}_random_best_rf.joblib\"\n",
        "    model = joblib.load(file_path)\n",
        "    y_test = get_y(test, pt)\n",
        "    model_rmse = validate_model(model, X_test, y_test)\n",
        "    model_rmses.append(model_rmse)\n",
        "    naive_rmse = naive_predictions(test, pt)\n",
        "    naive_rmses.append(naive_rmse)\n",
        "    \n",
        "\n",
        "x = range(len(price_targets))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar([i - width/2 for i in x], model_rmses, width, label='Model RMSE')\n",
        "ax.bar([i + width/2 for i in x], naive_rmses, width, label='Naive predictor RMSE')\n",
        "\n",
        "ax.set_xlabel('Price Target')\n",
        "ax.set_ylabel('RMSE')\n",
        "ax.set_title('Model vs. Naive RMSE by Price Target')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(price_targets)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
